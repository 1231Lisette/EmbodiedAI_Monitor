
    window.RESEARCH_DATA = [{"id": "nvidia/music-flamingo-hf", "type": "models", "title": "nvidia/music-flamingo-hf", "author": "nvidia", "abstract": "ðŸ›ï¸ Official release from nvidia. Task: audio-text-to-text", "date": "2025-12-04", "url": "https://huggingface.co/nvidia/music-flamingo-hf", "stars": 56, "score": 10, "tags": ["text2text-generation", "music/songs", "license:other", "music understanding", "region:us", "music reasoning", "safetensors", "General", "audio-text-to-text", "audioflamingo3", "arxiv:2511.10289", "dataset:nvidia/MF-Skills", "arxiv:2505.13032", "endpoints_compatible", "transformers", "en"], "source": "HuggingFace"}, {"id": "nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1", "type": "models", "title": "nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1", "author": "nvidia", "abstract": "ðŸ›ï¸ Official release from nvidia. Task: image-text-to-text", "date": "2025-12-04", "url": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1", "stars": 168, "score": 10, "tags": ["license:other", "safetensors", "General", "nvidia", "llama3.1", "VLM", "endpoints_compatible", "image-text-to-text", "transformers", "region:us"], "source": "HuggingFace"}, {"id": "nvidia/Fixer", "type": "models", "title": "nvidia/Fixer", "author": "nvidia", "abstract": "ðŸ›ï¸ Official release from nvidia. Task: image-to-image", "date": "2025-12-04", "url": "https://huggingface.co/nvidia/Fixer", "stars": 8, "score": 10, "tags": ["region:us", "license:other", "image2image", "General", "image-to-image", "nvidia", "arxiv:2503.01774", "fixer", "en"], "source": "HuggingFace"}, {"id": "allenai/Olmo-3-1125-32B", "type": "models", "title": "allenai/Olmo-3-1125-32B", "author": "allenai", "abstract": "ðŸ›ï¸ Official release from allenai. Task: text-generation", "date": "2025-12-03", "url": "https://huggingface.co/allenai/Olmo-3-1125-32B", "stars": 93, "score": 10, "tags": ["region:us", "olmo3", "safetensors", "General", "model-index", "text-generation", "dataset:allenai/dolma3_mix-5.5T-1125", "license:apache-2.0", "endpoints_compatible", "transformers", "en"], "source": "HuggingFace"}, {"id": "allenai/Olmo-3-1025-7B", "type": "models", "title": "allenai/Olmo-3-1025-7B", "author": "allenai", "abstract": "ðŸ›ï¸ Official release from allenai. Task: text-generation", "date": "2025-12-03", "url": "https://huggingface.co/allenai/Olmo-3-1025-7B", "stars": 35, "score": 10, "tags": ["region:us", "olmo3", "safetensors", "General", "text-generation", "license:apache-2.0", "dataset:allenai/dolma3_mix-6T-1025", "endpoints_compatible", "transformers", "en"], "source": "HuggingFace"}, {"id": "allenai/Olmo-3-7B-RL-Zero-IF", "type": "models", "title": "allenai/Olmo-3-7B-RL-Zero-IF", "author": "allenai", "abstract": "ðŸ›ï¸ Official release from allenai. Task: text-generation", "date": "2025-12-01", "url": "https://huggingface.co/allenai/Olmo-3-7B-RL-Zero-IF", "stars": 4, "score": 10, "tags": ["base_model:finetune:allenai/Olmo-3-1025-7B", "conversational", "region:us", "olmo3", "safetensors", "Reinforcement", "text-generation", "license:apache-2.0", "dataset:allenai/Dolci-RLZero-IF-7B", "endpoints_compatible", "base_model:allenai/Olmo-3-1025-7B", "transformers", "en"], "source": "HuggingFace"}, {"id": "microsoft/VibeVoice-Realtime-0.5B", "type": "models", "title": "microsoft/VibeVoice-Realtime-0.5B", "author": "microsoft", "abstract": "ðŸ›ï¸ Official release from microsoft. Task: text-to-speech", "date": "2025-12-05", "url": "https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B", "stars": 189, "score": 10, "tags": ["region:us", "endpoints_compatible", "Long-from speech generation", "safetensors", "arxiv:2508.19205", "General", "license:mit", "vibevoice_streaming", "Streaming text input", "text-to-speech", "Realtime TTS", "arxiv:2412.08635", "transformers", "en"], "source": "HuggingFace"}, {"id": "microsoft/Dayhoff-170m-UR50", "type": "models", "title": "microsoft/Dayhoff-170m-UR50", "author": "microsoft", "abstract": "ðŸ›ï¸ Official release from microsoft. Task: text-generation", "date": "2025-12-04", "url": "https://huggingface.co/microsoft/Dayhoff-170m-UR50", "stars": 2, "score": 10, "tags": ["protein-generation", "safetensors", "General", "license:mit", "arxiv:2502.12479", "text-generation", "jamba", "dataset:microsoft/Dayhoff", "deploy:azure", "endpoints_compatible", "transformers", "region:us"], "source": "HuggingFace"}, {"id": "microsoft/Dayhoff-3b-GR-HM-c", "type": "models", "title": "microsoft/Dayhoff-3b-GR-HM-c", "author": "microsoft", "abstract": "ðŸ›ï¸ Official release from microsoft. Task: text-generation", "date": "2025-12-04", "url": "https://huggingface.co/microsoft/Dayhoff-3b-GR-HM-c", "stars": 1, "score": 10, "tags": ["protein-generation", "safetensors", "General", "license:mit", "arxiv:2502.12479", "text-generation", "jamba", "dataset:microsoft/Dayhoff", "deploy:azure", "endpoints_compatible", "transformers", "region:us"], "source": "HuggingFace"}, {"id": "lerobot/xvla-agibot-world", "type": "models", "title": "lerobot/xvla-agibot-world", "author": "lerobot", "abstract": "ðŸ›ï¸ Official release from lerobot. Task: robotics", "date": "2025-12-03", "url": "https://huggingface.co/lerobot/xvla-agibot-world", "stars": 3, "score": 10, "tags": ["safetensors", "Reinforcement", "xvla", "dataset:unknown", "license:apache-2.0", "robotics", "LLM/VLA", "lerobot", "region:us"], "source": "HuggingFace"}, {"id": "2512.04960v1", "type": "papers", "title": "Hybrid-Diffusion Models: Combining Open-loop Routines with Visuomotor Diffusion Policies", "author": "Jonne Van Haastregt, Bastian Orthmann, Michael C. Welle", "abstract": "Despite the fact that visuomotor-based policies obtained via imitation learning demonstrate good performances in complex manipulation tasks, they usually struggle to achieve the same accuracy and speed as traditional control based methods. In this work, we introduce Hybrid-Diffusion models that combine open-loop routines with visuomotor diffusion policies. We develop Teleoperation Augmentation Primitives (TAPs) that allow the operator to perform predefined routines, such as locking specific axes, moving to perching waypoints, or triggering task-specific routines seamlessly during demonstrations. Our Hybrid-Diffusion method learns to trigger such TAPs during inference. We validate the method on challenging real-world tasks: Vial Aspiration, Open-Container Liquid Transfer, and container unscrewing. All experimental videos are available on the project's website: https://hybriddiffusion.github.io/", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04960v1", "score": 9, "tags": ["Manipulation", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "74627617", "type": "projects", "title": "commaai/openpilot", "author": "commaai", "abstract": "openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.", "date": "2025-12-05", "url": "https://github.com/commaai/openpilot", "stars": 59131, "score": 8, "tags": ["General"], "source": "GitHub"}, {"id": "801867971", "type": "projects", "title": "TianxingChen/Embodied-AI-Guide", "author": "TianxingChen", "abstract": "[Lumina Embodied AI] å…·èº«æ™ºèƒ½æŠ€æœ¯æŒ‡å— Embodied-AI-Guide", "date": "2025-12-05", "url": "https://github.com/TianxingChen/Embodied-AI-Guide", "stars": 9463, "score": 8, "tags": ["General"], "source": "GitHub"}, {"id": "InternRobotics/InternVLA-N1", "type": "models", "title": "InternRobotics/InternVLA-N1", "author": "InternRobotics", "abstract": "ðŸ”¥ Trending on HF (36 likes). Task: robotics", "date": "None", "url": "https://huggingface.co/InternRobotics/InternVLA-N1", "stars": 36, "score": 8, "tags": ["text-generation-inference", "video-language-model", "license:cc-by-nc-sa-4.0", "safetensors", "navigation", "autotrain_compatible", "vision-language-model", "text-generation", "robotics", "LLM/VLA", "transformers", "endpoints_compatible", "qwen2_5_vl", "region:us"], "source": "HuggingFace"}, {"id": "IPEC-COMMUNITY/spatialvla-4b-224-sft-fractal", "type": "models", "title": "IPEC-COMMUNITY/spatialvla-4b-224-sft-fractal", "author": "IPEC-COMMUNITY", "abstract": "ðŸ”¥ Trending on HF (0 likes). Task: image-text-to-text", "date": "None", "url": "https://huggingface.co/IPEC-COMMUNITY/spatialvla-4b-224-sft-fractal", "stars": 0, "score": 8, "tags": ["region:us", "base_model:IPEC-COMMUNITY/spatialvla-4b-224-pt", "arxiv:2501.15830", "custom_code", "VLA", "safetensors", "Generalist Robot Policy", "license:mit", "base_model:finetune:IPEC-COMMUNITY/spatialvla-4b-224-pt", "image-text-to-text", "spatialvla", "Foundation Vision-language-action Model", "robotics", "LLM/VLA", "feature-extraction", "transformers", "en"], "source": "HuggingFace"}, {"id": "2512.05060v1", "type": "papers", "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer", "author": "Xianfeng Wu, Yajing Bai, Minghan Li", "abstract": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05060v1", "score": 7, "tags": ["Vision", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04579v1", "type": "papers", "title": "Gauss-Newton accelerated MPPI Control", "author": "Hannes Homburger, Katrin BaumgÃ¤rtner, Moritz Diehl", "abstract": "Model Predictive Path Integral (MPPI) control is a sampling-based optimization method that has recently attracted attention, particularly in the robotics and reinforcement learning communities. MPPI has been widely applied as a GPU-accelerated random search method to deterministic direct single-shooting optimal control problems arising in model predictive control (MPC) formulations. MPPI offers several key advantages, including flexibility, robustness, ease of implementation, and inherent parallelizability. However, its performance can deteriorate in high-dimensional settings since the optimal control problem is solved via Monte Carlo sampling. To address this limitation, this paper proposes an enhanced MPPI method that incorporates a Jacobian reconstruction technique and the second-order Generalized Gauss-Newton method. This novel approach is called \\textit{Gauss-Newton accelerated MPPI}. The numerical results show that the Gauss-Newton accelerated MPPI approach substantially improves MPPI scalability and computational efficiency while preserving the key benefits of the classical MPPI framework, making it a promising approach even for high-dimensional problems.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04579v1", "score": 7, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04535v1", "type": "papers", "title": "GTM: Simulating the World of Tools for AI Agents", "author": "Zhenzhen Ren, Xinpeng Zhang, Zhenxing Qian", "abstract": "The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04535v1", "score": 7, "tags": ["LLM/VLA", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.04973v1", "type": "papers", "title": "Preliminary Analysis and Simulation of a Compact Variable Stiffness Wrist", "author": "Giuseppe Milazzo, Manuel G. Catalano, Antonio Bicchi", "abstract": "Variable Stiffness Actuators prove invaluable for robotics applications in unstructured environments, fostering safe interactions and enhancing task adaptability. Nevertheless, their mechanical design inevitably results in larger and heavier structures compared to classical rigid actuators. This paper introduces a novel 3 Degrees of Freedom (DoFs) parallel wrist that achieves variable stiffness through redundant elastic actuation. Leveraging its parallel architecture, the device employs only four motors, rendering it compact and lightweight. This characteristic makes it particularly well-suited for applications in prosthetics or humanoid robotics. The manuscript delves into the theoretical model of the device and proposes a sophisticated control strategy for independent regulation of joint position and stiffness. Furthermore, it validates the proposed controller through simulation, utilizing a comprehensive analysis of the system dynamics. The reported results affirm the ability of the device to achieve high accuracy and disturbance rejection in rigid configurations while minimizing interaction forces with its compliant behavior.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04973v1", "score": 6, "tags": ["Humanoid", "Sim2Real", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04813v1", "type": "papers", "title": "MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation", "author": "Huanqian Wang, Chi Bene Chen, Yang Yue", "abstract": "Imitation learning method has shown immense promise for robotic manipulation, yet its practical deployment is fundamentally constrained by the data scarcity. Despite prior work on collecting large-scale datasets, there still remains a significant gap to robust spatial generalization. We identify a key limitation: individual trajectories, regardless of their length, are typically collected from a \\emph{single, static spatial configuration} of the environment. This includes fixed object and target spatial positions as well as unchanging camera viewpoints, which significantly restricts the diversity of spatial information available for learning. To address this critical bottleneck in data efficiency, we propose \\textbf{MOtion-Based Variability Enhancement} (\\emph{MOVE}), a simple yet effective data collection paradigm that enables the acquisition of richer spatial information from dynamic demonstrations. Our core contribution is an augmentation strategy that injects motion into any movable objects within the environment for each demonstration. This process implicitly generates a dense and diverse set of spatial configurations within a single trajectory. We conduct extensive experiments in both simulation and real-world environments to validate our approach. For example, in simulation tasks requiring strong spatial generalization, \\emph{MOVE} achieves an average success rate of 39.1\\%, a 76.1\\% relative improvement over the static data collection paradigm (22.2\\%), and yields up to 2--5$\\times$ gains in data efficiency on certain tasks. Our code is available at https://github.com/lucywang720/MOVE.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04813v1", "score": 6, "tags": ["Vision", "Manipulation", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.04731v1", "type": "papers", "title": "Bridging Simulation and Reality: Cross-Domain Transfer with Semantic 2D Gaussian Splatting", "author": "Jian Tang, Pu Pang, Haowen Sun", "abstract": "Cross-domain transfer in robotic manipulation remains a longstanding challenge due to the significant domain gap between simulated and real-world environments. Existing methods such as domain randomization, adaptation, and sim-real calibration often require extensive tuning or fail to generalize to unseen scenarios. To address this issue, we observe that if domain-invariant features are utilized during policy training in simulation, and the same features can be extracted and provided as the input to policy during real-world deployment, the domain gap can be effectively bridged, leading to significantly improved policy generalization. Accordingly, we propose Semantic 2D Gaussian Splatting (S2GS), a novel representation method that extracts object-centric, domain-invariant spatial features. S2GS constructs multi-view 2D semantic fields and projects them into a unified 3D space via feature-level Gaussian splatting. A semantic filtering mechanism removes irrelevant background content, ensuring clean and consistent inputs for policy learning. To evaluate the effectiveness of S2GS, we adopt Diffusion Policy as the downstream learning algorithm and conduct experiments in the ManiSkill simulation environment, followed by real-world deployment. Results demonstrate that S2GS significantly improves sim-to-real transferability, maintaining high and stable task performance in real-world scenarios.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04731v1", "score": 6, "tags": ["Vision", "Manipulation", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.04686v1", "type": "papers", "title": "Towards Cross-View Point Correspondence in Vision-Language Models", "author": "Yipu Wang, Yuheng Ji, Yuyang Liu", "abstract": "Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of \"perceive\", \"reason\", and \"correspond\". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04686v1", "score": 6, "tags": ["Vision", "Manipulation", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04537v1", "type": "papers", "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale", "author": "Pei Yang, Hai Ci, Yiren Song", "abstract": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04537v1", "score": 6, "tags": ["Vision", "Humanoid", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04515v1", "type": "papers", "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion", "author": "Liuzhou Zhang, Jiarui Ye, Yuanlei Wang", "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04515v1", "score": 6, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "810385004", "type": "projects", "title": "RobotecAI/rai", "author": "RobotecAI", "abstract": "RAI is a vendor-agnostic agentic framework for robotics, utilizing ROS 2 tools to perform complex actions, defined scenarios, free interface execution, log summaries, voice interaction and more.", "date": "2025-12-03", "url": "https://github.com/RobotecAI/rai", "stars": 420, "score": 6, "tags": ["General"], "source": "GitHub"}, {"id": "2512.04779v1", "type": "papers", "title": "YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance", "author": "Junjie Zheng, Chunbo Hao, Guobin Ma", "abstract": "Singing Voice Synthesis (SVS) remains constrained in practical deployment due to its strong dependence on accurate phoneme-level alignment and manually annotated melody contours, requirements that are resource-intensive and hinder scalability. To overcome these limitations, we propose a melody-driven SVS framework capable of synthesizing arbitrary lyrics following any reference melody, without relying on phoneme-level alignment. Our method builds on a Diffusion Transformer (DiT) architecture, enhanced with a dedicated melody extraction module that derives melody representations directly from reference audio. To ensure robust melody encoding, we employ a teacher model to guide the optimization of the melody extractor, alongside an implicit alignment mechanism that enforces similarity distribution constraints for improved melodic stability and coherence. Additionally, we refine duration modeling using weakly annotated song data and introduce a Flow-GRPO reinforcement learning strategy with a multi-objective reward function to jointly enhance pronunciation clarity and melodic fidelity. Experiments show that our model achieves superior performance over existing approaches in both objective measures and subjective listening tests, especially in zero-shot and lyric adaptation settings, while maintaining high audio quality without manual annotation. This work offers a practical and scalable solution for advancing data-efficient singing voice synthesis. To support reproducibility, we release our inference code and model checkpoints.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04779v1", "score": 5, "tags": ["LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "1087207313", "type": "projects", "title": "knightnemo/Awesome-World-Models", "author": "knightnemo", "abstract": "A Curated List of Awesome Works in World Modeling, Aiming to Serve as a One-stop Resource for Researchers, Practitioners, and Enthusiasts Interested in World Modeling.", "date": "2025-12-05", "url": "https://github.com/knightnemo/Awesome-World-Models", "stars": 1320, "score": 5, "tags": ["Reinforcement"], "source": "GitHub"}, {"id": "1019503597", "type": "projects", "title": "ROBOTIS-GIT/robotis_lab", "author": "ROBOTIS-GIT", "abstract": "This repository provides tutorials for reinforcement learning and imitation learning using ROBOTIS robots, and supports Sim2Real functionality for deploying the learned policies on real robots.", "date": "2025-12-04", "url": "https://github.com/ROBOTIS-GIT/robotis_lab", "stars": 75, "score": 5, "tags": ["Reinforcement", "Sim2Real"], "source": "GitHub"}, {"id": "169164391", "type": "projects", "title": "facebookresearch/habitat-lab", "author": "facebookresearch", "abstract": "A modular high-level library to train embodied AI agents across a variety of tasks and environments.", "date": "2025-12-04", "url": "https://github.com/facebookresearch/habitat-lab", "stars": 2713, "score": 5, "tags": ["General"], "source": "GitHub"}, {"id": "169164539", "type": "projects", "title": "facebookresearch/habitat-sim", "author": "facebookresearch", "abstract": "A flexible, high-performance 3D simulator for Embodied AI research.", "date": "2025-12-04", "url": "https://github.com/facebookresearch/habitat-sim", "stars": 3374, "score": 5, "tags": ["Vision"], "source": "GitHub"}, {"id": "128430621", "type": "projects", "title": "OpenTTD/OpenTTD", "author": "OpenTTD", "abstract": "OpenTTD is an open source simulation game based upon Transport Tycoon Deluxe", "date": "2025-12-05", "url": "https://github.com/OpenTTD/OpenTTD", "stars": 7314, "score": 5, "tags": ["Sim2Real"], "source": "GitHub"}, {"id": "18348651", "type": "projects", "title": "OpenRCT2/OpenRCT2", "author": "OpenRCT2", "abstract": "An open source re-implementation of RollerCoaster Tycoon 2 ðŸŽ¢", "date": "2025-12-05", "url": "https://github.com/OpenRCT2/OpenRCT2", "stars": 14868, "score": 5, "tags": ["General"], "source": "GitHub"}, {"id": "71932349", "type": "projects", "title": "ray-project/ray", "author": "ray-project", "abstract": "Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.", "date": "2025-12-05", "url": "https://github.com/ray-project/ray", "stars": 40153, "score": 5, "tags": ["General"], "source": "GitHub"}, {"id": "1004147641", "type": "projects", "title": "microsoft/agent-lightning", "author": "microsoft", "abstract": "The absolute trainer to light up AI agents.", "date": "2025-12-05", "url": "https://github.com/microsoft/agent-lightning", "stars": 9389, "score": 5, "tags": ["General"], "source": "GitHub"}, {"id": "999275152", "type": "projects", "title": "mujocolab/mjlab", "author": "mujocolab", "abstract": "Isaac Lab API, powered by MuJoCo-Warp, for RL and robotics research.", "date": "2025-12-05", "url": "https://github.com/mujocolab/mjlab", "stars": 1111, "score": 5, "tags": ["Reinforcement"], "source": "GitHub"}, {"id": "664913876", "type": "projects", "title": "InternLM/xtuner", "author": "InternLM", "abstract": "A Next-Generation Training Engine Built for Ultra-Large MoE Models", "date": "2025-12-05", "url": "https://github.com/InternLM/xtuner", "stars": 5015, "score": 5, "tags": ["General"], "source": "GitHub"}, {"id": "lerobot/xvla-base", "type": "models", "title": "lerobot/xvla-base", "author": "lerobot", "abstract": "ðŸ”¥ Trending on HF (5 likes). Task: robotics", "date": "None", "url": "https://huggingface.co/lerobot/xvla-base", "stars": 5, "score": 5, "tags": ["safetensors", "xvla", "dataset:unknown", "license:apache-2.0", "robotics", "LLM/VLA", "lerobot", "region:us"], "source": "HuggingFace"}, {"id": "nvidia/GR00T-N1.5-3B", "type": "models", "title": "nvidia/GR00T-N1.5-3B", "author": "nvidia", "abstract": "ðŸ”¥ Trending on HF (165 likes). Task: robotics", "date": "None", "url": "https://huggingface.co/nvidia/GR00T-N1.5-3B", "stars": 165, "score": 5, "tags": ["dataset:nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim", "region:us", "gr00t_n1_5", "safetensors", "General", "arxiv:2410.24164", "robotics", "arxiv:2209.03003", "arxiv:2501.14818"], "source": "HuggingFace"}, {"id": "lerobot/xvla-google-robot", "type": "models", "title": "lerobot/xvla-google-robot", "author": "lerobot", "abstract": "ðŸ”¥ Trending on HF (4 likes). Task: robotics", "date": "None", "url": "https://huggingface.co/lerobot/xvla-google-robot", "stars": 4, "score": 5, "tags": ["safetensors", "xvla", "dataset:unknown", "license:apache-2.0", "robotics", "LLM/VLA", "lerobot", "region:us"], "source": "HuggingFace"}, {"id": "lerobot/xvla-folding", "type": "models", "title": "lerobot/xvla-folding", "author": "lerobot", "abstract": "ðŸ”¥ Trending on HF (4 likes). Task: robotics", "date": "None", "url": "https://huggingface.co/lerobot/xvla-folding", "stars": 4, "score": 5, "tags": ["safetensors", "xvla", "dataset:unknown", "license:apache-2.0", "robotics", "LLM/VLA", "lerobot", "region:us"], "source": "HuggingFace"}, {"id": "Alibaba-DAMO-Academy/WorldVLA", "type": "models", "title": "Alibaba-DAMO-Academy/WorldVLA", "author": "Alibaba-DAMO-Academy", "abstract": "ðŸ”¥ Trending on HF (12 likes). Task: None", "date": "None", "url": "https://huggingface.co/Alibaba-DAMO-Academy/WorldVLA", "stars": 12, "score": 5, "tags": ["base_model:facebook/chameleon-7b", "VLA", "safetensors", "Reinforcement", "arxiv:2506.21539", "Robotics", "license:apache-2.0", "LLM/VLA", "base_model:finetune:facebook/chameleon-7b", "region:us"], "source": "HuggingFace"}, {"id": "IPEC-COMMUNITY/spatialvla-4b-224-pt", "type": "models", "title": "IPEC-COMMUNITY/spatialvla-4b-224-pt", "author": "IPEC-COMMUNITY", "abstract": "ðŸ”¥ Trending on HF (11 likes). Task: image-text-to-text", "date": "None", "url": "https://huggingface.co/IPEC-COMMUNITY/spatialvla-4b-224-pt", "stars": 11, "score": 5, "tags": ["region:us", "arxiv:2501.15830", "custom_code", "VLA", "safetensors", "Generalist Robot Policy", "base_model:finetune:google/paligemma2-3b-pt-224", "license:mit", "image-text-to-text", "spatialvla", "Foundation Vision-language-action Model", "robotics", "base_model:google/paligemma2-3b-pt-224", "LLM/VLA", "feature-extraction", "transformers", "en"], "source": "HuggingFace"}, {"id": "IPEC-COMMUNITY/spatialvla-4b-mix-224-pt", "type": "models", "title": "IPEC-COMMUNITY/spatialvla-4b-mix-224-pt", "author": "IPEC-COMMUNITY", "abstract": "ðŸ”¥ Trending on HF (4 likes). Task: image-text-to-text", "date": "None", "url": "https://huggingface.co/IPEC-COMMUNITY/spatialvla-4b-mix-224-pt", "stars": 4, "score": 5, "tags": ["region:us", "base_model:IPEC-COMMUNITY/spatialvla-4b-224-pt", "arxiv:2501.15830", "custom_code", "VLA", "safetensors", "Generalist Robot Policy", "license:mit", "base_model:finetune:IPEC-COMMUNITY/spatialvla-4b-224-pt", "image-text-to-text", "spatialvla", "Foundation Vision-language-action Model", "robotics", "LLM/VLA", "feature-extraction", "transformers", "en"], "source": "HuggingFace"}, {"id": "IPEC-COMMUNITY/spatialvla-4b-224-sft-bridge", "type": "models", "title": "IPEC-COMMUNITY/spatialvla-4b-224-sft-bridge", "author": "IPEC-COMMUNITY", "abstract": "ðŸ”¥ Trending on HF (1 likes). Task: robotics", "date": "None", "url": "https://huggingface.co/IPEC-COMMUNITY/spatialvla-4b-224-sft-bridge", "stars": 1, "score": 5, "tags": ["region:us", "arxiv:2506.09930", "arxiv:2501.15830", "custom_code", "base_model:IPEC-COMMUNITY/spatialvla-4b-224-pt", "VLA", "safetensors", "Generalist Robot Policy", "license:mit", "base_model:finetune:IPEC-COMMUNITY/spatialvla-4b-224-pt", "spatialvla", "Foundation Vision-language-action Model", "robotics", "LLM/VLA", "feature-extraction", "transformers", "en"], "source": "HuggingFace"}, {"id": "PrimeIntellect/INTELLECT-3", "type": "models", "title": "PrimeIntellect/INTELLECT-3", "author": "PrimeIntellect", "abstract": "ðŸ”¥ Trending on HF (176 likes). Task: text-generation", "date": "None", "url": "https://huggingface.co/PrimeIntellect/INTELLECT-3", "stars": 176, "score": 5, "tags": ["text-generation", "transformers", "agentic", "custom_code", "license:mit", "prime-intellect", "base_model:zai-org/GLM-4.5-Air-Base", "mixture-of-experts", "reasoning", "verifiers", "safetensors", "General", "base_model:finetune:zai-org/GLM-4.5-Air-Base", "reinforcement-learning", "endpoints_compatible", "region:us", "en", "conversational", "prime-rl", "glm4_moe"], "source": "HuggingFace"}, {"id": "bartowski/PrimeIntellect_INTELLECT-3-GGUF", "type": "models", "title": "bartowski/PrimeIntellect_INTELLECT-3-GGUF", "author": "bartowski", "abstract": "ðŸ”¥ Trending on HF (20 likes). Task: text-generation", "date": "None", "url": "https://huggingface.co/bartowski/PrimeIntellect_INTELLECT-3-GGUF", "stars": 20, "score": 5, "tags": ["agentic", "region:us", "prime-rl", "reasoning", "mixture-of-experts", "conversational", "verifiers", "base_model:PrimeIntellect/INTELLECT-3", "imatrix", "General", "license:mit", "gguf", "prime-intellect", "text-generation", "reinforcement-learning", "endpoints_compatible", "base_model:quantized:PrimeIntellect/INTELLECT-3", "en"], "source": "HuggingFace"}, {"id": "PrimeIntellect/INTELLECT-3-FP8", "type": "models", "title": "PrimeIntellect/INTELLECT-3-FP8", "author": "PrimeIntellect", "abstract": "ðŸ”¥ Trending on HF (18 likes). Task: text-generation", "date": "None", "url": "https://huggingface.co/PrimeIntellect/INTELLECT-3-FP8", "stars": 18, "score": 5, "tags": ["text-generation", "transformers", "agentic", "license:mit", "prime-intellect", "base_model:zai-org/GLM-4.5-Air-Base", "mixture-of-experts", "reasoning", "verifiers", "safetensors", "General", "autotrain_compatible", "base_model:quantized:zai-org/GLM-4.5-Air-Base", "compressed-tensors", "reinforcement-learning", "endpoints_compatible", "en", "region:us", "conversational", "prime-rl", "glm4_moe"], "source": "HuggingFace"}, {"id": "AXONVERTEX-AI-RESEARCH/Orchestrator-8B-Q8_0-GGUF", "type": "models", "title": "AXONVERTEX-AI-RESEARCH/Orchestrator-8B-Q8_0-GGUF", "author": "AXONVERTEX-AI-RESEARCH", "abstract": "ðŸ”¥ Trending on HF (7 likes). Task: reinforcement-learning", "date": "None", "url": "https://huggingface.co/AXONVERTEX-AI-RESEARCH/Orchestrator-8B-Q8_0-GGUF", "stars": 7, "score": 5, "tags": ["region:us", "conversational", "tool-calling", "reinforcementlearning", "General", "base_model:quantized:nvidia/Orchestrator-8B", "llama-cpp", "gguf", "gguf-my-repo", "multi-reasoning", "reinforcement-learning", "endpoints_compatible", "base_model:nvidia/Orchestrator-8B", "orchestrator-model", "orchestration", "en"], "source": "HuggingFace"}, {"id": "Adilbai/stock-trading-rl-agent", "type": "models", "title": "Adilbai/stock-trading-rl-agent", "author": "Adilbai", "abstract": "ðŸ”¥ Trending on HF (68 likes). Task: reinforcement-learning", "date": "None", "url": "https://huggingface.co/Adilbai/stock-trading-rl-agent", "stars": 68, "score": 5, "tags": ["finance", "arxiv:1707.06347", "stock-market", "stable-baselines3", "algorithmic-trading", "tensorboard", "financial-ai", "license:mit", "Reinforcement", "model-index", "trading", "portfolio-management", "reinforcement-learning", "dataset:yahoo-finance", "deep-reinforcement-learning", "ppo", "quantitative-finance", "region:us"], "source": "HuggingFace"}, {"id": "microsoft/Phi-4-multimodal-instruct", "type": "models", "title": "microsoft/Phi-4-multimodal-instruct", "author": "microsoft", "abstract": "ðŸ”¥ Trending on HF (1544 likes). Task: automatic-speech-recognition", "date": "None", "url": "https://huggingface.co/microsoft/Phi-4-multimodal-instruct", "stars": 1544, "score": 5, "tags": ["no", "phi-4-mini", "ru", "th", "speech-summarization", "text-generation", "code", "zh", "ja", "transformers", "pt", "phi", "ar", "phi4mm", "custom_code", "tr", "sv", "license:mit", "fi", "fr", "multilingual", "pl", "nl", "uk", "hu", "ko", "safetensors", "arxiv:2407.13833", "General", "autotrain_compatible", "cs", "phi-4-multimodal", "da", "en", "region:us", "speech-translation", "arxiv:2503.01743", "automatic-speech-recognition", "it", "es", "he", "de", "visual-question-answering", "audio", "nlp"], "source": "HuggingFace"}, {"id": "microsoft/Phi-4-multimodal-instruct-onnx", "type": "models", "title": "microsoft/Phi-4-multimodal-instruct-onnx", "author": "microsoft", "abstract": "ðŸ”¥ Trending on HF (81 likes). Task: automatic-speech-recognition", "date": "None", "url": "https://huggingface.co/microsoft/Phi-4-multimodal-instruct-onnx", "stars": 81, "score": 5, "tags": ["phi", "speech-translation", "phi-4-mini", "automatic-speech-recognition", "General", "onnx", "license:mit", "speech-summarization", "code", "visual-question-answering", "audio", "multilingual", "phi-4-multimodal", "nlp", "region:us"], "source": "HuggingFace"}, {"id": "google/cxr-foundation", "type": "models", "title": "google/cxr-foundation", "author": "google", "abstract": "ðŸ”¥ Trending on HF (94 likes). Task: image-classification", "date": "None", "url": "https://huggingface.co/google/cxr-foundation", "stars": 94, "score": 5, "tags": ["arxiv:2103.00020", "medical-embeddings", "arxiv:1911.04252", "zero-shot-image-classification", "arxiv:2308.01317", "arxiv:1810.04805", "arxiv:2004.11362", "license:other", "General", "arxiv:2301.12597", "image-feature-extraction", "image-text-to-text", "en", "region:us", "x-ray", "image-classification", "medical", "chest-x-ray", "visual-question-answering", "cxr-foundation", "tf-keras"], "source": "HuggingFace"}, {"id": "dandelin/vilt-b32-finetuned-vqa", "type": "models", "title": "dandelin/vilt-b32-finetuned-vqa", "author": "dandelin", "abstract": "ðŸ”¥ Trending on HF (416 likes). Task: visual-question-answering", "date": "None", "url": "https://huggingface.co/dandelin/vilt-b32-finetuned-vqa", "stars": 416, "score": 5, "tags": ["region:us", "pytorch", "General", "vilt", "visual-question-answering", "license:apache-2.0", "endpoints_compatible", "transformers", "arxiv:2102.03334"], "source": "HuggingFace"}, {"id": "google/pix2struct-docvqa-base", "type": "models", "title": "google/pix2struct-docvqa-base", "author": "google", "abstract": "ðŸ”¥ Trending on HF (40 likes). Task: visual-question-answering", "date": "None", "url": "https://huggingface.co/google/pix2struct-docvqa-base", "stars": 40, "score": 5, "tags": ["region:us", "pix2struct", "pytorch", "safetensors", "General", "arxiv:2210.03347", "de", "image-to-text", "visual-question-answering", "fr", "license:apache-2.0", "multilingual", "ro", "transformers", "en"], "source": "HuggingFace"}, {"id": "tencent/HunyuanVideo-1.5", "type": "models", "title": "tencent/HunyuanVideo-1.5", "author": "tencent", "abstract": "ðŸ”¥ Trending on HF (796 likes). Task: text-to-video", "date": "None", "url": "https://huggingface.co/tencent/HunyuanVideo-1.5", "stars": 796, "score": 5, "tags": ["arxiv:2511.18870", "license:other", "region:us", "safetensors", "General", "HunyuanVideo-1.5", "image-to-video", "zh", "text-to-video", "en", "diffusers"], "source": "HuggingFace"}, {"id": "lightx2v/Wan2.2-Lightning", "type": "models", "title": "lightx2v/Wan2.2-Lightning", "author": "lightx2v", "abstract": "ðŸ”¥ Trending on HF (551 likes). Task: text-to-video", "date": "None", "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning", "stars": 551, "score": 5, "tags": ["region:us", "safetensors", "General", "video-generation;", "arxiv:2309.14509", "base_model:Wan-AI/Wan2.2-I2V-A14B", "image-to-video;", "base_model:finetune:Wan-AI/Wan2.2-I2V-A14B", "license:apache-2.0", "text-to-video", "en", "comfyUI;", "text-to-video;", "diffusers"], "source": "HuggingFace"}, {"id": "Phr00t/HunyuanVideo-1.5-Rapid-AIO", "type": "models", "title": "Phr00t/HunyuanVideo-1.5-Rapid-AIO", "author": "Phr00t", "abstract": "ðŸ”¥ Trending on HF (47 likes). Task: text-to-video", "date": "None", "url": "https://huggingface.co/Phr00t/HunyuanVideo-1.5-Rapid-AIO", "stars": 47, "score": 5, "tags": ["region:us", "license:other", "t2v", "aio", "General", "base_model:tencent/HunyuanVideo-1.5", "base_model:finetune:tencent/HunyuanVideo-1.5", "rapid", "text-to-video", "i2v", "hunyuan"], "source": "HuggingFace"}, {"id": "meituan-longcat/LongCat-Video", "type": "models", "title": "meituan-longcat/LongCat-Video", "author": "meituan-longcat", "abstract": "ðŸ”¥ Trending on HF (354 likes). Task: text-to-video", "date": "None", "url": "https://huggingface.co/meituan-longcat/LongCat-Video", "stars": 354, "score": 5, "tags": ["region:us", "safetensors", "General", "license:mit", "image-to-video", "video-continuation", "zh", "arxiv:2510.22200", "text-to-video", "en", "transformers", "diffusers"], "source": "HuggingFace"}, {"id": "Wan-AI/Wan2.2-TI2V-5B", "type": "models", "title": "Wan-AI/Wan2.2-TI2V-5B", "author": "Wan-AI", "abstract": "ðŸ”¥ Trending on HF (453 likes). Task: text-to-video", "date": "None", "url": "https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B", "stars": 453, "score": 5, "tags": ["region:us", "safetensors", "General", "ti2v", "wan2.2", "zh", "license:apache-2.0", "text-to-video", "en", "arxiv:2503.20314", "diffusers"], "source": "HuggingFace"}, {"id": "2512.05111v1", "type": "papers", "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning", "author": "Shengyuan Ding, Xinyu Fang, Ziyu Liu", "abstract": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05111v1", "score": 4, "tags": ["Vision", "Reinforcement"], "source": "arXiv"}, {"id": "2512.05107v1", "type": "papers", "title": "STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models", "author": "Feng Xu, Guangyao Zhai, Xin Kong", "abstract": "Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -> Preference -> Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05107v1", "score": 4, "tags": ["Vision", "Manipulation", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.05103v1", "type": "papers", "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation", "author": "Xiaochuang Han, Youssef Emad, Melissa Hall", "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05103v1", "score": 4, "tags": ["Vision", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.05094v1", "type": "papers", "title": "From Generated Human Videos to Physically Plausible Robot Trajectories", "author": "James Ni, Zekai Wang, Wei Lin", "abstract": "Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05094v1", "score": 4, "tags": ["Vision", "Humanoid", "Sim2Real", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04958v1", "type": "papers", "title": "Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning", "author": "Roberto Cipollone, Luca Iocchi, Matteo Leonetti", "abstract": "The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04958v1", "score": 4, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04949v1", "type": "papers", "title": "CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent", "author": "Leyang Shen, Yang Zhang, Chun Kai Ling", "abstract": "Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for multi-step agents. CARL achieves focused training through providing action-level optimization signals for high-criticality actions while excluding low-criticality actions from model update. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency during training and inference across diverse evaluation settings.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04949v1", "score": 4, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04773v1", "type": "papers", "title": "Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions", "author": "Giorgos Polychronis, Foivos Pournaropoulos, Christos D. Antonopoulos", "abstract": "Drones are becoming indispensable in many application domains. In data-driven missions, besides sensing, the drone must process the collected data at runtime to decide whether additional action must be taken on the spot, before moving to the next point of interest. If processing does not reveal an event or situation that requires such an action, the drone has waited in vain instead of moving to the next point. If, however, the drone starts moving to the next point and it turns out that a follow-up action is needed at the previous point, it must spend time to fly-back. To take this decision, we propose different machine-learning methods based on branch prediction and reinforcement learning. We evaluate these methods for a wide range of scenarios where the probability of event occurrence changes with time. Our results show that the proposed methods consistently outperform the regression-based method proposed in the literature and can significantly improve the worst-case mission time by up to 4.1x. Also, the achieved median mission time is very close, merely up to 2.7% higher, to that of a method with perfect knowledge of the current underlying event probability at each point of interest.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04773v1", "score": 4, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04552v1", "type": "papers", "title": "RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS", "author": "Cong Wang, Changfeng Gao, Yang Xiang", "abstract": "Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: https://lrwinr.github.io/RRPO-CosyVoice.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04552v1", "score": 4, "tags": ["Vision", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04463v1", "type": "papers", "title": "MARL Warehouse Robots", "author": "Price Allman, Lian Thang, Dre Simmons", "abstract": "We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04463v1", "score": 4, "tags": ["Vision", "LLM/VLA", "Sim2Real", "Reinforcement"], "source": "arXiv"}, {"id": "1063851187", "type": "projects", "title": "2toinf/X-VLA", "author": "2toinf", "abstract": "The offical Implementation of \"Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model\"", "date": "2025-12-05", "url": "https://github.com/2toinf/X-VLA", "stars": 302, "score": 4, "tags": ["Vision", "LLM/VLA"], "source": "GitHub"}, {"id": "2512.05112v1", "type": "papers", "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation", "author": "Dongzhi Jiang, Renrui Zhang, Haodong Li", "abstract": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05112v1", "score": 3, "tags": ["LLM/VLA", "Manipulation", "Reinforcement"], "source": "arXiv"}, {"id": "2512.05089v1", "type": "papers", "title": "The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception", "author": "Eduardo Di Santi", "abstract": "Real-world physical processes do not generate arbitrary variability: their signals concentrate on compact and low-variability subsets of functional space. This geometric structure enables rapid generalization from a few examples in both biological and artificial systems.   This work develops a deterministic functional-topological framework in which the set of valid realizations of a physical phenomenon forms a compact perceptual manifold with stable invariants and a finite Hausdorff radius. We show that the boundaries of this manifold can be discovered in a fully self-supervised manner through Monte Carlo sampling, even when the governing equations of the system are unknown.   We provide theoretical guarantees, practical estimators of knowledge boundaries, and empirical validations across three domains: electromechanical railway point machines, electrochemical battery discharge curves, and physiological ECG signals.   Our results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction, explaining why biological learners and self-supervised AI models can generalize from limited observations.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05089v1", "score": 3, "tags": ["Vision", "Reinforcement"], "source": "arXiv"}, {"id": "2512.05079v1", "type": "papers", "title": "Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints", "author": "Minghan Zhu, Zhiyi Wang, Qihang Sun", "abstract": "Object geometry is key information for robot manipulation. Yet, object reconstruction is a challenging task because cameras only capture partial observations of objects, especially when occlusion occurs. In this paper, we leverage two extra sources of information to reduce the ambiguity of vision signals. First, generative models learn priors of the shapes of commonly seen objects, allowing us to make reasonable guesses of the unseen part of geometry. Second, contact information, which can be obtained from videos and physical interactions, provides sparse constraints on the boundary of the geometry. We combine the two sources of information through contact-guided 3D generation. The guidance formulation is inspired by drag-based editing in generative models. Experiments on synthetic and real-world data show that our approach improves the reconstruction compared to pure 3D generation and contact-based optimization.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05079v1", "score": 3, "tags": ["Vision", "Manipulation", "Reinforcement"], "source": "arXiv"}, {"id": "2512.05066v1", "type": "papers", "title": "Multi-LLM Collaboration for Medication Recommendation", "author": "Huascar Sanchez, Briland Hitaj, Jules Bergmann", "abstract": "As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05066v1", "score": 3, "tags": ["LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.05049v1", "type": "papers", "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory", "author": "Yu-Chao Hsu, Jiun-Cheng Jiang, Chun-Hua Lin", "abstract": "Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05049v1", "score": 3, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.05045v1", "type": "papers", "title": "On random matrix statistics of 3d gravity", "author": "Daniel L. Jafferis, Liza Rozenberg, Debmalya Sarkar", "abstract": "We show that 3d gravity on manifolds that are topologically a Riemann surface times an interval $Î£_{g,n}\\times I$ with end-of-the-world branes at the ends of the interval is described by a random matrix model, namely the Virasoro minimal string. Because these manifolds have $n$ annular asymptotic boundaries, the path integrals naturally correspond to spectral correlators of open strings upon inverse Fourier transforms. For $g=0$ and $n=2$, we carry out an explicit path integration and find precise agreement with the universal random matrix expression. For Riemann surfaces with negative Euler characteristic, we evaluate the path integral as a gravitational inner product between states prepared by two copies of Virasoro TQFT. Along the way, we clarify the effects of gauging the mapping class group and the connection to chiral 3d gravity.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05045v1", "score": 3, "tags": ["Vision", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04988v1", "type": "papers", "title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets", "author": "Christopher Chiu, Simpson Zhang, Mihaela van der Schaar", "abstract": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04988v1", "score": 3, "tags": ["LLM/VLA", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.04987v1", "type": "papers", "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction", "author": "Nex-AGI Team,  :, Yuxuan Cai", "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04987v1", "score": 3, "tags": ["LLM/VLA", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.04976v1", "type": "papers", "title": "Multipole decomposition of the gravitational field of a point mass at the black hole horizon", "author": "JoÃ£o P. B. Brito, Atsushi Higuchi, LuÃ­s C. B. Crispino", "abstract": "The portion of the gravitational energy absorbed by the black hole due to the radial infall of a point mass is known to diverge at leading order in perturbation theory. This divergence is an artifact of the point-particle model, where the contribution of each multipole to the total absorbed energy is observed to be roughly constant. We show explicitly that this divergent energy arises from the infinite energy present in the singular static field arbitrarily close to the point mass, which also flows into the black hole when the particle trajectory crosses the horizon. We perform a multipole decomposition of the linearized gravitational field generated by the point mass near its world line at the black hole horizon. By applying the standard field-theoretical approach to the particle field, we compute the corresponding partial energy and find that it matches the constant multipole contribution.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04976v1", "score": 3, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04952v1", "type": "papers", "title": "FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization", "author": "Yicheng Liu, Shiduo Zhang, Zibin Dong", "abstract": "Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04952v1", "score": 3, "tags": ["Vision", "Manipulation", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04945v1", "type": "papers", "title": "TripleC Learning and Lightweight Speech Enhancement for Multi-Condition Target Speech Extraction", "author": "Ziling Huang", "abstract": "In our recent work, we proposed Lightweight Speech Enhancement Guided Target Speech Extraction (LGTSE) and demonstrated its effectiveness in multi-speaker-plus-noise scenarios. However, real-world applications often involve more diverse and complex conditions, such as one-speaker-plus-noise or two-speaker-without-noise. To address this challenge, we extend LGTSE with a Cross-Condition Consistency learning strategy, termed TripleC Learning. This strategy is first validated under multi-speaker-plus-noise condition and then evaluated for its generalization across diverse scenarios. Moreover, building upon the lightweight front-end denoiser in LGTSE, which can flexibly process both noisy and clean mixtures and shows strong generalization to unseen conditions, we integrate TripleC learning with a proposed parallel universal training scheme that organizes batches containing multiple scenarios for the same target speaker. By enforcing consistent extraction across different conditions, easier cases can assist harder ones, thereby fully exploiting diverse training data and fostering a robust universal model. Experimental results on the Libri2Mix three-condition tasks demonstrate that the proposed LGTSE with TripleC learning achieves superior performance over condition-specific models, highlighting its strong potential for universal deployment in real-world speech applications.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04945v1", "score": 3, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04883v1", "type": "papers", "title": "SDG-Track: A Heterogeneous Observer-Follower Framework for High-Resolution UAV Tracking on Embedded Platforms", "author": "Jiawen Wen, Yu Hu, Suixuan Qiu", "abstract": "Real-time tracking of small unmanned aerial vehicles (UAVs) on edge devices faces a fundamental resolution-speed conflict. Downsampling high-resolution imagery to standard detector input sizes causes small target features to collapse below detectable thresholds. Yet processing native 1080p frames on resource-constrained platforms yields insufficient throughput for smooth gimbal control. We propose SDG-Track, a Sparse Detection-Guided Tracker that adopts an Observer-Follower architecture to reconcile this conflict. The Observer stream runs a high-capacity detector at low frequency on the GPU to provide accurate position anchors from 1920x1080 frames. The Follower stream performs high-frequency trajectory interpolation via ROI-constrained sparse optical flow on the CPU. To handle tracking failures from occlusion or model drift caused by spectrally similar distractors, we introduce Dual-Space Recovery, a training-free re-acquisition mechanism combining color histogram matching with geometric consistency constraints. Experiments on a ground-to-air tracking station demonstrate that SDG-Track achieves 35.1 FPS system throughput while retaining 97.2\\% of the frame-by-frame detection precision. The system successfully tracks agile FPV drones under real-world operational conditions on an NVIDIA Jetson Orin Nano. Our paper code is publicly available at https://github.com/Jeffry-wen/SDG-Track", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04883v1", "score": 3, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04831v1", "type": "papers", "title": "Clustering country-level all-cause mortality data: a review", "author": "Pedro Menezes de Araujo, Isobel Claire Gormley, Thomas Brendan Murphy", "abstract": "Mortality data are relevant to demography, public health, and actuarial science. Whilst clustering is increasingly used to explore patterns in such data, no study has reviewed its application to country-level all-cause mortality. This review therefore summarises recent work and addresses key questions: why clustering is used, which mortality data are analysed, which methods are most common, and what main findings emerge. To address these questions, we examine studies applying clustering to country-level all-cause mortality, focusing on mortality indices, data sources, and methodological choices, and we replicate some approaches using Human Mortality Database (HMD) data. Our analysis reveals that clustering is mainly motivated by forecasting and by studying convergence and inequality. Most studies use HMD data from developed countries and rely on k-means, hierarchical, or functional clustering. Main findings include a persistent East-West European division across applications, with clustering generally improving forecast accuracy over single-country models. Overall, this review highlights the methodological range in the literature, summarises clustering results, and identifies gaps, such as the limited evaluation of clustering quality and the underuse of data from countries outside the high-income world.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04831v1", "score": 3, "tags": ["Vision", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04800v1", "type": "papers", "title": "Time-periodic solutions to an energy balance model coupled with an active fluid under arbitrary large forces", "author": "Gianmarco Del Sarto, Matthias Hieber, Filippo Palma", "abstract": "This article concerns time-periodic solutions to a two-dimensional Sellers-type energy balance model coupled to the three-dimensional primitive equations via a dynamic boundary condition. It is shown that the underlying equations admit at least one strong time-periodic solution, provided the forcing term is time-periodic. The forcing term does not need to satisfy a smallness condition and is allowed to be arbitrarily large.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04800v1", "score": 3, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04797v1", "type": "papers", "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds", "author": " SIMA team, Adrian Bolton, Alexander Lerchner", "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04797v1", "score": 3, "tags": ["Vision", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04785v1", "type": "papers", "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications", "author": "Eranga Bandara, Amin Hass, Ross Gore", "abstract": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04785v1", "score": 3, "tags": ["Vision", "Manipulation", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.04753v1", "type": "papers", "title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing", "author": "Ruilin Li, Yibin Wang, Wenhong Zhu", "abstract": "Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04753v1", "score": 3, "tags": ["LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04733v1", "type": "papers", "title": "E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving", "author": "Yihong Tang, Haicheng Liao, Tong Nie", "abstract": "End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04733v1", "score": 3, "tags": ["Vision", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04699v1", "type": "papers", "title": "OmniScaleSR: Unleashing Scale-Controlled Diffusion Prior for Faithful and Realistic Arbitrary-Scale Image Super-Resolution", "author": "Xinning Chai, Zhengxue Cheng, Yuhong Zhang", "abstract": "Arbitrary-scale super-resolution (ASSR) overcomes the limitation of traditional super-resolution (SR) methods that operate only at fixed scales (e.g., 4x), enabling a single model to handle arbitrary magnification. Most existing ASSR approaches rely on implicit neural representation (INR), but its regression-driven feature extraction and aggregation intrinsically limit the ability to synthesize fine details, leading to low realism. Recent diffusion-based realistic image super-resolution (Real-ISR) models leverage powerful pre-trained diffusion priors and show impressive results at the 4x setting. We observe that they can also achieve ASSR because the diffusion prior implicitly adapts to scale by encouraging high-realism generation. However, without explicit scale control, the diffusion process cannot be properly adjusted for different magnification levels, resulting in excessive hallucination or blurry outputs, especially under ultra-high scales. To address these issues, we propose OmniScaleSR, a diffusion-based realistic arbitrary-scale SR framework designed to achieve both high fidelity and high realism. We introduce explicit, diffusion-native scale control mechanisms that work synergistically with implicit scale adaptation, enabling scale-aware and content-aware modulation of the diffusion process. In addition, we incorporate multi-domain fidelity enhancement designs to further improve reconstruction accuracy. Extensive experiments on bicubic degradation benchmarks and real-world datasets show that OmniScaleSR surpasses state-of-the-art methods in both fidelity and perceptual realism, with particularly strong performance at large magnification factors. Code will be released at https://github.com/chaixinning/OmniScaleSR.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04699v1", "score": 3, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04678v1", "type": "papers", "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation", "author": "Yunhong Lu, Yanhong Zeng, Haobo Li", "abstract": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04678v1", "score": 3, "tags": ["Vision", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04596v1", "type": "papers", "title": "QoSDiff: An Implicit Topological Embedding Learning Framework Leveraging Denoising Diffusion and Adversarial Attention for Robust QoS Prediction", "author": "Guanchen Du, Jianlong Xu, Wei Wei", "abstract": "Accurate Quality of Service (QoS) prediction is fundamental to service computing, providing essential data-driven guidance for service selection and ensuring superior user experiences. However, prevalent approaches, particularly Graph Neural Networks (GNNs), heavily rely on constructing explicit user--service interaction graphs. This dependency introduces severe scalability bottlenecks and limits performance when explicit connections are sparse or corrupted by noise. To address these challenges, this paper introduces \\emph{QoSDiff}, a novel embedding learning framework that bypasses the prerequisite of explicit graph construction. Specifically, it leverages a denoising diffusion probabilistic model to recover intrinsic latent structures from noisy initializations. To further capture high-order interactions, we propose an adversarial interaction module that integrates a bidirectional hybrid attention mechanism. This adversarial paradigm dynamically distinguishes informative patterns from noise, enabling a dual-perspective modeling of intricate user--service associations. Extensive experiments on two large-scale real-world datasets demonstrate that QoSDiff significantly outperforms state-of-the-art baselines. Notably, the results highlight the framework's superior cross-dataset generalization capability and exceptional robustness against data sparsity and observational noise.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04596v1", "score": 3, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04585v1", "type": "papers", "title": "SAM3-I: Segment Anything with Instructions", "author": "Jingjing Li, Yue Feng, Yuchen Guo", "abstract": "Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04585v1", "score": 3, "tags": ["Vision", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04513v1", "type": "papers", "title": "BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models", "author": "Yu-Wei Zhan, Xin Wang, Pengzhe Mao", "abstract": "Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04513v1", "score": 3, "tags": ["LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04480v1", "type": "papers", "title": "AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Substituitions", "author": "Pedro Passos", "abstract": "In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the \"FAGNER Paradox\" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the \"Lukaku Paradox\", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04480v1", "score": 3, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "1075128236", "type": "projects", "title": "wadeKeith/DeepThinkVLA", "author": "wadeKeith", "abstract": "DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models", "date": "2025-12-05", "url": "https://github.com/wadeKeith/DeepThinkVLA", "stars": 323, "score": 3, "tags": ["Vision", "LLM/VLA"], "source": "GitHub"}, {"id": "1078093679", "type": "projects", "title": "softmata/horus", "author": "softmata", "abstract": "Fastest Robotics Runtime System.  If phones have Android, robots deserve HORUS.", "date": "2025-12-05", "url": "https://github.com/softmata/horus", "stars": 43, "score": 3, "tags": ["General"], "source": "GitHub"}, {"id": "1037741738", "type": "projects", "title": "RLinf/RLinf", "author": "RLinf", "abstract": "RLinf is a flexible and scalable open-source infrastructure designed for post-training foundation models (LLMs, VLMs, VLAs) via reinforcement learning.", "date": "2025-12-05", "url": "https://github.com/RLinf/RLinf", "stars": 1563, "score": 3, "tags": ["LLM/VLA", "Reinforcement"], "source": "GitHub"}, {"id": "885743839", "type": "projects", "title": "leofan90/Awesome-World-Models", "author": "leofan90", "abstract": "A comprehensive list of papers for the definition of World Models and using World Models for General Video Generation, Embodied AI, and Autonomous Driving, including papers, codes, and related websites.", "date": "2025-12-05", "url": "https://github.com/leofan90/Awesome-World-Models", "stars": 859, "score": 3, "tags": ["Reinforcement"], "source": "GitHub"}, {"id": "790280061", "type": "projects", "title": "rh20624/Awesome-IMU-Sensing", "author": "rh20624", "abstract": "A collection of datasets, papers, and resources  for Generalizable Human Activity Recognition and IMU sensing.", "date": "2025-12-04", "url": "https://github.com/rh20624/Awesome-IMU-Sensing", "stars": 123, "score": 3, "tags": ["General"], "source": "GitHub"}, {"id": "988069032", "type": "projects", "title": "philfung/awesome-reliable-robotics", "author": "philfung", "abstract": "Robotics research demonstrating reliability and robustness in the real world (continuously updated)", "date": "2025-12-05", "url": "https://github.com/philfung/awesome-reliable-robotics", "stars": 114, "score": 3, "tags": ["Reinforcement"], "source": "GitHub"}, {"id": "794915217", "type": "projects", "title": "CroboticSolutions/arm_api2", "author": "CroboticSolutions", "abstract": "Code wrapper for MoveIt2! and ROS 2.", "date": "2025-12-02", "url": "https://github.com/CroboticSolutions/arm_api2", "stars": 84, "score": 3, "tags": ["General"], "source": "GitHub"}, {"id": "970597960", "type": "projects", "title": "InternRobotics/GenManip", "author": "InternRobotics", "abstract": "[CVPR 2025] Official implementation of \"GenManip: LLM-driven Simulation for Generalizable Instruction-Following Manipulation\"", "date": "2025-12-04", "url": "https://github.com/InternRobotics/GenManip", "stars": 115, "score": 3, "tags": ["LLM/VLA", "Manipulation", "Sim2Real"], "source": "GitHub"}, {"id": "1077985558", "type": "projects", "title": "Dexmal/dexbotic", "author": "Dexmal", "abstract": "Dexbotic: Open-Source Vision-Language-Action Toolbox", "date": "2025-12-05", "url": "https://github.com/Dexmal/dexbotic", "stars": 553, "score": 3, "tags": ["Vision"], "source": "GitHub"}, {"id": "914796651", "type": "projects", "title": "phospho-app/phosphobot", "author": "phospho-app", "abstract": "Control AI robots. Community-driven UI middleware for controlling robots, recording datasets, training action models. Compatible with SO-100 and SO-101 ", "date": "2025-12-04", "url": "https://github.com/phospho-app/phosphobot", "stars": 317, "score": 3, "tags": ["General"], "source": "GitHub"}, {"id": "914053359", "type": "projects", "title": "OpenMind/OM1", "author": "OpenMind", "abstract": "Modular AI runtime for robots", "date": "2025-12-05", "url": "https://github.com/OpenMind/OM1", "stars": 2198, "score": 2, "tags": ["General"], "source": "GitHub"}, {"id": "460615268", "type": "projects", "title": "dora-rs/dora", "author": "dora-rs", "abstract": "DORA (Dataflow-Oriented Robotic Architecture) is middleware designed to streamline and simplify the creation of AI-based robotic applications. It offers low latency, composable, and distributed dataflow capabilities. Applications are modeled as directed graphs, also referred to as pipelines.", "date": "2025-12-05", "url": "https://github.com/dora-rs/dora", "stars": 2718, "score": 2, "tags": ["General"], "source": "GitHub"}, {"id": "52715040", "type": "projects", "title": "shenwei356/seqkit", "author": "shenwei356", "abstract": "A cross-platform and ultrafast toolkit for FASTA/Q file manipulation", "date": "2025-12-04", "url": "https://github.com/shenwei356/seqkit", "stars": 1488, "score": 2, "tags": ["Manipulation"], "source": "GitHub"}, {"id": "937952934", "type": "projects", "title": "inclusionAI/AReaL", "author": "inclusionAI", "abstract": "Lightning-Fast RL for LLM Reasoning and Agents. Made Simple & Flexible.", "date": "2025-12-05", "url": "https://github.com/inclusionAI/AReaL", "stars": 3113, "score": 2, "tags": ["LLM/VLA", "Reinforcement"], "source": "GitHub"}, {"id": "2512.05105v1", "type": "papers", "title": "Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning", "author": "Purbesh Mitra, Sennur Ulukus", "abstract": "Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \\textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05105v1", "score": 1, "tags": ["LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.05100v1", "type": "papers", "title": "Structured Document Translation via Format Reinforcement Learning", "author": "Haiyue Song, Johannes Eschbach-Dymanus, Hour Kaing", "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05100v1", "score": 1, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.05098v1", "type": "papers", "title": "SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards", "author": "Yuan Gao, Jin Song", "abstract": "In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05098v1", "score": 1, "tags": ["LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04969v1", "type": "papers", "title": "Rethinking the Use of Vision Transformers for AI-Generated Image Detection", "author": "NaHyeon Park, Kunhee Kim, Junsuk Choe", "abstract": "Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04969v1", "score": 1, "tags": ["Vision", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04793v1", "type": "papers", "title": "YingMusic-SVC: Real-World Robust Zero-Shot Singing Voice Conversion with Flow-GRPO and Singing-Specific Inductive Biases", "author": "Gongyu Chen, Xiaoyu Zhang, Zhenqiang Weng", "abstract": "Singing voice conversion (SVC) aims to render the target singer's timbre while preserving melody and lyrics. However, existing zero-shot SVC systems remain fragile in real songs due to harmony interference, F0 errors, and the lack of inductive biases for singing. We propose YingMusic-SVC, a robust zero-shot framework that unifies continuous pre-training, robust supervised fine-tuning, and Flow-GRPO reinforcement learning. Our model introduces a singing-trained RVC timbre shifter for timbre-content disentanglement, an F0-aware timbre adaptor for dynamic vocal expression, and an energy-balanced rectified flow matching loss to enhance high-frequency fidelity. Experiments on a graded multi-track benchmark show that YingMusic-SVC achieves consistent improvements over strong open-source baselines in timbre similarity, intelligibility, and perceptual naturalness, especially under accompanied and harmony-contaminated conditions, demonstrating its effectiveness for real-world SVC deployment.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04793v1", "score": 1, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04653v1", "type": "papers", "title": "Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control", "author": "Pouria Yazdani, Arash Rezaali, Monireh Abdoos", "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04653v1", "score": 1, "tags": ["Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.04580v1", "type": "papers", "title": "A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution", "author": "Huifeng Zhu, Shijie Li, Qinfeng Li", "abstract": "To enhance the performance of large language models (LLMs) in various domain-specific applications, sensitive data such as healthcare, law, and finance are being used to privately customize or fine-tune these models. Such privately adapted LLMs are regarded as either personal privacy assets or corporate intellectual property. Therefore, protecting model weights and maintaining strict confidentiality during deployment and distribution have become critically important. However, existing model formats and deployment frameworks provide little to no built-in support for confidentiality, access control, or secure integration with trusted hardware. Current methods for securing model deployment either rely on computationally expensive cryptographic techniques or tightly controlled private infrastructure. Although these approaches can be effective in specific scenarios, they are difficult and costly for widespread deployment.   In this paper, we introduce CryptoTensors, a secure and format-compatible file structure for confidential LLM distribution. Built as an extension to the widely adopted Safetensors format, CryptoTensors incorporates tensor-level encryption and embedded access control policies, while preserving critical features such as lazy loading and partial deserialization. It enables transparent decryption and automated key management, supporting flexible licensing and secure model execution with minimal overhead. We implement a proof-of-concept library, benchmark its performance across serialization and runtime scenarios, and validate its compatibility with existing inference frameworks, including Hugging Face Transformers and vLLM. Our results highlight CryptoTensors as a light-weight, efficient, and developer-friendly solution for safeguarding LLM weights in real-world and widespread deployments.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04580v1", "score": 1, "tags": ["LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04563v1", "type": "papers", "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence", "author": "Zefeng Zhang, Xiangzhao Hao, Hengzhu Tang", "abstract": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \\textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \\textbf{6.91\\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \\textbf{7.92\\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04563v1", "score": 1, "tags": ["Vision", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "992630802", "type": "projects", "title": "ReinFlow/ReinFlow", "author": "ReinFlow", "abstract": "[NeurIPS 2025] Flow x RL. \"ReinFlow: Fine-tuning Flow Policy with Online Reinforcement Learning\". Support VLAs e.g., pi0, pi0.5. Fully open-sourced. ", "date": "2025-12-03", "url": "https://github.com/ReinFlow/ReinFlow", "stars": 185, "score": 1, "tags": ["LLM/VLA", "Reinforcement"], "source": "GitHub"}, {"id": "915137783", "type": "projects", "title": "LongchaoDa/AwesomeSim2Real", "author": "LongchaoDa", "abstract": "AwesomeSim2Real - An update-to-date Sim-to-Real repo of \"Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models\"", "date": "2025-12-01", "url": "https://github.com/LongchaoDa/AwesomeSim2Real", "stars": 110, "score": 1, "tags": ["LLM/VLA", "Reinforcement", "Sim2Real"], "source": "GitHub"}, {"id": "753968001", "type": "projects", "title": "Tinker-Twins/AutoDRIVE-Coopertitive-MARL", "author": "Tinker-Twins", "abstract": "Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles", "date": "2025-10-28", "url": "https://github.com/Tinker-Twins/AutoDRIVE-Coopertitive-MARL", "stars": 24, "score": 1, "tags": ["Reinforcement"], "source": "GitHub"}, {"id": "604105885", "type": "projects", "title": "AqwamCreates/DataPredict", "author": "AqwamCreates", "abstract": "Lua-Based Machine Learning, Deep Learning And Reinforcement Learning Library (For Roblox And Pure Lua). Contains Over 85 Models!", "date": "2025-12-05", "url": "https://github.com/AqwamCreates/DataPredict", "stars": 11, "score": 1, "tags": ["Reinforcement"], "source": "GitHub"}, {"id": "2512.05115v1", "type": "papers", "title": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "author": "Tianqi Liu, Zhaoxi Chen, Zihao Huang", "abstract": "Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05115v1", "score": 0, "tags": ["Vision", "Reinforcement"], "source": "arXiv"}, {"id": "2512.05076v1", "type": "papers", "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation", "author": "Yiming Wang, Qihang Zhang, Shengqu Cai", "abstract": "Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05076v1", "score": 0, "tags": ["Vision", "Manipulation", "Reinforcement"], "source": "arXiv"}, {"id": "2512.04885v1", "type": "papers", "title": "Stability-Guaranteed Dual Kalman Filtering for Electrochemical Battery State Estimation", "author": "Feng Guo, Guangdi Hu, Keyi Liao", "abstract": "Accurate and stable state estimation is critical for battery management. Although dual Kalman filtering can jointly estimate states and parameters, the strong coupling between filters may cause divergence under large initialization errors or model mismatch. This paper proposes a Stability Guaranteed Dual Kalman Filtering (SG-DKF) method. A Lyapunov-based analysis yields a sufficient stability condition, leading to an adaptive dead-zone rule that suspends parameter updates when the innovation exceeds a stability bound. Applied to an electrochemical battery model, SG-DKF achieves accuracy comparable to a dual EKF and reduces state of charge RMSE by over 45% under large initial state errors.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04885v1", "score": 0, "tags": ["General"], "source": "arXiv"}, {"id": "2512.04837v1", "type": "papers", "title": "A Sanity Check for Multi-In-Domain Face Forgery Detection in the Real World", "author": "Jikang Cheng, Renye Yan, Zhiyuan Yan", "abstract": "Existing methods for deepfake detection aim to develop generalizable detectors. Although \"generalizable\" is the ultimate target once and for all, with limited training forgeries and domains, it appears idealistic to expect generalization that covers entirely unseen variations, especially given the diversity of real-world deepfakes. Therefore, introducing large-scale multi-domain data for training can be feasible and important for real-world applications. However, within such a multi-domain scenario, the differences between multiple domains, rather than the subtle real/fake distinctions, dominate the feature space. As a result, despite detectors being able to relatively separate real and fake within each domain (i.e., high AUC), they struggle with single-image real/fake judgments in domain-unspecified conditions (i.e., low ACC). In this paper, we first define a new research paradigm named Multi-In-Domain Face Forgery Detection (MID-FFD), which includes sufficient volumes of real-fake domains for training. Then, the detector should provide definitive real-fake judgments to the domain-unspecified inputs, which simulate the frame-by-frame independent detection scenario in the real world. Meanwhile, to address the domain-dominant issue, we propose a model-agnostic framework termed DevDet (Developer for Detector) to amplify real/fake differences and make them dominant in the feature space. DevDet consists of a Face Forgery Developer (FFDev) and a Dose-Adaptive detector Fine-Tuning strategy (DAFT). Experiments demonstrate our superiority in predicting real-fake under the MID-FFD scenario while maintaining original generalization ability to unseen data.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04837v1", "score": 0, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04617v1", "type": "papers", "title": "Score Matching for Estimating Finite Point Processes", "author": "Haoqun Cao, Yixuan Zhang, Feng Zhou", "abstract": "Score matching estimators have garnered significant attention in recent years because they eliminate the need to compute normalizing constants, thereby mitigating the computational challenges associated with maximum likelihood estimation (MLE).While several studies have proposed score matching estimators for point processes, this work highlights the limitations of these existing methods, which stem primarily from the lack of a mathematically rigorous analysis of how score matching behaves on finite point processes -- special random configurations on bounded spaces where many of the usual assumptions and properties of score matching no longer hold. To this end, we develop a formal framework for score matching on finite point processes via Janossy measures and, within this framework, introduce an (autoregressive) weighted score-matching estimator, whose statistical properties we analyze in classical parametric settings. For general nonparametric (e.g., deep) point process models, we show that score matching alone does not uniquely identify the ground-truth distribution due to subtle normalization issues, and we propose a simple survival-classification augmentation that yields a complete, integration-free training objective for any intensity-based point process model for spatio-temporal case. Experiments on synthetic and real-world temporal and spatio-temporal datasets, demonstrate that our method accurately recovers intensities and achieves performance comparable to MLE with better efficiency.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04617v1", "score": 0, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04571v1", "type": "papers", "title": "Temp-SCONE: A Novel Out-of-Distribution Detection and Domain Generalization Framework for Wild Data with Temporal Shift", "author": "Aditi Naiknaware, Sanchit Singh, Hajar Homayouni", "abstract": "Open-world learning (OWL) requires models that can adapt to evolving environments while reliably detecting out-of-distribution (OOD) inputs. Existing approaches, such as SCONE, achieve robustness to covariate and semantic shifts but assume static environments, leading to degraded performance in dynamic domains. In this paper, we propose Temp-SCONE, a temporally consistent extension of SCONE designed to handle temporal shifts in dynamic environments. Temp-SCONE introduces a confidence-driven regularization loss based on Average Thresholded Confidence (ATC), penalizing instability in predictions across time steps while preserving SCONE's energy-margin separation. Experiments on dynamic datasets demonstrate that Temp-SCONE significantly improves robustness under temporal drift, yielding higher corrupted-data accuracy and more reliable OOD detection compared to SCONE. On distinct datasets without temporal continuity, Temp-SCONE maintains comparable performance, highlighting the importance and limitations of temporal regularization. Our theoretical insights on temporal stability and generalization error further establish Temp-SCONE as a step toward reliable OWL in evolving dynamic environments.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04571v1", "score": 0, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04530v1", "type": "papers", "title": "Explainable Graph Representation Learning via Graph Pattern Analysis", "author": "Xudong Wang, Ziheng Sun, Chris Ding", "abstract": "Explainable artificial intelligence (XAI) is an important area in the AI community, and interpretability is crucial for building robust and trustworthy AI models. While previous work has explored model-level and instance-level explainable graph learning, there has been limited investigation into explainable graph representation learning. In this paper, we focus on representation-level explainable graph learning and ask a fundamental question: What specific information about a graph is captured in graph representations? Our approach is inspired by graph kernels, which evaluate graph similarities by counting substructures within specific graph patterns. Although the pattern counting vector can serve as an explainable representation, it has limitations such as ignoring node features and being high-dimensional. To address these limitations, we introduce a framework (PXGL-GNN) for learning and explaining graph representations through graph pattern analysis. We start by sampling graph substructures of various patterns. Then, we learn the representations of these patterns and combine them using a weighted sum, where the weights indicate the importance of each graph pattern's contribution. We also provide theoretical analyses of our methods, including robustness and generalization. In our experiments, we show how to learn and explain graph representations for real-world data using pattern analysis. Additionally, we compare our method against multiple baselines in both supervised and unsupervised learning tasks to demonstrate its effectiveness.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04530v1", "score": 0, "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.04528v1", "type": "papers", "title": "Auto3R: Automated 3D Reconstruction and Scanning via Data-driven Uncertainty Quantification", "author": "Chentao Shen, Sizhe Zheng, Bingqian Wu", "abstract": "Traditional high-quality 3D scanning and reconstruction typically relies on human labor to plan the scanning procedure. With the rapid development of embodied systems such as drones and robots, there is a growing demand of performing accurate 3D scanning and reconstruction in an fully automated manner. We introduce Auto3R, a data-driven uncertainty quantification model that is designed to automate the 3D scanning and reconstruction of scenes and objects, including objects with non-lambertian and specular materials. Specifically, in a process of iterative 3D reconstruction and scanning, Auto3R can make efficient and accurate prediction of uncertainty distribution over potential scanning viewpoints, without knowing the ground truth geometry and appearance. Through extensive experiments, Auto3R achieves superior performance that outperforms the state-of-the-art methods by a large margin. We also deploy Auto3R on a robot arm equipped with a camera and demonstrate that Auto3R can be used to effectively digitize real-world 3D objects and delivers ready-to-use and photorealistic digital assets. Our homepage: https://tomatoma00.github.io/auto3r.github.io .", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04528v1", "score": 0, "tags": ["Vision", "Reinforcement"], "source": "arXiv"}, {"id": "889452427", "type": "projects", "title": "QuestNav/QuestNav", "author": "QuestNav", "abstract": "A project that enables streaming Meta Quest headset pose to an FRC robot over Network Tables. Built in Unity. ", "date": "2025-12-05", "url": "https://github.com/QuestNav/QuestNav", "stars": 113, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "102971729", "type": "projects", "title": "ros-controls/ros2_control", "author": "ros-controls", "abstract": "Generic and simple controls framework for ROS 2", "date": "2025-12-05", "url": "https://github.com/ros-controls/ros2_control", "stars": 767, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "486234106", "type": "projects", "title": "verivital/behaverify", "author": "verivital", "abstract": "BehaVerify: A Formal Verification Tool for Behavior Trees", "date": "2025-12-05", "url": "https://github.com/verivital/behaverify", "stars": 19, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "882091187", "type": "projects", "title": "AD-SDL/MADSci", "author": "AD-SDL", "abstract": "Main repository for the Modular Autonomous Discovery for Science (MADSci) Framework", "date": "2025-12-05", "url": "https://github.com/AD-SDL/MADSci", "stars": 30, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "973471437", "type": "projects", "title": "militarandroid/cybersecurity_hack", "author": "militarandroid", "abstract": "Social Media, Website, Crypto Hacking ðŸ”¥ Hacker, Cyber, Cybersecurity. Instagram, TikTok, YouTube, Telegram, Snapchat Hacking. Cryptocurrency Bitcoin Ethereum Litecoin Dogecoin Solana Wallet. Dark Deep Web Hacker. Password cracker, mail gmail, webapp, zip, vulnerability, RAT, malware, trojan, black hat, PHP HTML hacker. Exploit social engineering.", "date": "2025-12-05", "url": "https://github.com/militarandroid/cybersecurity_hack", "stars": 36, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "809911373", "type": "projects", "title": "umrover/mrover-ros2", "author": "umrover", "abstract": " MRover ROS2 Source Code", "date": "2025-12-05", "url": "https://github.com/umrover/mrover-ros2", "stars": 15, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "1061157005", "type": "projects", "title": "Grigorij-Dudnik/RoboCrew", "author": "Grigorij-Dudnik", "abstract": "ðŸ¦¾Set up your embodied LLM agent with the same ease as normal agents in CrewAI or Autogen", "date": "2025-12-04", "url": "https://github.com/Grigorij-Dudnik/RoboCrew", "stars": 21, "score": 0, "tags": ["LLM/VLA"], "source": "GitHub"}, {"id": "999550439", "type": "projects", "title": "SIBench/Awesome-Visual-Spatial-Reasoning", "author": "SIBench", "abstract": "This is a project about visual spatial reasoning.", "date": "2025-12-04", "url": "https://github.com/SIBench/Awesome-Visual-Spatial-Reasoning", "stars": 81, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "1082733610", "type": "projects", "title": "DexForce/EmbodiChain", "author": "DexForce", "abstract": "An end-to-end, GPU-accelerated, and modular platform for building generalized Embodied Intelligence.", "date": "2025-12-04", "url": "https://github.com/DexForce/EmbodiChain", "stars": 21, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "958377083", "type": "projects", "title": "ZizoTheDev/ffmpeg-mcp", "author": "ZizoTheDev", "abstract": "An MCP server for FFmpeg", "date": "2025-12-05", "url": "https://github.com/ZizoTheDev/ffmpeg-mcp", "stars": 15, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "491699829", "type": "projects", "title": "google/pyglove", "author": "google", "abstract": "Manipulating Python Programs", "date": "2025-12-04", "url": "https://github.com/google/pyglove", "stars": 700, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "371028278", "type": "projects", "title": "easystats/datawizard", "author": "easystats", "abstract": "Magic potions to clean and transform your data ðŸ§™ ", "date": "2025-12-01", "url": "https://github.com/easystats/datawizard", "stars": 233, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "1104737428", "type": "projects", "title": "AKS-Lab-Univertsity-of-Tartu/bimanual_manipulation", "author": "AKS-Lab-Univertsity-of-Tartu", "abstract": "This repository is for tracking progress of bimanual manipulation for different tasks. ", "date": "2025-12-04", "url": "https://github.com/AKS-Lab-Univertsity-of-Tartu/bimanual_manipulation", "stars": 11, "score": 0, "tags": ["Manipulation"], "source": "GitHub"}, {"id": "1073406342", "type": "projects", "title": "mujocolab/g1_spinkick_example", "author": "mujocolab", "abstract": "Train a Unitree G1 humanoid to perform a double spin kick using mjlab", "date": "2025-12-05", "url": "https://github.com/mujocolab/g1_spinkick_example", "stars": 174, "score": 0, "tags": ["Humanoid"], "source": "GitHub"}, {"id": "956670330", "type": "projects", "title": "evronix/quadruped_sim2sim", "author": "evronix", "abstract": "Deploying a quadruped robot policy(DreamWaQ, walk-these-ways, HIMLoco) trained in Isaacgym to ROS2 Gazebo for Sim-to-Sim", "date": "2025-12-04", "url": "https://github.com/evronix/quadruped_sim2sim", "stars": 33, "score": 0, "tags": ["Locomotion", "Reinforcement"], "source": "GitHub"}, {"id": "983137183", "type": "projects", "title": "taco-group/GenAI4AD", "author": "taco-group", "abstract": "a comprehensive and critical synthesis of the emerging role of GenAI across the full autonomous driving stack", "date": "2025-11-25", "url": "https://github.com/taco-group/GenAI4AD", "stars": 218, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "1048227303", "type": "projects", "title": "CAI23sbP/go2_parkour_deploy", "author": "CAI23sbP", "abstract": "IsaacLab to Mujoco GO2 deploy, IsaacLab to Real world GO2 deploy", "date": "2025-12-04", "url": "https://github.com/CAI23sbP/go2_parkour_deploy", "stars": 35, "score": 0, "tags": ["Reinforcement"], "source": "GitHub"}, {"id": "950053234", "type": "projects", "title": "zdchan/RobustDexGrasp", "author": "zdchan", "abstract": "This is a repository for RobustDexGrasp, which achieves robust dexterous grasping of 500+ unseen objects with random poses from single-view perception.", "date": "2025-12-01", "url": "https://github.com/zdchan/RobustDexGrasp", "stars": 123, "score": 0, "tags": ["Vision", "Manipulation"], "source": "GitHub"}, {"id": "921614730", "type": "projects", "title": "luohongk/Embodied-AI-Daily", "author": "luohongk", "abstract": "ðŸ“šè¿™ä¸ªä»“åº“æ˜¯åœ¨arxivä¸Šæ”¶é›†çš„æœ‰å…³VLNï¼ŒVLAï¼ŒWorld Modelï¼ŒSLAMï¼ŒGaussian Splatting,éžçº¿æ€§ä¼˜åŒ–ç­‰ç›¸å…³è®ºæ–‡ã€‚æ¯å¤©éƒ½ä¼šè‡ªåŠ¨æ›´æ–°ï¼issueåŒºåŸŸæ˜¯æœ€æ–°10ç¯‡è®ºæ–‡", "date": "2025-12-04", "url": "https://github.com/luohongk/Embodied-AI-Daily", "stars": 123, "score": 0, "tags": ["LLM/VLA", "Reinforcement"], "source": "GitHub"}, {"id": "1086165349", "type": "projects", "title": "Dexmal/realtime-vla", "author": "Dexmal", "abstract": "Running VLA at 30Hz frame rate and 480Hz trajectory frequency", "date": "2025-12-04", "url": "https://github.com/Dexmal/realtime-vla", "stars": 297, "score": 0, "tags": ["LLM/VLA"], "source": "GitHub"}, {"id": "1001011932", "type": "projects", "title": "nimiCurtis/so101_ros2", "author": "nimiCurtis", "abstract": "A ROS2 Bridge for Lerobot so101 manipulator", "date": "2025-11-26", "url": "https://github.com/nimiCurtis/so101_ros2", "stars": 20, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "1003090041", "type": "projects", "title": "utiasDSL/crisp_gym", "author": "utiasDSL", "abstract": "Gym environments for manipulators based on crisp_py and ROS2: collect data and deploy policies on real ROS2 enabled manipulators.", "date": "2025-11-24", "url": "https://github.com/utiasDSL/crisp_gym", "stars": 16, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "1076162882", "type": "projects", "title": "csmile-1006/DEAS-Isaac-GR00T", "author": "csmile-1006", "abstract": "DEAS + Isaac-GR00T + RoboCasa", "date": "2025-12-05", "url": "https://github.com/csmile-1006/DEAS-Isaac-GR00T", "stars": 14, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "484009818", "type": "projects", "title": "addmix/godot_aerodynamic_physics", "author": "addmix", "abstract": "", "date": "2025-12-05", "url": "https://github.com/addmix/godot_aerodynamic_physics", "stars": 156, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "912281829", "type": "projects", "title": "timschmidt/csgrs", "author": "timschmidt", "abstract": "Multi-modal constructive solid geometry kernel in Rust ", "date": "2025-12-05", "url": "https://github.com/timschmidt/csgrs", "stars": 170, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "89232721", "type": "projects", "title": "equinor/ert", "author": "equinor", "abstract": "ERT - Ensemble based Reservoir Tool - is designed for running ensembles of dynamical models such as reservoir models, in order to do sensitivity analysis and data assimilation. ERT supports data assimilation using the Ensemble Smoother (ES) and Ensemble Smoother with Multiple Data Assimilation (ES-MDA).", "date": "2025-12-05", "url": "https://github.com/equinor/ert", "stars": 136, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "674611614", "type": "projects", "title": "toruseo/UXsim", "author": "toruseo", "abstract": "Vehicular traffic flow simulator in road network, written in pure Python", "date": "2025-12-05", "url": "https://github.com/toruseo/UXsim", "stars": 222, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "211382037", "type": "projects", "title": "bgin/RF-EMT", "author": "bgin", "abstract": "Radio-Frequency Engineering Modeling Toolkit (RF-EMT)", "date": "2025-12-05", "url": "https://github.com/bgin/RF-EMT", "stars": 62, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "33634731", "type": "projects", "title": "APSIMInitiative/ApsimX", "author": "APSIMInitiative", "abstract": "ApsimX is the next generation of APSIM", "date": "2025-12-05", "url": "https://github.com/APSIMInitiative/ApsimX", "stars": 176, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "118596920", "type": "projects", "title": "ITISFoundation/osparc-simcore", "author": "ITISFoundation", "abstract": "ðŸ¼ osparc-simcore simulation framework", "date": "2025-12-05", "url": "https://github.com/ITISFoundation/osparc-simcore", "stars": 50, "score": 0, "tags": ["Sim2Real"], "source": "GitHub"}, {"id": "20775600", "type": "projects", "title": "opensim-org/opensim-core", "author": "opensim-org", "abstract": "SimTK OpenSim C++ libraries and command-line applications, and Java/Python wrapping.", "date": "2025-12-05", "url": "https://github.com/opensim-org/opensim-core", "stars": 960, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "607539353", "type": "projects", "title": "AspirinCode/papers-for-molecular-design-using-DL", "author": "AspirinCode", "abstract": "List of Molecular and Material design using Generative AI and Deep Learning ", "date": "2025-12-05", "url": "https://github.com/AspirinCode/papers-for-molecular-design-using-DL", "stars": 886, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "1044554840", "type": "projects", "title": "NVIDIA-NeMo/Gym", "author": "NVIDIA-NeMo", "abstract": "Build RL environments for LLM training", "date": "2025-12-05", "url": "https://github.com/NVIDIA-NeMo/Gym", "stars": 72, "score": 0, "tags": ["LLM/VLA", "Reinforcement"], "source": "GitHub"}, {"id": "1064168232", "type": "projects", "title": "JianxXiong/AAPO", "author": "JianxXiong", "abstract": "Implementation of AAPO (Arxiv: 2505.14264v2) paper", "date": "2025-12-05", "url": "https://github.com/JianxXiong/AAPO", "stars": 16, "score": 0, "tags": ["General"], "source": "GitHub"}, {"id": "531747451", "type": "projects", "title": "tinker495/JAxtar", "author": "tinker495", "abstract": "JAxtar is a project with a JAX-native implementation of parallelizeable A* & Q* solver for neural heuristic search research.", "date": "2025-12-05", "url": "https://github.com/tinker495/JAxtar", "stars": 42, "score": 0, "tags": ["General"], "source": "GitHub"}];
    window.ALL_TAGS = ["phi-4-mini", "t2v", "llama-cpp", "speech-summarization", "zh", "ja", "Streaming text input", "fixer", "Long-from speech generation", "text-to-video;", "agentic", "phi", "phi4mm", "zero-shot-image-classification", "onnx", "arxiv:2506.21539", "base_model:finetune:IPEC-COMMUNITY/spatialvla-4b-224-pt", "multilingual", "dataset:allenai/Dolci-RLZero-IF-7B", "Realtime TTS", "base_model:allenai/Olmo-3-1025-7B", "text-generation-inference", "safetensors", "VLA", "base_model:tencent/HunyuanVideo-1.5", "base_model:quantized:nvidia/Orchestrator-8B", "base_model:finetune:zai-org/GLM-4.5-Air-Base", "base_model:quantized:zai-org/GLM-4.5-Air-Base", "arxiv:2410.24164", "arxiv:2503.20314", "ppo", "image-text-to-text", "rapid", "image-to-text", "portfolio-management", "dataset:yahoo-finance", "visual-question-answering", "Sim2Real", "base_model:finetune:Wan-AI/Wan2.2-I2V-A14B", "LLM/VLA", "audio", "i2v", "tf-keras", "no", "arxiv:2506.09930", "base_model:facebook/chameleon-7b", "stock-market", "tensorboard", "pytorch", "ru", "th", "arxiv:2502.12479", "llama3.1", "Humanoid", "transformers", "ar", "protein-generation", "medical-embeddings", "sv", "image-to-image", "prime-intellect", "vision-language-model", "multi-reasoning", "arxiv:1810.04805", "quantitative-finance", "base_model:zai-org/GLM-4.5-Air-Base", "ko", "General", "arxiv:2508.19205", "financial-ai", "wan2.2", "dataset:allenai/dolma3_mix-5.5T-1125", "compressed-tensors", "video-continuation", "VLM", "dataset:allenai/dolma3_mix-6T-1025", "robotics", "reinforcement-learning", "gguf-my-repo", "image-feature-extraction", "phi-4-multimodal", "Locomotion", "region:us", "arxiv:1707.06347", "arxiv:2102.03334", "text2text-generation", "prime-rl", "image2image", "music reasoning", "license:cc-by-nc-sa-4.0", "automatic-speech-recognition", "navigation", "audio-text-to-text", "arxiv:2309.14509", "audioflamingo3", "chest-x-ray", "image-to-video", "image-to-video;", "cxr-foundation", "music understanding", "base_model:PrimeIntellect/INTELLECT-3", "aio", "base_model:finetune:google/paligemma2-3b-pt-224", "base_model:finetune:tencent/HunyuanVideo-1.5", "arxiv:2511.10289", "text-generation", "base_model:Wan-AI/Wan2.2-I2V-A14B", "base_model:finetune:facebook/chameleon-7b", "arxiv:2505.13032", "base_model:IPEC-COMMUNITY/spatialvla-4b-224-pt", "arxiv:2103.00020", "arxiv:2511.18870", "stable-baselines3", "olmo3", "gr00t_n1_5", "arxiv:1911.04252", "Generalist Robot Policy", "license:mit", "Robotics", "trading", "arxiv:2308.01317", "uk", "feature-extraction", "orchestration", "dataset:nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim", "license:other", "mixture-of-experts", "reasoning", "tool-calling", "arxiv:2301.12597", "Reinforcement", "autotrain_compatible", "deploy:azure", "endpoints_compatible", "lerobot", "arxiv:2501.14818", "Vision", "base_model:finetune:allenai/Olmo-3-1025-7B", "conversational", "algorithmic-trading", "it", "he", "de", "video-generation;", "glm4_moe", "medical", "ti2v", "license:apache-2.0", "Manipulation", "hunyuan", "nvidia", "code", "vilt", "Foundation Vision-language-action Model", "base_model:nvidia/Orchestrator-8B", "qwen2_5_vl", "pt", "finance", "diffusers", "music/songs", "arxiv:2501.15830", "custom_code", "tr", "HunyuanVideo-1.5", "fi", "dataset:unknown", "fr", "pl", "arxiv:2004.11362", "nl", "orchestrator-model", "hu", "verifiers", "reinforcementlearning", "arxiv:2407.13833", "gguf", "arxiv:2503.01774", "model-index", "vibevoice_streaming", "dataset:microsoft/Dayhoff", "dataset:nvidia/MF-Skills", "arxiv:2209.03003", "deep-reinforcement-learning", "cs", "arxiv:2412.08635", "base_model:quantized:PrimeIntellect/INTELLECT-3", "en", "da", "x-ray", "speech-translation", "video-language-model", "arxiv:2503.01743", "imatrix", "image-classification", "es", "pix2struct", "arxiv:2210.03347", "xvla", "arxiv:2510.22200", "jamba", "spatialvla", "base_model:google/paligemma2-3b-pt-224", "text-to-video", "ro", "text-to-speech", "comfyUI;", "nlp"];
    window.DAILY_SUMMARY = "**ä»Šå¤©çš„æŠ€æœ¯è¶‹åŠ¿**èšç„¦äºŽå¤§åž‹è¯­è¨€æ¨¡åž‹å’Œè·¨æ¨¡æ€ç”Ÿæˆæ¨¡åž‹çš„æŒç»­è¿›æ­¥ï¼Œæ—¨åœ¨æå‡æœºå™¨ç†è§£ã€ç”Ÿæˆå’Œè½¬åŒ–ä¸åŒæ¨¡æ€ä¿¡æ¯çš„èƒ½åŠ›ã€‚\n\n**æœ€æœ‰ä»·å€¼çš„å·¥ä½œ**åŒ…æ‹¬**NVIDIA**çš„**Llama-3.1-Nemotron-Nano-VL-8B-V1**å’Œ**Olmo-3-1125-32B**ã€‚å‰è€…ä¸“æ³¨äºŽå›¾åƒåˆ°æ–‡æœ¬çš„ç”Ÿæˆä»»åŠ¡ï¼Œå±•ç¤ºäº†åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„æœ€æ–°è¿›å±•ï¼›åŽè€…åˆ™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆæ¨¡åž‹ï¼Œå…·æœ‰320äº¿å‚æ•°ï¼Œæ˜¾è‘—æå‡äº†æ–‡æœ¬ç”Ÿæˆçš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚è¿™äº›æ¨¡åž‹ä¸ä»…æå‡äº†å„è‡ªé¢†åŸŸçš„æ€§èƒ½ï¼Œä¹Ÿä¸ºè·¨æ¨¡æ€ä»»åŠ¡æ‰“å¼€äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚æ­¤å¤–ï¼ŒNVIDIAçš„**Fixer**é¡¹ç›®å±•ç¤ºäº†å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢èƒ½åŠ›ï¼Œè¿™å¯¹äºŽè§†è§‰å†…å®¹çš„ç”Ÿæˆå’Œç¼–è¾‘å…·æœ‰é‡è¦æ„ä¹‰ã€‚è¿™äº›å·¥ä½œå…±åŒæŽ¨åŠ¨äº†å…·èº«æ™ºèƒ½å’Œæœºå™¨äººå­¦ä¹ é¢†åŸŸçš„æŠ€æœ¯è¾¹ç•Œã€‚";
    window.LAST_UPDATE = "2025-12-05 14:55:08";
    