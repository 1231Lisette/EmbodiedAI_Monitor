
    window.RESEARCH_DATA = [{"id": "2512.04069v1", "type": "papers", "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL", "author": "Siyi Chen, Mikaela Angelina Uy, Chan Hee Song", "abstract": "Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.04069v1", "tags": ["Manipulation", "Vision", "Reinforcement", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.03991v1", "type": "papers", "title": "When to Say \"Hi\" - Learn to Open a Conversation with an in-the-wild Dataset", "author": "Michael Schiffmann, Felix Struth, Sabina Jeschke", "abstract": "The social capabilities of socially interactive agents (SIA) are a key to successful and smooth interactions between the user and the SIA. A successful start of the interaction is one of the essential factors for satisfying SIA interactions. For a service and information task in which the SIA helps with information, e.g. about the location, it is an important skill to master the opening of the conversation and to recognize which interlocutor opens the conversation and when. We are therefore investigating the extent to which the opening of the conversation can be trained using the user's body language as an input for machine learning to ensure smooth conversation starts for the interaction. In this paper we propose the Interaction Initiation System (IIS) which we developed, trained and validated using an in-the-wild data set. In a field test at the Deutsches Museum Bonn, a Furhat robot from Furhat Robotics was used as a service and information point. Over the period of use we collected the data of \\textit{N} = 201 single user interactions for the training of the algorithms. We can show that the IIS, achieves a performance that allows the conclusion that this system is able to determine the greeting period and the opener of the interaction.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03991v1", "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.03945v1", "type": "papers", "title": "Classification of User Satisfaction in HRI with Social Signals in the Wild", "author": "Michael Schiffmann, Sabina Jeschke, Anja Richert", "abstract": "Socially interactive agents (SIAs) are being used in various scenarios and are nearing productive deployment. Evaluating user satisfaction with SIAs' performance is a key factor in designing the interaction between the user and SIA. Currently, subjective user satisfaction is primarily assessed manually through questionnaires or indirectly via system metrics. This study examines the automatic classification of user satisfaction through analysis of social signals, aiming to enhance both manual and autonomous evaluation methods for SIAs. During a field trial at the Deutsches Museum Bonn, a Furhat Robotics head was employed as a service and information hub, collecting an \"in-the-wild\" dataset. This dataset comprises 46 single-user interactions, including questionnaire responses and video data. Our method focuses on automatically classifying user satisfaction based on time series classification. We use time series of social signal metrics derived from the body pose, time series of facial expressions, and physical distance. This study compares three feature engineering approaches on different machine learning models. The results confirm the method's effectiveness in reliably identifying interactions with low user satisfaction without the need for manually annotated datasets. This approach offers significant potential for enhancing SIA performance and user experience through automated feedback mechanisms.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03945v1", "tags": ["General"], "source": "arXiv"}, {"id": "2512.03911v1", "type": "papers", "title": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware", "author": "Kenneth Stewart, Roxana Leontie, Samantha Chapin", "abstract": "We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03911v1", "tags": ["Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.03743v1", "type": "papers", "title": "Cross-embodied Co-design for Dexterous Hands", "author": "Kehlani Fay, Darin Anthony Djapri, Anya Zorin", "abstract": "Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03743v1", "tags": ["Manipulation", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.03736v1", "type": "papers", "title": "Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control", "author": "Kenneth Stewart, Samantha Chapin, Roxana Leontie", "abstract": "Reinforcement learning (RL) offers transformative potential for robotic control in space. We present the first on-orbit demonstration of RL-based autonomous control of a free-flying robot, the NASA Astrobee, aboard the International Space Station (ISS). Using NVIDIA's Omniverse physics simulator and curriculum learning, we trained a deep neural network to replace Astrobee's standard attitude and translation control, enabling it to navigate in microgravity. Our results validate a novel training pipeline that bridges the simulation-to-reality (Sim2Real) gap, utilizing a GPU-accelerated, scientific-grade simulation environment for efficient Monte Carlo RL training. This successful deployment demonstrates the feasibility of training RL policies terrestrially and transferring them to space-based applications. This paves the way for future work in In-Space Servicing, Assembly, and Manufacturing (ISAM), enabling rapid on-orbit adaptation to dynamic mission requirements.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03736v1", "tags": ["Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.03729v1", "type": "papers", "title": "Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing", "author": "Samantha Chapin, Kenneth Stewart, Roxana Leontie", "abstract": "The US Naval Research Laboratory's (NRL's) Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) experiment pioneers the use of reinforcement learning (RL) for control of free-flying robots in the zero-gravity (zero-G) environment of space. On Tuesday, May 27th 2025 the APIARY team conducted the first ever, to our knowledge, RL control of a free-flyer in space using the NASA Astrobee robot on-board the International Space Station (ISS). A robust 6-degrees of freedom (DOF) control policy was trained using an actor-critic Proximal Policy Optimization (PPO) network within the NVIDIA Isaac Lab simulation environment, randomizing over goal poses and mass distributions to enhance robustness. This paper details the simulation testing, ground testing, and flight validation of this experiment. This on-orbit demonstration validates the transformative potential of RL for improving robotic autonomy, enabling rapid development and deployment (in minutes to hours) of tailored behaviors for space exploration, logistics, and real-time mission needs.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03729v1", "tags": ["Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.03724v1", "type": "papers", "title": "PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention", "author": "Ziwen Li, Xin Wang, Hanlue Zhang", "abstract": "The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03724v1", "tags": ["Manipulation", "Vision", "Reinforcement", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.03707v1", "type": "papers", "title": "ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration", "author": "Sundas Rafat Mulkana, Ronyu Yu, Tanaya Guha", "abstract": "In collaborative human-robot tasks, safety requires not only avoiding collisions but also ensuring safe, intentional physical contact. We present ContactRL, a reinforcement learning (RL) based framework that directly incorporates contact safety into the reward function through force feedback. This enables a robot to learn adaptive motion profiles that minimize human-robot contact forces while maintaining task efficiency. In simulation, ContactRL achieves a low safety violation rate of 0.2\\% with a high task success rate of 87.7\\%, outperforming state-of-the-art constrained RL baselines. In order to guarantee deployment safety, we augment the learned policy with a kinetic energy based Control Barrier Function (eCBF) shield. Real-world experiments on an UR3e robotic platform performing small object handovers from a human hand across 360 trials confirm safe contact, with measured normal forces consistently below 10N. These results demonstrate that ContactRL enables safe and efficient physical collaboration, thereby advancing the deployment of collaborative robots in contact-rich tasks.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03707v1", "tags": ["Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.03684v1", "type": "papers", "title": "A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection", "author": "Shahid Ansari, Mahendra Kumar Gohil, Yusuke Maeda", "abstract": "This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping. The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting. For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination. An analytical model derived using the principle of virtual work relates servo torque to grasp force, enabling design-level reasoning about actuation requirements. During execution, closed-loop grasp-force regulation is achieved using a proportional--integral--derivative controller with feedback from force-sensitive resistors mounted on selected fingers to prevent slip and bruising. Motion execution is supported by Particle Swarm Optimization (PSO)--based trajectory planning for a 5-DOF manipulator. Experiments demonstrate complete picking cycles (approach, separation, cutting, grasping, transport, release) with an average cycle time of 24.34~s and an overall success rate of approximately 80\\%, while maintaining low grasp forces (0.20--0.50~N). These results validate the proposed hybrid gripper and integrated vision--control pipeline for reliable harvesting in cluttered environments.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03684v1", "tags": ["Manipulation", "Vision"], "source": "arXiv"}, {"id": "2512.03538v1", "type": "papers", "title": "AdaPower: Specializing World Foundation Models for Predictive Manipulation", "author": "Yuhang Huang, Shilong Zou, Jiazhao Zhang", "abstract": "World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce \\textbf{AdaPower} (\\textbf{Ada}pt and Em\\textbf{power}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41\\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03538v1", "tags": ["Manipulation", "Reinforcement", "Sim2Real", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.03444v1", "type": "papers", "title": "PerFACT: Motion Policy with LLM-Powered Dataset Synthesis and Fusion Action-Chunking Transformers", "author": "Davood Soleymanzadeh, Xiao Liang, Minghui Zheng", "abstract": "Deep learning methods have significantly enhanced motion planning for robotic manipulators by leveraging prior experiences within planning datasets. However, state-of-the-art neural motion planners are primarily trained on small datasets collected in manually generated workspaces, limiting their generalizability to out-of-distribution scenarios. Additionally, these planners often rely on monolithic network architectures that struggle to encode critical planning information. To address these challenges, we introduce Motion Policy with Dataset Synthesis powered by large language models (LLMs) and Fusion Action-Chunking Transformers (PerFACT), which incorporates two key components. Firstly, a novel LLM-powered workspace generation method, MotionGeneralizer, enables large-scale planning data collection by producing a diverse set of semantically feasible workspaces. Secondly, we introduce Fusion Motion Policy Networks (MpiNetsFusion), a generalist neural motion planner that uses a fusion action-chunking transformer to better encode planning signals and attend to multiple feature modalities. Leveraging MotionGeneralizer, we collect 3.5M trajectories to train and evaluate MpiNetsFusion against state-of-the-art planners, which shows that the proposed MpiNetsFusion can plan several times faster on the evaluated tasks.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03444v1", "tags": ["Reinforcement", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.03438v1", "type": "papers", "title": "Multimodal Reinforcement Learning with Agentic Verifier for AI Agents", "author": "Reuben Tan, Baolin Peng, Zhengyuan Yang", "abstract": "Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03438v1", "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.03429v1", "type": "papers", "title": "World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations", "author": "Raul Steinmetz, Fabio Demo Rosa, Victor Augusto Kich", "abstract": "Autonomous navigation of terrestrial robots using Reinforcement Learning (RL) from LIDAR observations remains challenging due to the high dimensionality of sensor data and the sample inefficiency of model-free approaches. Conventional policy networks struggle to process full-resolution LIDAR inputs, forcing prior works to rely on simplified observations that reduce spatial awareness and navigation robustness. This paper presents a novel model-based RL framework built on top of the DreamerV3 algorithm, integrating a Multi-Layer Perceptron Variational Autoencoder (MLP-VAE) within a world model to encode high-dimensional LIDAR readings into compact latent representations. These latent features, combined with a learned dynamics predictor, enable efficient imagination-based policy optimization. Experiments on simulated TurtleBot3 navigation tasks demonstrate that the proposed architecture achieves faster convergence and higher success rate compared to model-free baselines such as SAC, DDPG, and TD3. It is worth emphasizing that the DreamerV3-based agent attains a 100% success rate across all evaluated environments when using the full dataset of the Turtlebot3 LIDAR (360 readings), while model-free methods plateaued below 85%. These findings demonstrate that integrating predictive world models with learned latent representations enables more efficient and robust navigation from high-dimensional sensory data.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03429v1", "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.03422v1", "type": "papers", "title": "What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models", "author": "Tianchen Deng, Yue Pan, Shenghai Yuan", "abstract": "In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03422v1", "tags": ["Manipulation", "Vision", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.03418v1", "type": "papers", "title": "YOLOA: Real-Time Affordance Detection via LLM Adapter", "author": "Yuqi Ji, Junjie Ke, Lihuo He", "abstract": "Affordance detection aims to jointly address the fundamental \"what-where-how\" challenge in embodied AI by understanding \"what\" an object is, \"where\" the object is located, and \"how\" it can be used. However, most affordance learning methods focus solely on \"how\" objects can be used while neglecting the \"what\" and \"where\" aspects. Other affordance detection methods treat object detection and affordance learning as two independent tasks, lacking effective interaction and real-time capability. To overcome these limitations, we introduce YOLO Affordance (YOLOA), a real-time affordance detection model that jointly handles these two tasks via a large language model (LLM) adapter. Specifically, YOLOA employs a lightweight detector consisting of object detection and affordance learning branches refined through the LLM Adapter. During training, the LLM Adapter interacts with object and affordance preliminary predictions to refine both branches by generating more accurate class priors, box offsets, and affordance gates. Experiments on our relabeled ADG-Det and IIT-Heat benchmarks demonstrate that YOLOA achieves state-of-the-art accuracy (52.8 / 73.1 mAP on ADG-Det / IIT-Heat) while maintaining real-time performance (up to 89.77 FPS, and up to 846.24 FPS for the lightweight variant). This indicates that YOLOA achieves an excellent trade-off between accuracy and efficiency.", "date": "2025-12-03", "url": "https://arxiv.org/pdf/2512.03418v1", "tags": ["LLM/VLA"], "source": "arXiv"}, {"id": "2512.03256v1", "type": "papers", "title": "KALIKO: Kalman-Implicit Koopman Operator Learning For Prediction of Nonlinear Dynamical Systems", "author": "Albert H. Li, Ivan Dario Jimenez Rodriguez, Joel W. Burdick", "abstract": "Long-horizon dynamical prediction is fundamental in robotics and control, underpinning canonical methods like model predictive control. Yet, many systems and disturbance phenomena are difficult to model due to effects like nonlinearity, chaos, and high-dimensionality. Koopman theory addresses this by modeling the linear evolution of embeddings of the state under an infinite-dimensional linear operator that can be approximated with a suitable finite basis of embedding functions, effectively trading model nonlinearity for representational complexity. However, explicitly computing a good choice of basis is nontrivial, and poor choices may cause inaccurate forecasts or overfitting. To address this, we present Kalman-Implicit Koopman Operator (KALIKO) Learning, a method that leverages the Kalman filter to implicitly learn embeddings corresponding to latent dynamics without requiring an explicit encoder. KALIKO produces interpretable representations consistent with both theory and prior works, yielding high-quality reconstructions and inducing a globally linear latent dynamics. Evaluated on wave data generated by a high-dimensional PDE, KALIKO surpasses several baselines in open-loop prediction and in a demanding closed-loop simulated control task: stabilizing an underactuated manipulator's payload by predicting and compensating for strong wave disturbances.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.03256v1", "tags": ["General"], "source": "arXiv"}, {"id": "2512.03210v1", "type": "papers", "title": "Flux4D: Flow-based Unsupervised 4D Reconstruction", "author": "Jingkang Wang, Henry Che, Yun Chen", "abstract": "Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an \"as static as possible\" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.03210v1", "tags": ["Vision"], "source": "arXiv"}, {"id": "2512.03194v1", "type": "papers", "title": "GRAND: Guidance, Rebalancing, and Assignment for Networked Dispatch in Multi-Agent Path Finding", "author": "Johannes Gaber, Meshal Alharbi, Daniele Gammelli", "abstract": "Large robot fleets are now common in warehouses and other logistics settings, where small control gains translate into large operational impacts. In this article, we address task scheduling for lifelong Multi-Agent Pickup-and-Delivery (MAPD) and propose a hybrid method that couples learning-based global guidance with lightweight optimization. A graph neural network policy trained via reinforcement learning outputs a desired distribution of free agents over an aggregated warehouse graph. This signal is converted into region-to-region rebalancing through a minimum-cost flow, and finalized by small, local assignment problems, preserving accuracy while keeping per-step latency within a 1 s compute budget. On congested warehouse benchmarks from the League of Robot Runners (LRR) with up to 500 agents, our approach improves throughput by up to 10% over the 2024 winning scheduler while maintaining real-time execution. The results indicate that coupling graph-structured learned guidance with tractable solvers reduces congestion and yields a practical, scalable blueprint for high-throughput scheduling in large fleets.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.03194v1", "tags": ["Reinforcement"], "source": "arXiv"}, {"id": "2512.03166v1", "type": "papers", "title": "Multi-Agent Reinforcement Learning and Real-Time Decision-Making in Robotic Soccer for Virtual Environments", "author": "Aya Taourirte, Md Sohag Mia", "abstract": "The deployment of multi-agent systems in dynamic, adversarial environments like robotic soccer necessitates real-time decision-making, sophisticated cooperation, and scalable algorithms to avoid the curse of dimensionality. While Reinforcement Learning (RL) offers a promising framework, existing methods often struggle with the multi-granularity of tasks (long-term strategy vs. instant actions) and the complexity of large-scale agent interactions. This paper presents a unified Multi-Agent Reinforcement Learning (MARL) framework that addresses these challenges. First, we establish a baseline using Proximal Policy Optimization (PPO) within a client-server architecture for real-time action scheduling, with PPO demonstrating superior performance (4.32 avg. goals, 82.9% ball control). Second, we introduce a Hierarchical RL (HRL) structure based on the options framework to decompose the problem into a high-level trajectory planning layer (modeled as a Semi-Markov Decision Process) and a low-level action execution layer, improving global strategy (avg. goals increased to 5.26). Finally, to ensure scalability, we integrate mean-field theory into the HRL framework, simplifying many-agent interactions into a single agent vs. the population average. Our mean-field actor-critic method achieves a significant performance boost (5.93 avg. goals, 89.1% ball control, 92.3% passing accuracy) and enhanced training stability. Extensive simulations of 4v4 matches in the Webots environment validate our approach, demonstrating its potential for robust, scalable, and cooperative behavior in complex multi-agent domains.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.03166v1", "tags": ["Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.03044v1", "type": "papers", "title": "Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling", "author": "Yueru Jia, Jiaming Liu, Shengbang Liu", "abstract": "Robust perception and dynamics modeling are fundamental to real-world robotic policy learning. Recent methods employ video diffusion models (VDMs) to enhance robotic policies, improving their understanding and modeling of the physical world. However, existing approaches overlook the coherent and physically consistent motion representations inherently encoded across frames in VDMs. To this end, we propose Video2Act, a framework that efficiently guides robotic action learning by explicitly integrating spatial and motion-aware representations. Building on the inherent representations of VDMs, we extract foreground boundaries and inter-frame motion variations while filtering out background noise and task-irrelevant biases. These refined representations are then used as additional conditioning inputs to a diffusion transformer (DiT) action head, enabling it to reason about what to manipulate and how to move. To mitigate inference inefficiency, we propose an asynchronous dual-system design, where the VDM functions as the slow System 2 and the DiT head as the fast System 1, working collaboratively to generate adaptive actions. By providing motion-aware conditions to System 1, Video2Act maintains stable manipulation even with low-frequency updates from the VDM. For evaluation, Video2Act surpasses previous state-of-the-art VLA methods by 7.7% in simulation and 21.7% in real-world tasks in terms of average success rate, further exhibiting strong generalization capabilities.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.03044v1", "tags": ["LLM/VLA", "Manipulation", "Vision", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.02982v1", "type": "papers", "title": "U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences", "author": "Xiang Xu, Ao Liang, Youquan Liu", "abstract": "Modeling dynamic 3D environments from LiDAR sequences is central to building reliable 4D worlds for autonomous driving and embodied AI. Existing generative frameworks, however, often treat all spatial regions uniformly, overlooking the varying uncertainty across real-world scenes. This uniform generation leads to artifacts in complex or ambiguous regions, limiting realism and temporal stability. In this work, we present U4D, an uncertainty-aware framework for 4D LiDAR world modeling. Our approach first estimates spatial uncertainty maps from a pretrained segmentation model to localize semantically challenging regions. It then performs generation in a \"hard-to-easy\" manner through two sequential stages: (1) uncertainty-region modeling, which reconstructs high-entropy regions with fine geometric fidelity, and (2) uncertainty-conditioned completion, which synthesizes the remaining areas under learned structural priors. To further ensure temporal coherence, U4D incorporates a mixture of spatio-temporal (MoST) block that adaptively fuses spatial and temporal representations during diffusion. Extensive experiments show that U4D produces geometrically faithful and temporally consistent LiDAR sequences, advancing the reliability of 4D world modeling for autonomous perception and simulation.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.02982v1", "tags": ["Vision", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.02951v1", "type": "papers", "title": "Experimental Characterization of Fingertip Trajectory following for a 3-DoF Series-Parallel Hybrid Robotic Finger", "author": "Nicholas Baiata, Nilanjan Chakraborty", "abstract": "Task-space control of robotic fingers is a critical enabler of dexterous manipulation, as manipulation objectives are most naturally specified in terms of fingertip motions and applied forces rather than individual joint angles. While task-space planning and control have been extensively studied for larger, arm-scale manipulators, demonstrations of precise task-space trajectory tracking in compact, multi-DoF robotic fingers remain scarce. In this paper, we present the physical prototyping and experimental characterization of a three-degree-of-freedom, linkage-driven, series-parallel robotic finger with analytic forward kinematics and a closed-form Jacobian. A resolved motion rate control (RMRC) scheme is implemented to achieve closed-loop task-space trajectory tracking. We experimentally evaluate the fingertip tracking performance across a variety of trajectories, including straight lines, circles, and more complex curves, and report millimeter-level accuracy. To the best of our knowledge, this work provides one of the first systematic experimental demonstrations of precise task-space trajectory tracking in a linkage-driven robotic finger, thereby establishing a benchmark for future designs aimed at dexterous in-hand manipulation.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.02951v1", "tags": ["Manipulation"], "source": "arXiv"}, {"id": "2512.02851v2", "type": "papers", "title": "SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots", "author": "Iana Zhura, Sausar Karaf, Faryal Batool", "abstract": "Visual traversability estimation is critical for autonomous navigation, but existing VLM-based methods rely on hand-crafted prompts, generalize poorly across embodiments, and output only traversability maps, leaving trajectory generation to slow external planners. We propose SwarmDiffusion, a lightweight end-to-end diffusion model that jointly predicts traversability and generates a feasible trajectory from a single RGB image. To remove the need for annotated or planner-produced paths, we introduce a planner-free trajectory construction pipeline based on randomized waypoint sampling, Bezier smoothing, and regularization enforcing connectivity, safety, directionality, and path thinness. This enables learning stable motion priors without demonstrations. SwarmDiffusion leverages VLM-derived supervision without prompt engineering and conditions the diffusion process on a compact embodiment state, producing physically consistent, traversable paths that transfer across different robot platforms. Across indoor environments and two embodiments (quadruped and aerial), the method achieves 80-100% navigation success and 0.09s inference, and adapts to a new robot using only-500 additional visual samples. It generalizes reliably to unseen environments in simulation and real-world trials, offering a scalable, prompt-free approach to unified traversability reasoning and trajectory generation.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.02851v2", "tags": ["Locomotion", "Vision", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.02810v1", "type": "papers", "title": "Phase-Adaptive LLM Framework with Multi-Stage Validation for Construction Robot Task Allocation: A Systematic Benchmark Against Traditional Optimization Algorithms", "author": "Shyam prasad reddy Kaitha, Hongrui Yu", "abstract": "Multi-robot task allocation in construction automation has traditionally relied on optimization methods such as Dynamic Programming and Reinforcement Learning. This research introduces the LangGraph-based Task Allocation Agent (LTAA), an LLM-driven framework that integrates phase-adaptive allocation strategies, multi-stage validation with hierarchical retries, and dynamic prompting for efficient robot coordination. Although recent LLM approaches show potential for construction robotics, they largely lack rigorous validation and benchmarking against established algorithms. This paper presents the first systematic comparison of LLM-based task allocation with traditional methods in construction scenarios.The study validates LLM feasibility through SMART-LLM replication and addresses implementation challenges using a Self-Corrective Agent Architecture. LTAA leverages natural-language reasoning combined with structured validation mechanisms, achieving major computational gains reducing token usage by 94.6% and allocation time by 86% through dynamic prompting. The framework adjusts its strategy across phases: emphasizing execution feasibility early and workload balance in later allocations.The authors evaluate LTAA against Dynamic Programming, Q-learning, and Deep Q-Network (DQN) baselines using construction operations from the TEACh human-robot collaboration dataset. In the Heavy Excels setting, where robots have strong task specializations, LTAA achieves 77% task completion with superior workload balance, outperforming all traditional methods. These findings show that LLM-based reasoning with structured validation can match established optimization algorithms while offering additional advantages such as interpretability, adaptability, and the ability to update task logic without retraining.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.02810v1", "tags": ["Reinforcement", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.02787v2", "type": "papers", "title": "Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols", "author": "Xianchao Zeng, Xinyu Zhou, Youcheng Li", "abstract": "Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic manipulation, yet they remain limited in failure diagnosis and learning from failures. Additionally, existing failure datasets are mostly generated programmatically in simulation, which limits their generalization to the real world. In light of these, we introduce ViFailback, a framework designed to diagnose robotic manipulation failures and provide both textual and visual correction guidance. Our framework utilizes explicit visual symbols to enhance annotation efficiency. We further release the ViFailback dataset, a large-scale collection of 58,126 Visual Question Answering (VQA) pairs along with their corresponding 5,202 real-world manipulation trajectories. Based on the dataset, we establish ViFailback-Bench, a benchmark of 11 fine-grained VQA tasks designed to assess the failure diagnosis and correction abilities of Vision-Language Models (VLMs), featuring ViFailback-Bench Lite for closed-ended and ViFailback-Bench Hard for open-ended evaluation. To demonstrate the effectiveness of our framework, we built the ViFailback-8B VLM, which not only achieves significant overall performance improvement on ViFailback-Bench but also generates visual symbols for corrective action guidance. Finally, by integrating ViFailback-8B with a VLA model, we conduct real-world robotic experiments demonstrating its ability to assist the VLA model in recovering from failures. Project Website: https://x1nyuzhou.github.io/vifailback.github.io/", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.02787v2", "tags": ["LLM/VLA", "Manipulation", "Vision", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.02729v1", "type": "papers", "title": "RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning", "author": "Yuhong Zhang, Zihan Gao, Shengpeng Li", "abstract": "We introduce Robowheel, a data engine that converts human hand object interaction (HOI) videos into training-ready supervision for cross morphology robotic learning. From monocular RGB or RGB-D inputs, we perform high precision HOI reconstruction and enforce physical plausibility via a reinforcement learning (RL) optimizer that refines hand object relative poses under contact and penetration constraints. The reconstructed, contact rich trajectories are then retargeted to cross-embodiments, robot arms with simple end effectors, dexterous hands, and humanoids, yielding executable actions and rollouts. To scale coverage, we build a simulation-augmented framework on Isaac Sim with diverse domain randomization (embodiments, trajectories, object retrieval, background textures, hand motion mirroring), which enriches the distributions of trajectories and observations while preserving spatial relationships and physical plausibility. The entire data pipeline forms an end to end pipeline from video,reconstruction,retargeting,augmentation data acquisition. We validate the data on mainstream vision language action (VLA) and imitation learning architectures, demonstrating that trajectories produced by our pipeline are as stable as those from teleoperation and yield comparable continual performance gains. To our knowledge, this provides the first quantitative evidence that HOI modalities can serve as effective supervision for robotic learning. Compared with teleoperation, Robowheel is lightweight, a single monocular RGB(D) camera is sufficient to extract a universal, embodiment agnostic motion representation that could be flexibly retargeted across embodiments. We further assemble a large scale multimodal dataset combining multi-camera captures, monocular videos, and public HOI corpora for training and evaluating embodied models.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.02729v1", "tags": ["Humanoid", "Manipulation", "Vision", "Sim2Real", "LLM/VLA", "Reinforcement"], "source": "arXiv"}, {"id": "2512.02609v1", "type": "papers", "title": "SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction", "author": "Shengkai Wu, Jinrong Yang, Wenqiu Luo", "abstract": "Imitation learning for robotic grasping is often plagued by the multimodal problem: when a scene contains multiple valid targets, demonstrations of grasping different objects create conflicting training signals. Standard imitation learning policies fail by averaging these distinct actions into a single, invalid action. In this paper, we introduce SAM2Grasp, a novel framework that resolves this issue by reformulating the task as a uni-modal, prompt-conditioned prediction problem. Our method leverages the frozen SAM2 model to use its powerful visual temporal tracking capability and introduces a lightweight, trainable action head that operates in parallel with its native segmentation head. This design allows for training only the small action head on pre-computed temporal-visual features from SAM2. During inference, an initial prompt, such as a bounding box provided by an upstream object detection model, designates the specific object to be grasped. This prompt conditions the action head to predict a unique, unambiguous grasp trajectory for that object alone. In all subsequent video frames, SAM2's built-in temporal tracking capability automatically maintains stable tracking of the selected object, enabling our model to continuously predict the grasp trajectory from the video stream without further external guidance. This temporal-prompted approach effectively eliminates ambiguity from the visuomotor policy. We demonstrate through extensive experiments that SAM2Grasp achieves state-of-the-art performance in cluttered, multi-object grasping tasks.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.02609v1", "tags": ["Manipulation", "Reinforcement"], "source": "arXiv"}, {"id": "2512.02569v1", "type": "papers", "title": "Reframing Human-Robot Interaction Through Extended Reality: Unlocking Safer, Smarter, and More Empathic Interactions with Virtual Robots and Foundation Models", "author": "Yuchong Zhang, Yong Ma, Danica Kragic", "abstract": "This perspective reframes human-robot interaction (HRI) through extended reality (XR), arguing that virtual robots powered by large foundation models (FMs) can serve as cognitively grounded, empathic agents. Unlike physical robots, XR-native agents are unbound by hardware constraints and can be instantiated, adapted, and scaled on demand, while still affording embodiment and co-presence. We synthesize work across XR, HRI, and cognitive AI to show how such agents can support safety-critical scenarios, socially and cognitively empathic interaction across domains, and outreaching physical capabilities with XR and AI integration. We then discuss how multimodal large FMs (e.g., large language model, large vision model, and vision-language model) enable context-aware reasoning, affect-sensitive situations, and long-term adaptation, positioning virtual robots as cognitive and empathic mediators rather than mere simulation assets. At the same time, we highlight challenges and potential risks, including overtrust, cultural and representational bias, privacy concerns around biometric sensing, and data governance and transparency. The paper concludes by outlining a research agenda for human-centered, ethically grounded XR agents - emphasizing multi-layered evaluation frameworks, multi-user ecosystems, mixed virtual-physical embodiment, and societal and ethical design practices to envision XR-based virtual agents powered by FMs as reshaping future HRI into a more efficient and adaptive paradigm.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.02569v1", "tags": ["Vision", "Sim2Real", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.02458v1", "type": "papers", "title": "Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration", "author": "Zhongyi Cai, Yi Du, Chen Wang", "abstract": "Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.02458v1", "tags": ["Vision", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.02389v1", "type": "papers", "title": "Synthetic Error Injection Fails to Elicit Self-Correction In Language Models", "author": "David X. Wu, Shreyas Kapur, Anant Sahai", "abstract": "Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.02389v1", "tags": ["Reinforcement", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.02329v1", "type": "papers", "title": "Towards autonomous normative multi-agent systems for Human-AI software engineering teams", "author": "Hoa Khanh Dam, Geeta Mahala, Rashina Hoda", "abstract": "This paper envisions a transformative paradigm in software engineering, where Artificial Intelligence, embodied in fully autonomous agents, becomes the primary driver of the core software development activities. We introduce a new class of software engineering agents, empowered by Large Language Models and equipped with beliefs, desires, intentions, and memory to enable human-like reasoning. These agents collaborate with humans and other agents to design, implement, test, and deploy software systems with a level of speed, reliability, and adaptability far beyond the current software development processes. Their coordination and collaboration are governed by norms expressed as deontic modalities - commitments, obligations, prohibitions and permissions - that regulate interactions and ensure regulatory compliance. These innovations establish a scalable, transparent and trustworthy framework for future Human-AI software engineering teams.", "date": "2025-12-02", "url": "https://arxiv.org/pdf/2512.02329v1", "tags": ["Vision", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.02280v1", "type": "papers", "title": "Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence", "author": "Noorbakhsh Amiri Golilarz, Sindhuja Penchala, Shahram Rahimi", "abstract": "Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fundamentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self-monitoring, lack of meta-cognitive awareness, fixed and non-adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust generalization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.02280v1", "tags": ["Vision", "Reinforcement", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.02020v1", "type": "papers", "title": "EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI", "author": "Jianlei Chang, Ruofeng Mei, Wei Ke", "abstract": "Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.02020v1", "tags": ["Manipulation", "Reinforcement"], "source": "arXiv"}, {"id": "2512.02013v1", "type": "papers", "title": "ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation", "author": "Chenyang Gu, Jiaming Liu, Hao Chen", "abstract": "Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the \"how\" process from the \"what\" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.02013v1", "tags": ["Manipulation", "Vision", "Reinforcement", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.01996v1", "type": "papers", "title": "Learning Sim-to-Real Humanoid Locomotion in 15 Minutes", "author": "Younggyo Seo, Carmelo Sferrazza, Juyue Chen", "abstract": "Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive scale with thousands of parallel environments through carefully tuned design choices and minimalist reward functions. We demonstrate rapid end-to-end learning of humanoid locomotion controllers on Unitree G1 and Booster T1 robots under strong domain randomization, e.g., randomized dynamics, rough terrain, and push perturbations, as well as fast training of whole-body human-motion tracking policies. We provide videos and open-source implementation at: https://younggyo.me/fastsac-humanoid.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01996v1", "tags": ["Locomotion", "Humanoid", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.01946v2", "type": "papers", "title": "Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models", "author": "Paul Pacaud, Ricardo Garcia, Shizhe Chen", "abstract": "Robust robotic manipulation requires reliable failure detection and recovery. Although current Vision-Language Models (VLMs) show promise, their accuracy and generalization are limited by the scarcity of failure data. To address this data gap, we propose an automatic robot failure synthesis approach that procedurally perturbs successful trajectories to generate diverse planning and execution failures. This method produces not only binary classification labels but also fine-grained failure categories and step-by-step reasoning traces in both simulation and the real world. With it, we construct three new failure detection benchmarks: RLBench-Fail, BridgeDataV2-Fail, and UR5-Fail, substantially expanding the diversity and scale of existing failure datasets. We then train Guardian, a VLM with multi-view images for detailed failure reasoning and detection. Guardian achieves state-of-the-art performance on both existing and newly introduced benchmarks. It also effectively improves task success rates when integrated into a state-of-the-art manipulation system in simulation and real robots, demonstrating the impact of our generated failure data. Code, Data, and Models available at https://www.di.ens.fr/willow/research/guardian/.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01946v2", "tags": ["LLM/VLA", "Manipulation", "Vision", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.01924v1", "type": "papers", "title": "Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model", "author": "Kentaro Fujii, Shingo Murata", "abstract": "Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot. Results show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable. These findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01924v1", "tags": ["Manipulation", "Reinforcement"], "source": "arXiv"}, {"id": "2512.01908v1", "type": "papers", "title": "SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception", "author": "Gurmeher Khurana, Lan Wei, Dandan Zhang", "abstract": "Contact-rich robotic manipulation requires representations that encode local geometry. Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues. Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information. Most self-supervised learning (SSL) frameworks, however, compress feature maps into a global vector, discarding spatial structure and misaligning with the needs of manipulation. To address this, we propose SARL, a spatially-aware SSL framework that augments the Bootstrap Your Own Latent (BYOL) architecture with three map-level objectives, including Saliency Alignment (SAL), Patch-Prototype Distribution Alignment (PPDA), and Region Affinity Matching (RAM), to keep attentional focus, part composition, and geometric relations consistent across views. These losses act on intermediate feature maps, complementing the global objective. SARL consistently outperforms nine SSL baselines across six downstream tasks with fused visual-tactile data. On the geometry-sensitive edge-pose regression task, SARL achieves a Mean Absolute Error (MAE) of 0.3955, a 30% relative improvement over the next-best SSL method (0.5682 MAE) and approaching the supervised upper bound. These findings indicate that, for fused visual-tactile data, the most effective signal is structured spatial equivariance, in which features vary predictably with object geometry, which enables more capable robotic perception.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01908v1", "tags": ["Manipulation", "Vision", "Reinforcement"], "source": "arXiv"}, {"id": "2512.01889v1", "type": "papers", "title": "KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM", "author": "Zaid Nasser, Mikhail Iumanov, Tianhao Li", "abstract": "We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01889v1", "tags": ["Vision"], "source": "arXiv"}, {"id": "2512.01850v1", "type": "papers", "title": "Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching", "author": "Yue Pan, Tao Sun, Liyuan Zhu", "abstract": "Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01850v1", "tags": ["Vision", "Reinforcement"], "source": "arXiv"}, {"id": "2512.01801v2", "type": "papers", "title": "GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation", "author": "Yunfei Li, Xiao Ma, Jiafeng Xu", "abstract": "We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01801v2", "tags": ["Manipulation", "Vision", "Reinforcement", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.01773v1", "type": "papers", "title": "IGen: Scalable Data Generation for Robot Learning from Open-World Images", "author": "Chenghao Gu, Haolan Kang, Junchao Lin", "abstract": "The rise of generalist robotic policies has created an exponential demand for large-scale training data. However, on-robot data collection is labor-intensive and often limited to specific environments. In contrast, open-world images capture a vast diversity of real-world scenes that naturally align with robotic manipulation tasks, offering a promising avenue for low-cost, large-scale robot data acquisition. Despite this potential, the lack of associated robot actions hinders the practical use of open-world images for robot learning, leaving this rich visual resource largely unexploited. To bridge this gap, we propose IGen, a framework that scalably generates realistic visual observations and executable actions from open-world images. IGen first converts unstructured 2D pixels into structured 3D scene representations suitable for scene understanding and manipulation. It then leverages the reasoning capabilities of vision-language models to transform scene-specific task instructions into high-level plans and generate low-level actions as SE(3) end-effector pose sequences. From these poses, it synthesizes dynamic scene evolution and renders temporally coherent visual observations. Experiments validate the high quality of visuomotor data generated by IGen, and show that policies trained solely on IGen-synthesized data achieve performance comparable to those trained on real-world data. This highlights the potential of IGen to support scalable data generation from open-world images for generalist robotic policy training.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01773v1", "tags": ["Manipulation", "Vision", "Reinforcement", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.01715v1", "type": "papers", "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models", "author": "Wanpeng Zhang, Ye Wang, Hao Luo", "abstract": "Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01715v1", "tags": ["Manipulation", "Vision", "Reinforcement", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.01629v2", "type": "papers", "title": "SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge", "author": "Yumeng He, Ying Jiang, Jiayin Lu", "abstract": "Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling. Project page: https://heyumeng.com/SPARK/index.html.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01629v2", "tags": ["Manipulation", "Vision", "Sim2Real", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.01598v1", "type": "papers", "title": "A Cross-Embodiment Gripper Benchmark for Rigid-Object Manipulation in Aerial and Industrial Robotics", "author": "Marek Vagas, Martin Varga, Jaroslav Romancik", "abstract": "Robotic grippers are increasingly deployed across industrial, collaborative, and aerial platforms, where each embodiment imposes distinct mechanical, energetic, and operational constraints. Established YCB and NIST benchmarks quantify grasp success, force, or timing on a single platform, but do not evaluate cross-embodiment transferability or energy-aware performance, capabilities essential for modern mobile and aerial manipulation. This letter introduces the Cross-Embodiment Gripper Benchmark (CEGB), a compact and reproducible benchmarking suite extending YCB and selected NIST metrics with three additional components: a transfer-time benchmark measuring the practical effort required to exchange embodiments, an energy-consumption benchmark evaluating grasping and holding efficiency, and an intent-specific ideal payload assessment reflecting design-dependent operational capability. Together, these metrics characterize both grasp performance and the suitability of reusing a single gripper across heterogeneous robotic systems. A lightweight self-locking gripper prototype is implemented as a reference case. Experiments demonstrate rapid embodiment transfer (median ~= 17.6 s across user groups), low holding energy for gripper prototype (~= 1.5 J per 10 s), and consistent grasp performance with cycle times of 3.2 - 3.9 s and success rates exceeding 90%. CEGB thus provides a reproducible foundation for cross-platform, energy-aware evaluation of grippers in aerial and manipulators domains.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01598v1", "tags": ["Manipulation", "Sim2Real"], "source": "arXiv"}, {"id": "2512.01482v1", "type": "papers", "title": "On robotic manipulators with time-dependent inertial parameters: From physical consistency to boundedness of the mass matrix", "author": "Tom Kaufmann, Johann Reger", "abstract": "We generalize the robotics equation describing the dynamics of an open kinematic chain to include the effect of time-dependent change of inertial parameters as well as the effects of its cause, i.e. time dependency of the distributions of mass originating from parasitic movements of mass-carrying particles. The results generate insight that allows linking the novel concepts of uniform physical consistency and upper boundedness of inertial parameters -- ruling out approaching the edge to physical inconsistency or to diverge -- with the existence of finite, positive uniform bounds of the mass matrix.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01482v1", "tags": ["General"], "source": "arXiv"}, {"id": "2512.01446v1", "type": "papers", "title": "$\\mathbf{M^3A}$ Policy: Mutable Material Manipulation Augmentation Policy through Photometric Re-rendering", "author": "Jiayi Li, Yuxuan Hu, Haoran Geng", "abstract": "Material generalization is essential for real-world robotic manipulation, where robots must interact with objects exhibiting diverse visual and physical properties. This challenge is particularly pronounced for objects made of glass, metal, or other materials whose transparent or reflective surfaces introduce severe out-of-distribution variations. Existing approaches either rely on simulated materials in simulators and perform sim-to-real transfer, which is hindered by substantial visual domain gaps, or depend on collecting extensive real-world demonstrations, which is costly, time-consuming, and still insufficient to cover various materials. To overcome these limitations, we resort to computational photography and introduce Mutable Material Manipulation Augmentation (M$^3$A), a unified framework that leverages the physical characteristics of materials as captured by light transport for photometric re-rendering. The core idea is simple yet powerful: given a single real-world demonstration, we photometrically re-render the scene to generate a diverse set of highly realistic demonstrations with different material properties. This augmentation effectively decouples task-specific manipulation skills from surface appearance, enabling policies to generalize across materials without additional data collection. To systematically evaluate this capability, we construct the first comprehensive multi-material manipulation benchmark spanning both simulation and real-world environments. Extensive experiments show that the M$^3$A policy significantly enhances cross-material generalization, improving the average success rate across three real-world tasks by 58.03\\%, and demonstrating robust performance on previously unseen materials.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01446v1", "tags": ["Manipulation", "Reinforcement", "Sim2Real"], "source": "arXiv"}, {"id": "2512.01424v2", "type": "papers", "title": "\\textit{ViRectify}: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models", "author": "Xusen Hei, Jiali Chen, Jinyu Yang", "abstract": "As multimodal large language models (MLLMs) frequently exhibit errors in complex video reasoning scenarios, correcting these errors is critical for uncovering their weaknesses and improving performance. However, existing benchmarks lack systematic evaluation of MLLMs' ability to identify and correct these video reasoning errors. To bridge this gap, we propose \\textit{ViRectify}, a comprehensive benchmark to evaluate their fine-grained correction capability. Through an AI-assisted annotation pipeline with human verification, we construct a dataset of over 30\\textit{K} instances spanning dynamic perception, scientific reasoning, and embodied decision-making domains. In \\textit{ViRectify}, we challenge MLLMs to perform step-wise error identification and generate rationales with key video evidence grounding. In addition, we further propose the trajectory evidence-driven correction framework, comprising step-wise error trajectory and reward modeling on visual evidence-grounded correction. It encourages the model to explicitly concentrate on error propagation and key timestamps for correction. Extensive evaluation across 16 advanced MLLMs demonstrates that our \\textit{ViRectify} serves as a challenging testbed, where GPT-5 achieves only 31.94\\% correction accuracy. Our framework enables a Qwen2.5-VL-7B to consistently outperform the variants of 72B on \\textit{ViRectify}, showing the effectiveness of our approach. Further analysis uncovers systematic asymmetries in error correction across models, and our dataset is also a valuable data resource to perform reflection learning. We believe \\textit{ViRectify} provides a new direction for comprehensively evaluating the advanced MLLMs in video reasoning.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01424v2", "tags": ["Vision", "LLM/VLA"], "source": "arXiv"}, {"id": "2512.01383v1", "type": "papers", "title": "PointNet4D: A Lightweight 4D Point Cloud Video Backbone for Online and Offline Perception in Robotic Applications", "author": "Yunze Liu, Zifan Wang, Peiran Wu", "abstract": "Understanding dynamic 4D environments-3D space evolving over time-is critical for robotic and interactive systems. These applications demand systems that can process streaming point cloud video in real-time, often under resource constraints, while also benefiting from past and present observations when available. However, current 4D backbone networks rely heavily on spatiotemporal convolutions and Transformers, which are often computationally intensive and poorly suited to real-time applications. We propose PointNet4D, a lightweight 4D backbone optimized for both online and offline settings. At its core is a Hybrid Mamba-Transformer temporal fusion block, which integrates the efficient state-space modeling of Mamba and the bidirectional modeling power of Transformers. This enables PointNet4D to handle variable-length online sequences efficiently across different deployment scenarios. To enhance temporal understanding, we introduce 4DMAP, a frame-wise masked auto-regressive pretraining strategy that captures motion cues across frames. Our extensive evaluations across 9 tasks on 7 datasets, demonstrating consistent improvements across diverse domains. We further demonstrate PointNet4D's utility by building two robotic application systems: 4D Diffusion Policy and 4D Imitation Learning, achieving substantial gains on the RoboTwin and HandoverSim benchmarks.", "date": "2025-12-01", "url": "https://arxiv.org/pdf/2512.01383v1", "tags": ["Vision", "Reinforcement", "LLM/VLA"], "source": "arXiv"}, {"id": "198619957", "type": "projects", "title": "NestorDP/littlebot", "author": "NestorDP", "abstract": "Pacotes para o rob Littlebot", "date": "2025-12-04", "url": "https://github.com/NestorDP/littlebot", "stars": 10, "language": "C++", "tags": ["C++", "General"], "source": "GitHub"}, {"id": "1079661742", "type": "projects", "title": "whitedevil6242/budget-arduino-projects", "author": "whitedevil6242", "abstract": " Explore low-cost Arduino projects that enhance learning with easy-to-follow instructions, perfect for beginners and hobbyists.", "date": "2025-12-04", "url": "https://github.com/whitedevil6242/budget-arduino-projects", "stars": 0, "language": "C++", "tags": ["C++", "General"], "source": "GitHub"}, {"id": "526184903", "type": "projects", "title": "tanmay0145/px4-ros2-star-flight-mode", "author": "tanmay0145", "abstract": " Create custom flight modes for PX4 autopilot to fly in a 5-pointed star shape using ROS 2 with direct message publishing in Python.", "date": "2025-12-04", "url": "https://github.com/tanmay0145/px4-ros2-star-flight-mode", "stars": 2, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "1078817648", "type": "projects", "title": "john89472338/r-mel", "author": "john89472338", "abstract": " Analyze and visualize melodic structures in R, enhancing your music research and composition with powerful tools and insights.", "date": "2025-12-04", "url": "https://github.com/john89472338/r-mel", "stars": 0, "language": "N/A", "tags": [null, "General"], "source": "GitHub"}, {"id": "861606962", "type": "projects", "title": "AimRT/AimRT", "author": "AimRT", "abstract": "A high-performance runtime framework for modern robotics.", "date": "2025-12-04", "url": "https://github.com/AimRT/AimRT", "stars": 1257, "language": "C++", "tags": ["C++", "General"], "source": "GitHub"}, {"id": "359991260", "type": "projects", "title": "ItsKennethPlayz/lunarbot-smart-india-hackathon", "author": "ItsKennethPlayz", "abstract": " Convert markdown files to professional DOC and PDF formats easily, enhancing your document presentation with style and precision.", "date": "2025-12-04", "url": "https://github.com/ItsKennethPlayz/lunarbot-smart-india-hackathon", "stars": 0, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "1077771436", "type": "projects", "title": "ayoub147147/bip3d", "author": "ayoub147147", "abstract": " Bridge 2D images to 3D perception, enhancing embodied intelligence for robotics and computer vision tasks with BIP3D's innovative framework.", "date": "2025-12-04", "url": "https://github.com/ayoub147147/bip3d", "stars": 2, "language": "Python", "tags": ["Python", "Vision"], "source": "GitHub"}, {"id": "1077477186", "type": "projects", "title": "devdhananjay14/multim", "author": "devdhananjay14", "abstract": " Experiment with neural networks for binary classification on multimodal data using this extensible PyTorch framework.", "date": "2025-12-04", "url": "https://github.com/devdhananjay14/multim", "stars": 0, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "1108342143", "type": "projects", "title": "Bahna-Darius/NovaVision_BFMC", "author": "Bahna-Darius", "abstract": "Official repository for Team NovaVision competing in the Bosch Future Mobility Challenge (BFMC) 2025. This monorepo contains the complete software stack for our 1:10 scale autonomous vehicle, including perception, control, and embedded systems.", "date": "2025-12-04", "url": "https://github.com/Bahna-Darius/NovaVision_BFMC", "stars": 0, "language": "N/A", "tags": [null, "Vision"], "source": "GitHub"}, {"id": "1094294285", "type": "projects", "title": "l0uisgrange/mobile-2025-project", "author": "l0uisgrange", "abstract": "Semester project for the MICRO-452 Mobile Robotics course at EPFL", "date": "2025-12-04", "url": "https://github.com/l0uisgrange/mobile-2025-project", "stars": 0, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "1076336132", "type": "projects", "title": "LoGanNX1/TinyFOC-HAL", "author": "LoGanNX1", "abstract": " Simplify FOC implementation with TinyFOC-HAL, a practical guide that streamlines control for small motor projects using CubeMX configurations.", "date": "2025-12-04", "url": "https://github.com/LoGanNX1/TinyFOC-HAL", "stars": 1, "language": "C", "tags": ["C", "General"], "source": "GitHub"}, {"id": "365104576", "type": "projects", "title": "Cavilach/lidar_odometry", "author": "Cavilach", "abstract": " Achieve real-time LiDAR odometry with robust state estimation using Probabilistic Kernel Optimization for SLAM applications.", "date": "2025-12-04", "url": "https://github.com/Cavilach/lidar_odometry", "stars": 0, "language": "N/A", "tags": [null, "General"], "source": "GitHub"}, {"id": "1075624562", "type": "projects", "title": "brianneidklol/clover", "author": "brianneidklol", "abstract": " Build lightweight REST APIs and web apps with Clover PHP, a modern framework that combines Express.js simplicity with the power of PHP 8.4.", "date": "2025-12-04", "url": "https://github.com/brianneidklol/clover", "stars": 1, "language": "PHP", "tags": ["PHP", "General"], "source": "GitHub"}, {"id": "1055756035", "type": "projects", "title": "4thWaveAI/feeds", "author": "4thWaveAI", "abstract": "Future-tech intelligence platform delivering research feeds, analysis, and visualization across AI, Robotics, Quantum, Biotech, Nanotech.", "date": "2025-12-04", "url": "https://github.com/4thWaveAI/feeds", "stars": 1, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "989779503", "type": "projects", "title": "saltaher/challenge-tracker", "author": "saltaher", "abstract": " Track your personal challenges and boost your consistency with this simple web app designed for effective Frontend Development learning.", "date": "2025-12-04", "url": "https://github.com/saltaher/challenge-tracker", "stars": 1, "language": "JavaScript", "tags": ["JavaScript", "General"], "source": "GitHub"}, {"id": "1074723112", "type": "projects", "title": "Skw3mdy/Reinforcement-Learning-Projects", "author": "Skw3mdy", "abstract": " Explore reinforcement learning techniques with projects including a taxi agent using Q-Learning and a DQN-based Space Invaders agent.", "date": "2025-12-04", "url": "https://github.com/Skw3mdy/Reinforcement-Learning-Projects", "stars": 2, "language": "Jupyter Notebook", "tags": ["Jupyter Notebook", "Reinforcement"], "source": "GitHub"}, {"id": "845599017", "type": "projects", "title": "yaak-ai/rbyte", "author": "yaak-ai", "abstract": "Multimodal datasets for spatial intelligence", "date": "2025-12-04", "url": "https://github.com/yaak-ai/rbyte", "stars": 36, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "384626384", "type": "projects", "title": "jahanllol/kotlin-fpv", "author": "jahanllol", "abstract": " Build and manage FPV drone applications using Kotlin for efficient development and seamless integration with various hardware platforms.", "date": "2025-12-04", "url": "https://github.com/jahanllol/kotlin-fpv", "stars": 0, "language": "N/A", "tags": [null, "General"], "source": "GitHub"}, {"id": "1075128236", "type": "projects", "title": "wadeKeith/DeepThinkVLA", "author": "wadeKeith", "abstract": "DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models", "date": "2025-12-04", "url": "https://github.com/wadeKeith/DeepThinkVLA", "stars": 308, "language": "Python", "tags": ["Python", "Vision", "LLM/VLA"], "source": "GitHub"}, {"id": "1073372492", "type": "projects", "title": "chewbcca/plotly.cpp", "author": "chewbcca", "abstract": " Visualize data in C++ with Plotly.cpp, an interactive library for real-time and streaming graphical representation, leveraging the power of Plotly.js.", "date": "2025-12-04", "url": "https://github.com/chewbcca/plotly.cpp", "stars": 2, "language": "C++", "tags": ["C++", "General"], "source": "GitHub"}, {"id": "454087931", "type": "projects", "title": "ami-iit/jaxsim", "author": "ami-iit", "abstract": "A differentiable physics engine and multibody dynamics library for control and robot learning.", "date": "2025-12-04", "url": "https://github.com/ami-iit/jaxsim", "stars": 164, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "1073335480", "type": "projects", "title": "devilclover/pitlane-SGO", "author": "devilclover", "abstract": "", "date": "2025-12-04", "url": "https://github.com/devilclover/pitlane-SGO", "stars": 0, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "1000525788", "type": "projects", "title": "SHasnainALi/basekit", "author": "SHasnainALi", "abstract": "", "date": "2025-12-04", "url": "https://github.com/SHasnainALi/basekit", "stars": 0, "language": "CSS", "tags": ["CSS", "General"], "source": "GitHub"}, {"id": "1000500514", "type": "projects", "title": "5321444/pitlane-attest", "author": "5321444", "abstract": " Ensure robot safety with cryptographic attestations, telemetry integrity, and audit trails for compliance in production environments.", "date": "2025-12-04", "url": "https://github.com/5321444/pitlane-attest", "stars": 0, "language": "N/A", "tags": [null, "General"], "source": "GitHub"}, {"id": "422584085", "type": "projects", "title": "yyyanbj/arxiv-daily", "author": "yyyanbj", "abstract": " Automatically Update Some Fields Papers Daily using Github Actions (Update Every 12th hours)", "date": "2025-12-04", "url": "https://github.com/yyyanbj/arxiv-daily", "stars": 179, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "1072538821", "type": "projects", "title": "zGonzax/sj.h", "author": "zGonzax", "abstract": " Parse JSON effortlessly with sj.h, a compact C99 library that offers zero-allocations and precise error reporting for your project.", "date": "2025-12-04", "url": "https://github.com/zGonzax/sj.h", "stars": 1, "language": "C", "tags": ["C", "General"], "source": "GitHub"}, {"id": "1068577131", "type": "projects", "title": "Synria-Robotics/RobotCore", "author": "Synria-Robotics", "abstract": "Unified High-Throughput Robotics Library", "date": "2025-12-04", "url": "https://github.com/Synria-Robotics/RobotCore", "stars": 2, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "474706547", "type": "projects", "title": "ZPider0/Multimodal", "author": "ZPider0", "abstract": " Transform speech and text with this lightweight Python toolkit for transcription, analysis, and audio conversion tasks.", "date": "2025-12-04", "url": "https://github.com/ZPider0/Multimodal", "stars": 2, "language": "Jupyter Notebook", "tags": ["Jupyter Notebook", "General"], "source": "GitHub"}, {"id": "1070959556", "type": "projects", "title": "SodelixHurusop/ECE_F_CRT_PYTHON", "author": "SodelixHurusop", "abstract": " Streamline your embedded systems tasks with ECE_F_CRT_PYTHON, a Python library designed for efficient coding in electronics and control applications.", "date": "2025-12-04", "url": "https://github.com/SodelixHurusop/ECE_F_CRT_PYTHON", "stars": 0, "language": "N/A", "tags": [null, "General"], "source": "GitHub"}, {"id": "721276522", "type": "projects", "title": "gustavosousass/awesome-3d-in-the-wild", "author": "gustavosousass", "abstract": " Discover advancements in 3D scene understanding with LiDAR techniques for semantic and panoptic segmentation, plus occupancy prediction.", "date": "2025-12-04", "url": "https://github.com/gustavosousass/awesome-3d-in-the-wild", "stars": 2, "language": "HTML", "tags": ["HTML", "Vision"], "source": "GitHub"}, {"id": "1074925485", "type": "projects", "title": "jssfinhoap22/MesaTask", "author": "jssfinhoap22", "abstract": " Generate tabletop scenes using task-driven 3D spatial reasoning for enhanced scene understanding and interaction in various applications.", "date": "2025-12-04", "url": "https://github.com/jssfinhoap22/MesaTask", "stars": 2, "language": "Python", "tags": ["Python", "Vision"], "source": "GitHub"}, {"id": "230744355", "type": "projects", "title": "Maheee000/embodied-temporal-reasoning", "author": "Maheee000", "abstract": " Enhance embodied AI with continuous vision-language understanding for dynamic environment adaptation and achieve accurate multi-step temporal reasoning.", "date": "2025-12-04", "url": "https://github.com/Maheee000/embodied-temporal-reasoning", "stars": 3, "language": "Python", "tags": ["Python", "Vision"], "source": "GitHub"}, {"id": "1109798151", "type": "projects", "title": "AdnanSattar/Spatial-RAG-Worldmodel", "author": "AdnanSattar", "abstract": "A Spatial Retrieval-Augmented Generation system for latent world models, designed for embodied spatial intelligence in robotics, autonomous navigation, and embodied AI. Features ROS2 integration, real-time inference @ 25Hz, and complete robot build guide.", "date": "2025-12-04", "url": "https://github.com/AdnanSattar/Spatial-RAG-Worldmodel", "stars": 0, "language": "Python", "tags": ["Python", "Reinforcement"], "source": "GitHub"}, {"id": "1037741738", "type": "projects", "title": "RLinf/RLinf", "author": "RLinf", "abstract": "RLinf is a flexible and scalable open-source infrastructure designed for post-training foundation models (LLMs, VLMs, VLAs) via reinforcement learning.", "date": "2025-12-04", "url": "https://github.com/RLinf/RLinf", "stars": 1560, "language": "Python", "tags": ["Python", "Reinforcement", "LLM/VLA"], "source": "GitHub"}, {"id": "999550439", "type": "projects", "title": "SIBench/Awesome-Visual-Spatial-Reasoning", "author": "SIBench", "abstract": "This is a project about visual spatial reasoning.", "date": "2025-12-04", "url": "https://github.com/SIBench/Awesome-Visual-Spatial-Reasoning", "stars": 81, "language": "HTML", "tags": ["HTML", "General"], "source": "GitHub"}, {"id": "1087207313", "type": "projects", "title": "knightnemo/Awesome-World-Models", "author": "knightnemo", "abstract": "A Curated List of Awesome Works in World Modeling, Aiming to Serve as a One-stop Resource for Researchers, Practitioners, and Enthusiasts Interested in World Modeling.", "date": "2025-12-04", "url": "https://github.com/knightnemo/Awesome-World-Models", "stars": 1305, "language": "N/A", "tags": [null, "Reinforcement"], "source": "GitHub"}, {"id": "1082733610", "type": "projects", "title": "DexForce/EmbodiChain", "author": "DexForce", "abstract": "An end-to-end, GPU-accelerated, and modular platform for building generalized Embodied Intelligence.", "date": "2025-12-04", "url": "https://github.com/DexForce/EmbodiChain", "stars": 21, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "919463967", "type": "projects", "title": "KoHrABoT/OneOcc", "author": "KoHrABoT", "abstract": " Enhance legged robots' navigation with OneOcc, a lightweight framework for semantic occupancy prediction using a single panoramic camera.", "date": "2025-12-04", "url": "https://github.com/KoHrABoT/OneOcc", "stars": 0, "language": "N/A", "tags": [null, "Locomotion", "Vision"], "source": "GitHub"}, {"id": "1105094276", "type": "projects", "title": "Unresponsive-in384/temporal_reasoning_vision_system", "author": "Unresponsive-in384", "abstract": " Enhance computer vision with temporal reasoning for deeper understanding of video sequences, causal analysis, and event prediction.", "date": "2025-12-04", "url": "https://github.com/Unresponsive-in384/temporal_reasoning_vision_system", "stars": 0, "language": "Python", "tags": ["Python", "Vision"], "source": "GitHub"}, {"id": "460615268", "type": "projects", "title": "dora-rs/dora", "author": "dora-rs", "abstract": "DORA (Dataflow-Oriented Robotic Architecture) is middleware designed to streamline and simplify the creation of AI-based robotic applications. It offers low latency, composable, and distributed dataflow capabilities. Applications are modeled as directed graphs, also referred to as pipelines.", "date": "2025-12-04", "url": "https://github.com/dora-rs/dora", "stars": 2715, "language": "Rust", "tags": ["Rust", "General"], "source": "GitHub"}, {"id": "885743839", "type": "projects", "title": "leofan90/Awesome-World-Models", "author": "leofan90", "abstract": "A comprehensive list of papers for the definition of World Models and using World Models for General Video Generation, Embodied AI, and Autonomous Driving, including papers, codes, and related websites.", "date": "2025-12-04", "url": "https://github.com/leofan90/Awesome-World-Models", "stars": 850, "language": "N/A", "tags": [null, "Reinforcement"], "source": "GitHub"}, {"id": "1105050193", "type": "projects", "title": "ssrpw2/human-ai-translation-dictionary", "author": "ssrpw2", "abstract": "A reference point for phenomena that have been reported to occur inside AI systems but have no direct mapping into natural language. ", "date": "2025-12-04", "url": "https://github.com/ssrpw2/human-ai-translation-dictionary", "stars": 0, "language": "N/A", "tags": [null, "General"], "source": "GitHub"}, {"id": "1035313184", "type": "projects", "title": "threetau/kinitro", "author": "threetau", "abstract": "Kinitro: Accelerating Robotic Intelligence", "date": "2025-12-04", "url": "https://github.com/threetau/kinitro", "stars": 5, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "810385004", "type": "projects", "title": "RobotecAI/rai", "author": "RobotecAI", "abstract": "RAI is a vendor-agnostic agentic framework for robotics, utilizing ROS 2 tools to perform complex actions, defined scenarios, free interface execution, log summaries, voice interaction and more.", "date": "2025-12-03", "url": "https://github.com/RobotecAI/rai", "stars": 420, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "801867971", "type": "projects", "title": "TianxingChen/Embodied-AI-Guide", "author": "TianxingChen", "abstract": "[Lumina Embodied AI]  Embodied-AI-Guide", "date": "2025-12-04", "url": "https://github.com/TianxingChen/Embodied-AI-Guide", "stars": 9451, "language": "N/A", "tags": [null, "General"], "source": "GitHub"}, {"id": "1109039439", "type": "projects", "title": "avanturist322/awesome-memory-vla", "author": "avanturist322", "abstract": "Awesome Memory-VLA: A curated list of Visual-Language-Action models with memory", "date": "2025-12-03", "url": "https://github.com/avanturist322/awesome-memory-vla", "stars": 7, "language": "N/A", "tags": [null, "LLM/VLA"], "source": "GitHub"}, {"id": "1107536613", "type": "projects", "title": "cdb342/OccStudio", "author": "cdb342", "abstract": "A unified framework for 3D Occupancy Prediction", "date": "2025-12-03", "url": "https://github.com/cdb342/OccStudio", "stars": 9, "language": "Python", "tags": ["Python", "Vision"], "source": "GitHub"}, {"id": "1080936020", "type": "projects", "title": "Robin-WZQ/Awesome-Backdoor-on-LMMs", "author": "Robin-WZQ", "abstract": "Awesome-Backdoor-on-LMMs is a collection of state-of-the-art, novel, exciting backdoor methods on LLMs. It contains papers, codes and datasets.", "date": "2025-12-03", "url": "https://github.com/Robin-WZQ/Awesome-Backdoor-on-LMMs", "stars": 2, "language": "N/A", "tags": [null, "LLM/VLA"], "source": "GitHub"}, {"id": "790280061", "type": "projects", "title": "rh20624/Awesome-IMU-Sensing", "author": "rh20624", "abstract": "A collection of datasets, papers, and resources  for Generalizable Human Activity Recognition and IMU sensing.", "date": "2025-12-04", "url": "https://github.com/rh20624/Awesome-IMU-Sensing", "stars": 122, "language": "N/A", "tags": [null, "General"], "source": "GitHub"}, {"id": "1065553327", "type": "projects", "title": "Evan-wyl/humanoid-robot-intelligence", "author": "Evan-wyl", "abstract": "humanoid robot intelligence papers", "date": "2025-12-02", "url": "https://github.com/Evan-wyl/humanoid-robot-intelligence", "stars": 1, "language": "N/A", "tags": [null, "Humanoid"], "source": "GitHub"}, {"id": "813930147", "type": "projects", "title": "MSR3D/MSR3D", "author": "MSR3D", "abstract": "[NeurIPS 2024] Official code repository for MSR3D paper", "date": "2025-12-02", "url": "https://github.com/MSR3D/MSR3D", "stars": 68, "language": "Python", "tags": ["Python", "Vision"], "source": "GitHub"}, {"id": "859294552", "type": "projects", "title": "RoboTwin-Platform/RoboTwin", "author": "RoboTwin-Platform", "abstract": "RoboTwin 2.0 Offical Repo", "date": "2025-12-04", "url": "https://github.com/RoboTwin-Platform/RoboTwin", "stars": 1715, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "885265417", "type": "projects", "title": "cdb342/ALOcc", "author": "cdb342", "abstract": "[ICCV 2025] ALOcc: Adaptive Lifting-based 3D Semantic Occupancy and Cost Volume-based Flow Prediction", "date": "2025-12-03", "url": "https://github.com/cdb342/ALOcc", "stars": 32, "language": "Python", "tags": ["Python", "Vision"], "source": "GitHub"}, {"id": "1094047145", "type": "projects", "title": "herrkaefer/vibe-narrator", "author": "herrkaefer", "abstract": "Give your coding agent a voice with personality!", "date": "2025-12-02", "url": "https://github.com/herrkaefer/vibe-narrator", "stars": 0, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "1061157005", "type": "projects", "title": "Grigorij-Dudnik/RoboCrew", "author": "Grigorij-Dudnik", "abstract": "Set up your embodied LLM agent with the same ease as normal agents in CrewAI or Autogen", "date": "2025-11-30", "url": "https://github.com/Grigorij-Dudnik/RoboCrew", "stars": 20, "language": "Python", "tags": ["Python", "LLM/VLA"], "source": "GitHub"}, {"id": "614178379", "type": "projects", "title": "rllab-snu/RNR-Map", "author": "rllab-snu", "abstract": "Official Github repository for\"Renderable Neural Radiance Map for Visual Navigation\". (CVPR 2023 Highlight)", "date": "2025-11-30", "url": "https://github.com/rllab-snu/RNR-Map", "stars": 143, "language": "Python", "tags": ["Python", "Reinforcement"], "source": "GitHub"}, {"id": "954538244", "type": "projects", "title": "minnie-lin/Awesome-Physics-Cognition-based-Video-Generation", "author": "minnie-lin", "abstract": "A comprehensive list of papers investigating physical cognition in video generation, including papers, codes, and related websites.", "date": "2025-12-04", "url": "https://github.com/minnie-lin/Awesome-Physics-Cognition-based-Video-Generation", "stars": 216, "language": "N/A", "tags": [null, "General"], "source": "GitHub"}, {"id": "1072350532", "type": "projects", "title": "Shreemahor/GPT-On-Wheels", "author": "Shreemahor", "abstract": "My homemade autonomous AI robot: What happens when you put a pan-tilt camera, speaker & mic, tof distance sensors, environmental sensors, and more on omni-directional wheels and then let an AI control the body?", "date": "2025-11-27", "url": "https://github.com/Shreemahor/GPT-On-Wheels", "stars": 0, "language": "Python", "tags": ["Python", "Vision"], "source": "GitHub"}, {"id": "1090939101", "type": "projects", "title": "shiven2001/syndefect3d", "author": "shiven2001", "abstract": "SynDefect3D", "date": "2025-11-27", "url": "https://github.com/shiven2001/syndefect3d", "stars": 0, "language": "C++", "tags": ["C++", "Vision"], "source": "GitHub"}, {"id": "1038176853", "type": "projects", "title": "chengame/vscode-control", "author": "chengame", "abstract": "VS Code - Control is a macOS Raycast extension to interact with VS Code and editors. It lists commands, npm scripts, Flutter devices, opens settings. ", "date": "2025-12-04", "url": "https://github.com/chengame/vscode-control", "stars": 0, "language": "JavaScript", "tags": ["JavaScript", "General"], "source": "GitHub"}, {"id": "964637285", "type": "projects", "title": "Ruda1106/SocketShroud", "author": "Ruda1106", "abstract": "LAN-level socket cloak and manipulation toolkit", "date": "2025-12-04", "url": "https://github.com/Ruda1106/SocketShroud", "stars": 0, "language": "Python", "tags": ["Python", "Manipulation"], "source": "GitHub"}, {"id": "958377083", "type": "projects", "title": "ZizoTheDev/ffmpeg-mcp", "author": "ZizoTheDev", "abstract": "An MCP server for FFmpeg", "date": "2025-12-04", "url": "https://github.com/ZizoTheDev/ffmpeg-mcp", "stars": 15, "language": "TypeScript", "tags": ["TypeScript", "General"], "source": "GitHub"}, {"id": "756904357", "type": "projects", "title": "agimus-project/agimus_controller", "author": "agimus-project", "abstract": "Whole Body Model Predictive Control in the AGIMUS architecture", "date": "2025-12-04", "url": "https://github.com/agimus-project/agimus_controller", "stars": 6, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "52715040", "type": "projects", "title": "shenwei356/seqkit", "author": "shenwei356", "abstract": "A cross-platform and ultrafast toolkit for FASTA/Q file manipulation", "date": "2025-12-04", "url": "https://github.com/shenwei356/seqkit", "stars": 1488, "language": "Go", "tags": ["Go", "Manipulation"], "source": "GitHub"}, {"id": "1063851187", "type": "projects", "title": "2toinf/X-VLA", "author": "2toinf", "abstract": "The offical Implementation of \"Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model\"", "date": "2025-12-04", "url": "https://github.com/2toinf/X-VLA", "stars": 297, "language": "C++", "tags": ["C++", "Vision", "LLM/VLA"], "source": "GitHub"}, {"id": "1103185440", "type": "projects", "title": "JinxSeven/ml-sandbox", "author": "JinxSeven", "abstract": "A collection of Jupyter notebooks, scripts, and small projects for learning and experimenting with machine learning concepts. This repo is a sandbox for exploring algorithms, libraries, and techniques through focused experiments.", "date": "2025-12-03", "url": "https://github.com/JinxSeven/ml-sandbox", "stars": 0, "language": "Jupyter Notebook", "tags": ["Jupyter Notebook", "General"], "source": "GitHub"}, {"id": "833368311", "type": "projects", "title": "aminekun90/advanced-color-utils", "author": "aminekun90", "abstract": "Unleash the full potential of color manipulation with the ColorUtils library! Designed for developers who need precise control over color processing  ", "date": "2025-12-03", "url": "https://github.com/aminekun90/advanced-color-utils", "stars": 1, "language": "TypeScript", "tags": ["TypeScript", "Manipulation"], "source": "GitHub"}, {"id": "1050495907", "type": "projects", "title": "Agdistys/Schemas", "author": "Agdistys", "abstract": " Agdistys  Diane Serant - J'update mes schmas en permanence. Lien de la Galerie : https://Agdistys.github.io/Schemas/  Lien de mon LinkTree : https://Agdistys.github.io/Schemas/charte-ami.html - Travail sous licence Creative Commons Attribution - ShareAlike 4.0 International (CC BY-SA 4.0). https://creativecommons.org/licenses/by-sa/4.0/ ", "date": "2025-12-03", "url": "https://github.com/Agdistys/Schemas", "stars": 0, "language": "HTML", "tags": ["HTML", "General"], "source": "GitHub"}, {"id": "988069032", "type": "projects", "title": "philfung/awesome-reliable-robotics", "author": "philfung", "abstract": "Robotics research demonstrating reliability and robustness in the real world (continuously updated)", "date": "2025-12-02", "url": "https://github.com/philfung/awesome-reliable-robotics", "stars": 113, "language": "N/A", "tags": [null, "Reinforcement"], "source": "GitHub"}, {"id": "950995063", "type": "projects", "title": "KNEEpoleon/Boneparte", "author": "KNEEpoleon", "abstract": "BONE. P.recision A.ugmented R.eality T.racking E.quipment", "date": "2025-12-02", "url": "https://github.com/KNEEpoleon/Boneparte", "stars": 4, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "879219215", "type": "projects", "title": "fest-live/dom.ts", "author": "fest-live", "abstract": " DOM Utilities ", "date": "2025-12-02", "url": "https://github.com/fest-live/dom.ts", "stars": 2, "language": "TypeScript", "tags": ["TypeScript", "General"], "source": "GitHub"}, {"id": "794915217", "type": "projects", "title": "CroboticSolutions/arm_api2", "author": "CroboticSolutions", "abstract": "Code wrapper for MoveIt2! and ROS 2.", "date": "2025-12-02", "url": "https://github.com/CroboticSolutions/arm_api2", "stars": 84, "language": "C++", "tags": ["C++", "General"], "source": "GitHub"}, {"id": "1085126777", "type": "projects", "title": "legalaspro/manipulation_project", "author": "legalaspro", "abstract": "ROS 2 manipulation system for UR3E with vision-guided pick-and-place, motion planning, and gripper control. Simulation + real hardware ready.", "date": "2025-12-02", "url": "https://github.com/legalaspro/manipulation_project", "stars": 0, "language": "C++", "tags": ["C++", "Manipulation", "Vision", "Sim2Real"], "source": "GitHub"}, {"id": "491699829", "type": "projects", "title": "google/pyglove", "author": "google", "abstract": "Manipulating Python Programs", "date": "2025-12-01", "url": "https://github.com/google/pyglove", "stars": 700, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "970597960", "type": "projects", "title": "InternRobotics/GenManip", "author": "InternRobotics", "abstract": "[CVPR 2025] Official implementation of \"GenManip: LLM-driven Simulation for Generalizable Instruction-Following Manipulation\"", "date": "2025-12-04", "url": "https://github.com/InternRobotics/GenManip", "stars": 114, "language": "Python", "tags": ["Python", "Manipulation", "Sim2Real", "LLM/VLA"], "source": "GitHub"}, {"id": "371028278", "type": "projects", "title": "easystats/datawizard", "author": "easystats", "abstract": "Magic potions to clean and transform your data  ", "date": "2025-12-01", "url": "https://github.com/easystats/datawizard", "stars": 233, "language": "R", "tags": ["R", "General"], "source": "GitHub"}, {"id": "992630802", "type": "projects", "title": "ReinFlow/ReinFlow", "author": "ReinFlow", "abstract": "[NeurIPS 2025] Flow x RL. \"ReinFlow: Fine-tuning Flow Policy with Online Reinforcement Learning\". Support VLAs e.g., pi0, pi0.5. Fully open-sourced. ", "date": "2025-12-03", "url": "https://github.com/ReinFlow/ReinFlow", "stars": 185, "language": "Python", "tags": ["Python", "Reinforcement", "LLM/VLA"], "source": "GitHub"}, {"id": "898546391", "type": "projects", "title": "vuebro/flat-json-tree", "author": "vuebro", "abstract": "A Vue 3 composable that transforms JSON tree objects into flat arrays for easy manipulation with standard array operations while maintaining tree structure through computed properties", "date": "2025-11-28", "url": "https://github.com/vuebro/flat-json-tree", "stars": 0, "language": "TypeScript", "tags": ["TypeScript", "Manipulation"], "source": "GitHub"}, {"id": "1104737428", "type": "projects", "title": "AKS-Lab-Univertsity-of-Tartu/bimanual_manipulation", "author": "AKS-Lab-Univertsity-of-Tartu", "abstract": "This repository is for tracking progress of bimanual manipulation for different tasks. ", "date": "2025-12-04", "url": "https://github.com/AKS-Lab-Univertsity-of-Tartu/bimanual_manipulation", "stars": 11, "language": "Jupyter Notebook", "tags": ["Jupyter Notebook", "Manipulation"], "source": "GitHub"}, {"id": "1006688475", "type": "projects", "title": "ian-chuang/gaze-av-aloha", "author": "ian-chuang", "abstract": "Code for paper: \"Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers\"", "date": "2025-11-30", "url": "https://github.com/ian-chuang/gaze-av-aloha", "stars": 21, "language": "Jupyter Notebook", "tags": ["Jupyter Notebook", "Vision", "LLM/VLA"], "source": "GitHub"}, {"id": "1104770614", "type": "projects", "title": "nvidia-isaac/cumotion", "author": "nvidia-isaac", "abstract": "cuMotion: GPU-Accelerated Motion Generation for Robotics", "date": "2025-12-04", "url": "https://github.com/nvidia-isaac/cumotion", "stars": 11, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "987598518", "type": "projects", "title": "jiaming-zhou/X-ICM", "author": "jiaming-zhou", "abstract": "official repo for AGNOSTOS, a cross-task manipulation benchmark, and X-ICM method, a cross-task in-context manipulation (VLA) method", "date": "2025-12-02", "url": "https://github.com/jiaming-zhou/X-ICM", "stars": 51, "language": "Python", "tags": ["Python", "Manipulation", "LLM/VLA"], "source": "GitHub"}, {"id": "1043996645", "type": "projects", "title": "ansh1113/humanoid-motion-planning", "author": "ansh1113", "abstract": "Humanoid whole-body motion planning with ZMP constraints - 40% task improvement, 15% faster execution, complete implementation with trajectory optimization and kinematics", "date": "2025-11-24", "url": "https://github.com/ansh1113/humanoid-motion-planning", "stars": 0, "language": "Python", "tags": ["Python", "Humanoid"], "source": "GitHub"}, {"id": "941260074", "type": "projects", "title": "hsp-iit/HannesImitation", "author": "hsp-iit", "abstract": "HannesImitation is an imitation learning approach to control the Hannes prosthetic hand with a single Diffusion Policy to grasp several objects in diverse scenarios.", "date": "2025-11-24", "url": "https://github.com/hsp-iit/HannesImitation", "stars": 6, "language": "Jupyter Notebook", "tags": ["Jupyter Notebook", "Reinforcement"], "source": "GitHub"}, {"id": "769340584", "type": "projects", "title": "rise-policy/RISE", "author": "rise-policy", "abstract": "[IROS 2024]  RISE: 3D Perception Makes Real-World Robot Imitation Simple and Effective", "date": "2025-11-26", "url": "https://github.com/rise-policy/RISE", "stars": 138, "language": "Python", "tags": ["Python", "Vision", "Reinforcement"], "source": "GitHub"}, {"id": "1031852861", "type": "projects", "title": "Guowei-Zou/d2ppo-release", "author": "Guowei-Zou", "abstract": "[AAAI 2026] DPPO: Diffusion Policy Policy Optimization with Dispersive Loss.", "date": "2025-12-04", "url": "https://github.com/Guowei-Zou/d2ppo-release", "stars": 35, "language": "Python", "tags": ["Python", "Reinforcement"], "source": "GitHub"}, {"id": "1090916988", "type": "projects", "title": "kalaiselvan-t/vision-sorter", "author": "kalaiselvan-t", "abstract": "Pick and place automation for Franka FR3 using ROS2, MoveIt Task Constructor and RGBD object detection ", "date": "2025-11-21", "url": "https://github.com/kalaiselvan-t/vision-sorter", "stars": 1, "language": "C++", "tags": ["C++", "Manipulation", "Vision"], "source": "GitHub"}, {"id": "496565760", "type": "projects", "title": "crwlrsoft/query-string", "author": "crwlrsoft", "abstract": "A library for convenient handling of query strings used in HTTP requests.", "date": "2025-11-21", "url": "https://github.com/crwlrsoft/query-string", "stars": 20, "language": "PHP", "tags": ["PHP", "General"], "source": "GitHub"}, {"id": "377937374", "type": "projects", "title": "supermarsx/writetofillet", "author": "supermarsx", "abstract": "Fast and Multithreaded File Pumper", "date": "2025-11-20", "url": "https://github.com/supermarsx/writetofillet", "stars": 1, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "1062212552", "type": "projects", "title": "Souhailahh/ICML-2025-Papers", "author": "Souhailahh", "abstract": " Curate and access papers for the ICML 2025 conference, ensuring easy navigation and exploration of cutting-edge research in machine learning.", "date": "2025-12-04", "url": "https://github.com/Souhailahh/ICML-2025-Papers", "stars": 0, "language": "N/A", "tags": [null, "General"], "source": "GitHub"}, {"id": "1096183898", "type": "projects", "title": "sisterly-neck577/IsaacSim", "author": "sisterly-neck577", "abstract": " Develop, test, and deploy AI-powered robot simulations in realistic environments using NVIDIA Isaac Sim on the Omniverse platform.", "date": "2025-12-04", "url": "https://github.com/sisterly-neck577/IsaacSim", "stars": 0, "language": "Python", "tags": ["Python", "Reinforcement", "Sim2Real"], "source": "GitHub"}, {"id": "497997070", "type": "projects", "title": "minheinchay/g1_spinkick_example", "author": "minheinchay", "abstract": " Teach the Unitree G1 humanoid to perform a double spin kick using mjlab, with training data, pretrained models, and deployment instructions.", "date": "2025-12-04", "url": "https://github.com/minheinchay/g1_spinkick_example", "stars": 1, "language": "Python", "tags": ["Python", "Humanoid"], "source": "GitHub"}, {"id": "1105510715", "type": "projects", "title": "GH-X-ST/Nausicaa", "author": "GH-X-ST", "abstract": "Sim-to-Real Transfer and Co-Design Optimisation of a Small-Scale Glider", "date": "2025-12-03", "url": "https://github.com/GH-X-ST/Nausicaa", "stars": 1, "language": "Python", "tags": ["Python", "Sim2Real"], "source": "GitHub"}, {"id": "1001198342", "type": "projects", "title": "BenBenyamin/ArcadeRobot", "author": "BenBenyamin", "abstract": "", "date": "2025-12-03", "url": "https://github.com/BenBenyamin/ArcadeRobot", "stars": 0, "language": "Jupyter Notebook", "tags": ["Jupyter Notebook", "General"], "source": "GitHub"}, {"id": "1019503597", "type": "projects", "title": "ROBOTIS-GIT/robotis_lab", "author": "ROBOTIS-GIT", "abstract": "This repository provides tutorials for reinforcement learning and imitation learning using ROBOTIS robots, and supports Sim2Real functionality for deploying the learned policies on real robots.", "date": "2025-12-04", "url": "https://github.com/ROBOTIS-GIT/robotis_lab", "stars": 75, "language": "Python", "tags": ["Python", "Reinforcement", "Sim2Real"], "source": "GitHub"}, {"id": "1100927244", "type": "projects", "title": "KYH-99/Nvidia-Isaac-Sim---Isaac-Lab-Dev", "author": "KYH-99", "abstract": "A research and development repository focused on Physical AI, realistic physics simulation, and reinforcement learning for JetBot autonomous driving using NVIDIA Isaac Sim and Isaac Lab.", "date": "2025-11-24", "url": "https://github.com/KYH-99/Nvidia-Isaac-Sim---Isaac-Lab-Dev", "stars": 1, "language": "N/A", "tags": [null, "Reinforcement", "Sim2Real"], "source": "GitHub"}, {"id": "1073406342", "type": "projects", "title": "mujocolab/g1_spinkick_example", "author": "mujocolab", "abstract": "Train a Unitree G1 humanoid to perform a double spin kick using mjlab", "date": "2025-12-03", "url": "https://github.com/mujocolab/g1_spinkick_example", "stars": 173, "language": "Python", "tags": ["Python", "Humanoid"], "source": "GitHub"}, {"id": "1100777633", "type": "projects", "title": "Sha01in/world-models-jax", "author": "Sha01in", "abstract": "World Models (Ha & Schmidhuber) reproduced in JAX/Equinox. Features parallelized 'dream' training (256 agents), asymmetric loss for Sim2Real transfer, and active failure learning.", "date": "2025-11-22", "url": "https://github.com/Sha01in/world-models-jax", "stars": 0, "language": "Python", "tags": ["Python", "Reinforcement", "Sim2Real"], "source": "GitHub"}, {"id": "956670330", "type": "projects", "title": "evronix/quadruped_sim2sim", "author": "evronix", "abstract": "Deploying a quadruped robot policy(DreamWaQ, walk-these-ways, HIMLoco) trained in Isaacgym to ROS2 Gazebo for Sim-to-Sim", "date": "2025-12-02", "url": "https://github.com/evronix/quadruped_sim2sim", "stars": 32, "language": "Python", "tags": ["Python", "Locomotion", "Reinforcement"], "source": "GitHub"}, {"id": "1072718529", "type": "projects", "title": "Extwin-Synthesis/Synthesis-Assets-Explorer", "author": "Extwin-Synthesis", "abstract": "Synthesis Asset Explorer is an Omniverse&Isaac sim extension for loading high quality open source USD asserts, including photorealistic articulated sim-ready assets,  architectural models, 3D Gaussian Splatting models, and 1:1 real-world fully interactive simulation scenes from", "date": "2025-12-02", "url": "https://github.com/Extwin-Synthesis/Synthesis-Assets-Explorer", "stars": 7, "language": "Python", "tags": ["Python", "Vision", "Reinforcement", "Sim2Real"], "source": "GitHub"}, {"id": "869798157", "type": "projects", "title": "GunesCaginAydin/dynamical-metalearning", "author": "GunesCaginAydin", "abstract": "Comparitive work on dynamical identification that extends previous in-context learning based identification methods to different neural architectures.", "date": "2025-11-09", "url": "https://github.com/GunesCaginAydin/dynamical-metalearning", "stars": 0, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "995953314", "type": "projects", "title": "flexivrobotics/flexiv_sim_plugin", "author": "flexivrobotics", "abstract": "A middleware plugin to connect Flexiv Elements Studio to any external simulator. Supports C++ and Python. Compatible with Linux only.", "date": "2025-10-28", "url": "https://github.com/flexivrobotics/flexiv_sim_plugin", "stars": 4, "language": "C++", "tags": ["C++", "General"], "source": "GitHub"}, {"id": "1054302951", "type": "projects", "title": "AccelerationConsortium/Matterix", "author": "AccelerationConsortium", "abstract": "Digital Twin for Robotics-Assisted Chemistry Lab Automation", "date": "2025-10-22", "url": "https://github.com/AccelerationConsortium/Matterix", "stars": 3, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "169164391", "type": "projects", "title": "facebookresearch/habitat-lab", "author": "facebookresearch", "abstract": "A modular high-level library to train embodied AI agents across a variety of tasks and environments.", "date": "2025-12-04", "url": "https://github.com/facebookresearch/habitat-lab", "stars": 2712, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "169164539", "type": "projects", "title": "facebookresearch/habitat-sim", "author": "facebookresearch", "abstract": "A flexible, high-performance 3D simulator for Embodied AI research.", "date": "2025-12-04", "url": "https://github.com/facebookresearch/habitat-sim", "stars": 3374, "language": "C++", "tags": ["C++", "Vision"], "source": "GitHub"}, {"id": "1011140875", "type": "projects", "title": "Tinker-Twins/Sim2Real-Diffusion", "author": "Tinker-Twins", "abstract": "Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving", "date": "2025-09-24", "url": "https://github.com/Tinker-Twins/Sim2Real-Diffusion", "stars": 3, "language": "Jupyter Notebook", "tags": ["Jupyter Notebook", "Sim2Real"], "source": "GitHub"}, {"id": "983137183", "type": "projects", "title": "taco-group/GenAI4AD", "author": "taco-group", "abstract": "a comprehensive and critical synthesis of the emerging role of GenAI across the full autonomous driving stack", "date": "2025-11-25", "url": "https://github.com/taco-group/GenAI4AD", "stars": 218, "language": "N/A", "tags": [null, "General"], "source": "GitHub"}, {"id": "874177341", "type": "projects", "title": "AndrewDarnall/SynReal", "author": "AndrewDarnall", "abstract": "SynReal: an Attention-Guided GAN architecture for Unpaired Image 2 Image Domain Translation.", "date": "2025-09-12", "url": "https://github.com/AndrewDarnall/SynReal", "stars": 0, "language": "Jupyter Notebook", "tags": ["Jupyter Notebook", "General"], "source": "GitHub"}, {"id": "1048227303", "type": "projects", "title": "CAI23sbP/go2_parkour_deploy", "author": "CAI23sbP", "abstract": "IsaacLab to Mujoco GO2 deploy, IsaacLab to Real world GO2 deploy", "date": "2025-12-04", "url": "https://github.com/CAI23sbP/go2_parkour_deploy", "stars": 35, "language": "Python", "tags": ["Python", "Reinforcement"], "source": "GitHub"}, {"id": "915137783", "type": "projects", "title": "LongchaoDa/AwesomeSim2Real", "author": "LongchaoDa", "abstract": "AwesomeSim2Real - An update-to-date Sim-to-Real repo of \"Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models\"", "date": "2025-12-01", "url": "https://github.com/LongchaoDa/AwesomeSim2Real", "stars": 110, "language": "Python", "tags": ["Python", "Reinforcement", "Sim2Real", "LLM/VLA"], "source": "GitHub"}, {"id": "985268028", "type": "projects", "title": "AmmarAlh/p00ickey-car", "author": "AmmarAlh", "abstract": "This is a weekend project Im doing for fun and to stay up-to-date with ROS 2. The Pickey-Car is basically a car with a 9-degree-of-freedom robot hand strapped to the top. Im building everything for both simulation and real hardware environments.", "date": "2025-09-01", "url": "https://github.com/AmmarAlh/p00ickey-car", "stars": 0, "language": "Python", "tags": ["Python", "Sim2Real"], "source": "GitHub"}, {"id": "950053234", "type": "projects", "title": "zdchan/RobustDexGrasp", "author": "zdchan", "abstract": "This is a repository for RobustDexGrasp, which achieves robust dexterous grasping of 500+ unseen objects with random poses from single-view perception.", "date": "2025-12-01", "url": "https://github.com/zdchan/RobustDexGrasp", "stars": 123, "language": "C++", "tags": ["C++", "Manipulation", "Vision"], "source": "GitHub"}, {"id": "753968001", "type": "projects", "title": "Tinker-Twins/AutoDRIVE-Coopertitive-MARL", "author": "Tinker-Twins", "abstract": "Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles", "date": "2025-10-28", "url": "https://github.com/Tinker-Twins/AutoDRIVE-Coopertitive-MARL", "stars": 24, "language": "Python", "tags": ["Python", "Reinforcement"], "source": "GitHub"}, {"id": "963704828", "type": "projects", "title": "louislelay/kinova_isaaclab_sim2real", "author": "louislelay", "abstract": "Codebase for Kinova Gen3 training in Isaac Lab, sim2real transfer and ROS deployment.", "date": "2025-12-04", "url": "https://github.com/louislelay/kinova_isaaclab_sim2real", "stars": 181, "language": "Python", "tags": ["Python", "Sim2Real"], "source": "GitHub"}, {"id": "1019657359", "type": "projects", "title": "smallfryy/icml2025-robotics-papers", "author": "smallfryy", "abstract": "21 robotics papers from ICML 2025 - TLDRs, virtual posters, and interactive papers on Bytez", "date": "2025-11-15", "url": "https://github.com/smallfryy/icml2025-robotics-papers", "stars": 4, "language": "N/A", "tags": [null, "General"], "source": "GitHub"}, {"id": "699452387", "type": "projects", "title": "luckyrobots/luckyrobots", "author": "luckyrobots", "abstract": "Realtime API for Lucky World simulator with ROS-like interface", "date": "2025-12-01", "url": "https://github.com/luckyrobots/luckyrobots", "stars": 170, "language": "Python", "tags": ["Python", "Reinforcement"], "source": "GitHub"}, {"id": "360542551", "type": "projects", "title": "nadeemlab/CEP", "author": "nadeemlab", "abstract": "Computational Endoscopy Platform (advanced deep learning toolset for analyzing endoscopy videos) [MICCAI'25, MICCAI'22, MICCAI'21, ISBI'21, CVPR'20]", "date": "2025-11-20", "url": "https://github.com/nadeemlab/CEP", "stars": 82, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "580403826", "type": "projects", "title": "bheijden/rex", "author": "bheijden", "abstract": "Rex is a JAX-powered framework for sim-to-real robotics.", "date": "2025-12-01", "url": "https://github.com/bheijden/rex", "stars": 50, "language": "Python", "tags": ["Python", "General"], "source": "GitHub"}, {"id": "260224454", "type": "projects", "title": "ika-rwth-aachen/Cam2BEV", "author": "ika-rwth-aachen", "abstract": "TensorFlow Implementation for Computing a Semantically Segmented Bird's Eye View (BEV) Image Given the Images of Multiple Vehicle-Mounted Cameras.", "date": "2025-12-04", "url": "https://github.com/ika-rwth-aachen/Cam2BEV", "stars": 773, "language": "Python", "tags": ["Python", "Vision"], "source": "GitHub"}];
    window.ALL_TAGS = ["HTML", "C++", "Manipulation", "Vision", "Sim2Real", "Python", "Jupyter Notebook", "Humanoid", "TypeScript", "Go", "C", "General", "Rust", "LLM/VLA", "R", "PHP", "JavaScript", "CSS", "Locomotion", "Reinforcement"];
    window.LAST_UPDATE = "2025-12-04 21:53:05";
    