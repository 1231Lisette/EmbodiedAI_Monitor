
    window.RESEARCH_DATA = [{"id": "2512.05115v1", "type": "papers", "title": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "author": "Tianqi Liu, Zhaoxi Chen, Zihao Huang", "abstract": "Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05115v1", "score": 0, "tags": ["perception"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了联合控制摄像机轨迹和照明的视频生成框架，但在实验部分缺乏详细的数据和结果展示。"}, {"id": "2512.05111v1", "type": "papers", "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning", "author": "Shengyuan Ding, Xinyu Fang, Ziyu Liu", "abstract": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05111v1", "score": 3, "tags": ["perception"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了结合工具使用和视觉推理的奖励模型新方法，增强了多模态生成能力，但缺少具体实验验证其效果。"}, {"id": "2512.05107v1", "type": "papers", "title": "STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models", "author": "Feng Xu, Guangyao Zhai, Xin Kong", "abstract": "Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -> Preference -> Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05107v1", "score": 3, "tags": ["LLM/VLA", "perception", "manipulation"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了阶段感知的强化学习模块，针对长时序动作优化问题，但缺乏具体实验验证其效果。"}, {"id": "2512.05103v1", "type": "papers", "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation", "author": "Xiaochuang Han, Youssef Emad, Melissa Hall", "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05103v1", "score": 3, "tags": ["LLM/VLA", "perception"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了一个新颖的交互式文本和视频生成框架，但缺乏具体的应用场景和实机实验验证。"}, {"id": "2512.05100v1", "type": "papers", "title": "Structured Document Translation via Format Reinforcement Learning", "author": "Haiyue Song, Johannes Eschbach-Dymanus, Hour Kaing", "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05100v1", "score": 0, "tags": [], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了Format Reinforcement Learning方法来处理文档级别的XML或HTML结构翻译，但实验部分仅展示了六项指标的改进，缺乏更全面的评估和对比。"}, {"id": "2512.05098v1", "type": "papers", "title": "SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards", "author": "Yuan Gao, Jin Song", "abstract": "In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05098v1", "score": 0, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了多维度评价框架SA-IQA，填补了现有方法在室内场景评估的空白，但需进一步验证其在实际应用中的效果。"}, {"id": "2512.05094v1", "type": "papers", "title": "From Generated Human Videos to Physically Plausible Robot Trajectories", "author": "James Ni, Zekai Wang, Wei Lin", "abstract": "Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05094v1", "score": 3, "tags": ["humanoid", "sim2real"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了从生成的人类视频到物理上合理的机器人轨迹的两阶段方法，但在实验部分需要进一步验证其在实际机器人控制中的效果。"}, {"id": "2512.05089v1", "type": "papers", "title": "The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception", "author": "Eduardo Di Santi", "abstract": "Real-world physical processes do not generate arbitrary variability: their signals concentrate on compact and low-variability subsets of functional space. This geometric structure enables rapid generalization from a few examples in both biological and artificial systems.   This work develops a deterministic functional-topological framework in which the set of valid realizations of a physical phenomenon forms a compact perceptual manifold with stable invariants and a finite Hausdorff radius. We show that the boundaries of this manifold can be discovered in a fully self-supervised manner through Monte Carlo sampling, even when the governing equations of the system are unknown.   We provide theoretical guarantees, practical estimators of knowledge boundaries, and empirical validations across three domains: electromechanical railway point machines, electrochemical battery discharge curves, and physiological ECG signals.   Our results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction, explaining why biological learners and self-supervised AI models can generalize from limited observations.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05089v1", "score": 3, "tags": [], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了一个新颖的功能拓扑框架，但缺乏具体的应用实例和实验验证。"}, {"id": "2512.05079v1", "type": "papers", "title": "Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints", "author": "Minghan Zhu, Zhiyi Wang, Qihang Sun", "abstract": "Object geometry is key information for robot manipulation. Yet, object reconstruction is a challenging task because cameras only capture partial observations of objects, especially when occlusion occurs. In this paper, we leverage two extra sources of information to reduce the ambiguity of vision signals. First, generative models learn priors of the shapes of commonly seen objects, allowing us to make reasonable guesses of the unseen part of geometry. Second, contact information, which can be obtained from videos and physical interactions, provides sparse constraints on the boundary of the geometry. We combine the two sources of information through contact-guided 3D generation. The guidance formulation is inspired by drag-based editing in generative models. Experiments on synthetic and real-world data show that our approach improves the reconstruction compared to pure 3D generation and contact-based optimization.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05079v1", "score": 3, "tags": ["perception", "manipulation"], "source": "arXiv", "ai_score": 8, "ai_comment": "利用生成模型先验和接触信息有效解决了遮挡下的物体重建问题，但实验部分仅提及合成数据，实际机器人场景验证不足。"}, {"id": "2512.05076v1", "type": "papers", "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation", "author": "Yiming Wang, Qihang Zhang, Shengqu Cai", "abstract": "Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05076v1", "score": 0, "tags": ["LLM/VLA", "perception", "manipulation"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了一个能够独立控制时间和摄像机姿态的4D可控视频扩散框架，增强了视频生成的灵活性，但需要更多实验证据来验证其效果。"}, {"id": "2512.05060v1", "type": "papers", "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer", "author": "Xianfeng Wu, Yajing Bai, Minghan Li", "abstract": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05060v1", "score": 3, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了Transformer在4D语言视觉几何融合中的应用，但缺乏详细的实机实验验证其有效性和泛化能力。"}, {"id": "2512.04969v1", "type": "papers", "title": "Rethinking the Use of Vision Transformers for AI-Generated Image Detection", "author": "NaHyeon Park, Kunhee Kim, Junsuk Choe", "abstract": "Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04969v1", "score": 0, "tags": ["LLM/VLA", "perception"], "source": "arXiv", "ai_score": 8, "ai_comment": "系统分析了Vision Transformer各层特征在AI生成图像检测中的作用，并提出了一种新的动态集成方法，但实验主要集中在数据集层面，缺乏具身智能的具体应用实例。"}, {"id": "2512.04960v1", "type": "papers", "title": "Hybrid-Diffusion Models: Combining Open-loop Routines with Visuomotor Diffusion Policies", "author": "Jonne Van Haastregt, Bastian Orthmann, Michael C. Welle", "abstract": "Despite the fact that visuomotor-based policies obtained via imitation learning demonstrate good performances in complex manipulation tasks, they usually struggle to achieve the same accuracy and speed as traditional control based methods. In this work, we introduce Hybrid-Diffusion models that combine open-loop routines with visuomotor diffusion policies. We develop Teleoperation Augmentation Primitives (TAPs) that allow the operator to perform predefined routines, such as locking specific axes, moving to perching waypoints, or triggering task-specific routines seamlessly during demonstrations. Our Hybrid-Diffusion method learns to trigger such TAPs during inference. We validate the method on challenging real-world tasks: Vial Aspiration, Open-Container Liquid Transfer, and container unscrewing. All experimental videos are available on the project's website: https://hybriddiffusion.github.io/", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04960v1", "score": 6, "tags": ["LLM/VLA", "sim2real", "manipulation"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了结合开放环策略和视觉运动扩散政策的混合扩散模型，但在实际任务验证方面还需加强。"}, {"id": "2512.04958v1", "type": "papers", "title": "Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning", "author": "Roberto Cipollone, Luca Iocchi, Matteo Leonetti", "abstract": "The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04958v1", "score": 3, "tags": [], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了Realizable Abstractions的新概念，具有近最优性保证，但缺乏具体实机实验验证其有效性。"}, {"id": "2512.04952v1", "type": "papers", "title": "FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization", "author": "Yicheng Liu, Shiduo Zhang, Zibin Dong", "abstract": "Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04952v1", "score": 3, "tags": ["LLM/VLA", "perception", "manipulation"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了基于VLA的新架构，通过神经动作分词提高效率，但在实际机器人任务中的表现和细节未充分展示。"}, {"id": "2512.04885v1", "type": "papers", "title": "Stability-Guaranteed Dual Kalman Filtering for Electrochemical Battery State Estimation", "author": "Feng Guo, Guangdi Hu, Keyi Liao", "abstract": "Accurate and stable state estimation is critical for battery management. Although dual Kalman filtering can jointly estimate states and parameters, the strong coupling between filters may cause divergence under large initialization errors or model mismatch. This paper proposes a Stability Guaranteed Dual Kalman Filtering (SG-DKF) method. A Lyapunov-based analysis yields a sufficient stability condition, leading to an adaptive dead-zone rule that suspends parameter updates when the innovation exceeds a stability bound. Applied to an electrochemical battery model, SG-DKF achieves accuracy comparable to a dual EKF and reduces state of charge RMSE by over 45% under large initial state errors.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04885v1", "score": 0, "tags": [], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了稳定性保证的双卡尔曼滤波方法，有效解决了初始化误差和模型不匹配问题，但在实际应用中的具体效果和对比实验需要进一步验证。"}, {"id": "2512.04883v1", "type": "papers", "title": "SDG-Track: A Heterogeneous Observer-Follower Framework for High-Resolution UAV Tracking on Embedded Platforms", "author": "Jiawen Wen, Yu Hu, Suixuan Qiu", "abstract": "Real-time tracking of small unmanned aerial vehicles (UAVs) on edge devices faces a fundamental resolution-speed conflict. Downsampling high-resolution imagery to standard detector input sizes causes small target features to collapse below detectable thresholds. Yet processing native 1080p frames on resource-constrained platforms yields insufficient throughput for smooth gimbal control. We propose SDG-Track, a Sparse Detection-Guided Tracker that adopts an Observer-Follower architecture to reconcile this conflict. The Observer stream runs a high-capacity detector at low frequency on the GPU to provide accurate position anchors from 1920x1080 frames. The Follower stream performs high-frequency trajectory interpolation via ROI-constrained sparse optical flow on the CPU. To handle tracking failures from occlusion or model drift caused by spectrally similar distractors, we introduce Dual-Space Recovery, a training-free re-acquisition mechanism combining color histogram matching with geometric consistency constraints. Experiments on a ground-to-air tracking station demonstrate that SDG-Track achieves 35.1 FPS system throughput while retaining 97.2\\% of the frame-by-frame detection precision. The system successfully tracks agile FPV drones under real-world operational conditions on an NVIDIA Jetson Orin Nano. Our paper code is publicly available at https://github.com/Jeffry-wen/SDG-Track", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04883v1", "score": 3, "tags": [], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了Observer-Follower架构以解决高分辨率UAV跟踪的分辨率-速度冲突，但在实际应用中的效果和对比实验需要进一步验证。"}, {"id": "2512.04797v1", "type": "papers", "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds", "author": " SIMA team, Adrian Bolton, Alexander Lerchner", "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04797v1", "score": 3, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了一个能够理解并执行复杂指令的通用具身代理，但在实际游戏环境中的表现和开放性方面还需进一步验证。"}, {"id": "2512.04793v1", "type": "papers", "title": "YingMusic-SVC: Real-World Robust Zero-Shot Singing Voice Conversion with Flow-GRPO and Singing-Specific Inductive Biases", "author": "Gongyu Chen, Xiaoyu Zhang, Zhenqiang Weng", "abstract": "Singing voice conversion (SVC) aims to render the target singer's timbre while preserving melody and lyrics. However, existing zero-shot SVC systems remain fragile in real songs due to harmony interference, F0 errors, and the lack of inductive biases for singing. We propose YingMusic-SVC, a robust zero-shot framework that unifies continuous pre-training, robust supervised fine-tuning, and Flow-GRPO reinforcement learning. Our model introduces a singing-trained RVC timbre shifter for timbre-content disentanglement, an F0-aware timbre adaptor for dynamic vocal expression, and an energy-balanced rectified flow matching loss to enhance high-frequency fidelity. Experiments on a graded multi-track benchmark show that YingMusic-SVC achieves consistent improvements over strong open-source baselines in timbre similarity, intelligibility, and perceptual naturalness, especially under accompanied and harmony-contaminated conditions, demonstrating its effectiveness for real-world SVC deployment.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04793v1", "score": 0, "tags": [], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了新颖的Flow-GRPO和唱歌特定的诱导偏置，但在实际应用中的实验结果有待进一步验证。"}, {"id": "2512.04779v1", "type": "papers", "title": "YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance", "author": "Junjie Zheng, Chunbo Hao, Guobin Ma", "abstract": "Singing Voice Synthesis (SVS) remains constrained in practical deployment due to its strong dependence on accurate phoneme-level alignment and manually annotated melody contours, requirements that are resource-intensive and hinder scalability. To overcome these limitations, we propose a melody-driven SVS framework capable of synthesizing arbitrary lyrics following any reference melody, without relying on phoneme-level alignment. Our method builds on a Diffusion Transformer (DiT) architecture, enhanced with a dedicated melody extraction module that derives melody representations directly from reference audio. To ensure robust melody encoding, we employ a teacher model to guide the optimization of the melody extractor, alongside an implicit alignment mechanism that enforces similarity distribution constraints for improved melodic stability and coherence. Additionally, we refine duration modeling using weakly annotated song data and introduce a Flow-GRPO reinforcement learning strategy with a multi-objective reward function to jointly enhance pronunciation clarity and melodic fidelity. Experiments show that our model achieves superior performance over existing approaches in both objective measures and subjective listening tests, especially in zero-shot and lyric adaptation settings, while maintaining high audio quality without manual annotation. This work offers a practical and scalable solution for advancing data-efficient singing voice synthesis. To support reproducibility, we release our inference code and model checkpoints.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04779v1", "score": 3, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了无需精确音素级对齐和手动标注旋律轮廓的零样本歌唱语音合成框架，但缺乏具体实验细节和结果展示。"}, {"id": "2512.04753v1", "type": "papers", "title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing", "author": "Ruilin Li, Yibin Wang, Wenhong Zhu", "abstract": "Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04753v1", "score": 3, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了有效的知识编辑方法，但在实际应用中的表现仍有待验证，缺乏具体实验数据支持。"}, {"id": "2512.04733v1", "type": "papers", "title": "E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving", "author": "Yihong Tang, Haicheng Liao, Tong Nie", "abstract": "End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04733v1", "score": 3, "tags": ["LLM/VLA", "perception"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了一个结合情绪感知的VLA框架，但在实际驾驶场景中的效果有待验证。"}, {"id": "2512.04731v1", "type": "papers", "title": "Bridging Simulation and Reality: Cross-Domain Transfer with Semantic 2D Gaussian Splatting", "author": "Jian Tang, Pu Pang, Haowen Sun", "abstract": "Cross-domain transfer in robotic manipulation remains a longstanding challenge due to the significant domain gap between simulated and real-world environments. Existing methods such as domain randomization, adaptation, and sim-real calibration often require extensive tuning or fail to generalize to unseen scenarios. To address this issue, we observe that if domain-invariant features are utilized during policy training in simulation, and the same features can be extracted and provided as the input to policy during real-world deployment, the domain gap can be effectively bridged, leading to significantly improved policy generalization. Accordingly, we propose Semantic 2D Gaussian Splatting (S2GS), a novel representation method that extracts object-centric, domain-invariant spatial features. S2GS constructs multi-view 2D semantic fields and projects them into a unified 3D space via feature-level Gaussian splatting. A semantic filtering mechanism removes irrelevant background content, ensuring clean and consistent inputs for policy learning. To evaluate the effectiveness of S2GS, we adopt Diffusion Policy as the downstream learning algorithm and conduct experiments in the ManiSkill simulation environment, followed by real-world deployment. Results demonstrate that S2GS significantly improves sim-to-real transferability, maintaining high and stable task performance in real-world scenarios.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04731v1", "score": 6, "tags": ["LLM/VLA", "sim2real", "manipulation"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了利用语义2D高斯点积的新方法来桥接模拟与现实之间的领域差距，但缺乏具体实验验证其有效性。"}, {"id": "2512.04699v1", "type": "papers", "title": "OmniScaleSR: Unleashing Scale-Controlled Diffusion Prior for Faithful and Realistic Arbitrary-Scale Image Super-Resolution", "author": "Xinning Chai, Zhengxue Cheng, Yuhong Zhang", "abstract": "Arbitrary-scale super-resolution (ASSR) overcomes the limitation of traditional super-resolution (SR) methods that operate only at fixed scales (e.g., 4x), enabling a single model to handle arbitrary magnification. Most existing ASSR approaches rely on implicit neural representation (INR), but its regression-driven feature extraction and aggregation intrinsically limit the ability to synthesize fine details, leading to low realism. Recent diffusion-based realistic image super-resolution (Real-ISR) models leverage powerful pre-trained diffusion priors and show impressive results at the 4x setting. We observe that they can also achieve ASSR because the diffusion prior implicitly adapts to scale by encouraging high-realism generation. However, without explicit scale control, the diffusion process cannot be properly adjusted for different magnification levels, resulting in excessive hallucination or blurry outputs, especially under ultra-high scales. To address these issues, we propose OmniScaleSR, a diffusion-based realistic arbitrary-scale SR framework designed to achieve both high fidelity and high realism. We introduce explicit, diffusion-native scale control mechanisms that work synergistically with implicit scale adaptation, enabling scale-aware and content-aware modulation of the diffusion process. In addition, we incorporate multi-domain fidelity enhancement designs to further improve reconstruction accuracy. Extensive experiments on bicubic degradation benchmarks and real-world datasets show that OmniScaleSR surpasses state-of-the-art methods in both fidelity and perceptual realism, with particularly strong performance at large magnification factors. Code will be released at https://github.com/chaixinning/OmniScaleSR.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04699v1", "score": 3, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了基于扩散模型的任意尺度超分辨率方法，但缺乏对具体应用场景和实验结果的深入探讨。"}, {"id": "2512.04686v1", "type": "papers", "title": "Towards Cross-View Point Correspondence in Vision-Language Models", "author": "Yipu Wang, Yuheng Ji, Yuyang Liu", "abstract": "Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of \"perceive\", \"reason\", and \"correspond\". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04686v1", "score": 3, "tags": ["LLM/VLA", "perception", "manipulation"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了Cross-View Point Correspondence任务和基准，但缺乏具体模型架构和实机实验，理论性较强。"}, {"id": "2512.04678v1", "type": "papers", "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation", "author": "Yunhong Lu, Yanhong Zeng, Haobo Li", "abstract": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04678v1", "score": 3, "tags": ["LLM/VLA", "perception"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了Reward Forcing框架，通过EMA-Sink改进了视频生成中的滑动窗口注意力机制，但缺乏详细的实机实验验证其效果。"}, {"id": "2512.04653v1", "type": "papers", "title": "Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control", "author": "Pouria Yazdani, Arash Rezaali, Monireh Abdoos", "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04653v1", "score": 0, "tags": ["sim2real"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了Semi-Centralized Training, Decentralized Execution (SEMI-CTDE)架构，解决了MARL在ATSC中的局限性，但缺乏具体实机实验验证其效果。"}, {"id": "2512.04585v1", "type": "papers", "title": "SAM3-I: Segment Anything with Instructions", "author": "Jingjing Li, Yue Feng, Yuchen Guo", "abstract": "Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04585v1", "score": 3, "tags": ["perception"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了通过指令进行概念分割的新方法，但依赖于外部多模态代理进行指令转换，缺乏直接处理复杂指令的能力。"}, {"id": "2512.04571v1", "type": "papers", "title": "Temp-SCONE: A Novel Out-of-Distribution Detection and Domain Generalization Framework for Wild Data with Temporal Shift", "author": "Aditi Naiknaware, Sanchit Singh, Hajar Homayouni", "abstract": "Open-world learning (OWL) requires models that can adapt to evolving environments while reliably detecting out-of-distribution (OOD) inputs. Existing approaches, such as SCONE, achieve robustness to covariate and semantic shifts but assume static environments, leading to degraded performance in dynamic domains. In this paper, we propose Temp-SCONE, a temporally consistent extension of SCONE designed to handle temporal shifts in dynamic environments. Temp-SCONE introduces a confidence-driven regularization loss based on Average Thresholded Confidence (ATC), penalizing instability in predictions across time steps while preserving SCONE's energy-margin separation. Experiments on dynamic datasets demonstrate that Temp-SCONE significantly improves robustness under temporal drift, yielding higher corrupted-data accuracy and more reliable OOD detection compared to SCONE. On distinct datasets without temporal continuity, Temp-SCONE maintains comparable performance, highlighting the importance and limitations of temporal regularization. Our theoretical insights on temporal stability and generalization error further establish Temp-SCONE as a step toward reliable OWL in evolving dynamic environments.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04571v1", "score": 0, "tags": [], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了针对动态环境的Temporal SCONE框架，通过引入ATC正则化损失有效处理时间漂移问题，但需进一步验证其在更多实际场景中的表现。"}, {"id": "2512.04537v1", "type": "papers", "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale", "author": "Pei Yang, Hai Ci, Yiren Song", "abstract": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04537v1", "score": 3, "tags": ["LLM/VLA", "humanoid", "perception"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了针对复杂场景的视频生成方法，但缺乏具体实验结果和对比分析"}, {"id": "2512.04528v1", "type": "papers", "title": "Auto3R: Automated 3D Reconstruction and Scanning via Data-driven Uncertainty Quantification", "author": "Chentao Shen, Sizhe Zheng, Bingqian Wu", "abstract": "Traditional high-quality 3D scanning and reconstruction typically relies on human labor to plan the scanning procedure. With the rapid development of embodied systems such as drones and robots, there is a growing demand of performing accurate 3D scanning and reconstruction in an fully automated manner. We introduce Auto3R, a data-driven uncertainty quantification model that is designed to automate the 3D scanning and reconstruction of scenes and objects, including objects with non-lambertian and specular materials. Specifically, in a process of iterative 3D reconstruction and scanning, Auto3R can make efficient and accurate prediction of uncertainty distribution over potential scanning viewpoints, without knowing the ground truth geometry and appearance. Through extensive experiments, Auto3R achieves superior performance that outperforms the state-of-the-art methods by a large margin. We also deploy Auto3R on a robot arm equipped with a camera and demonstrate that Auto3R can be used to effectively digitize real-world 3D objects and delivers ready-to-use and photorealistic digital assets. Our homepage: https://tomatoma00.github.io/auto3r.github.io .", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04528v1", "score": 0, "tags": ["perception"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了数据驱动的不确定性量化模型，能够自动进行3D重建和扫描，但在非朗伯体和镜面材料方面缺乏具体实验验证。"}, {"id": "2512.04515v1", "type": "papers", "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion", "author": "Liuzhou Zhang, Jiarui Ye, Yuanlei Wang", "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04515v1", "score": 3, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 8, "ai_comment": "提出了结合长时稀疏KV缓存和局部适应的创新框架，但在实验部分未详细展示具体效果。"}, {"id": "169164391", "type": "projects", "title": "facebookresearch/habitat-lab", "author": "facebookresearch", "abstract": "A modular high-level library to train embodied AI agents across a variety of tasks and environments.", "date": "2025-12-04", "url": "https://github.com/facebookresearch/habitat-lab", "stars": 2713, "score": 2, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/169164391", "ai_score": 8, "ai_comment": "提供了多功能的高层库，适用于多种任务和环境，但缺乏具体实机实验结果"}, {"id": "169164539", "type": "projects", "title": "facebookresearch/habitat-sim", "author": "facebookresearch", "abstract": "A flexible, high-performance 3D simulator for Embodied AI research.", "date": "2025-12-04", "url": "https://github.com/facebookresearch/habitat-sim", "stars": 3374, "score": 2, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/169164539", "ai_score": 8, "ai_comment": "提供了高性能的3D模拟环境，但缺乏具体的应用案例和详细实验结果"}, {"id": "999275152", "type": "projects", "title": "mujocolab/mjlab", "author": "mujocolab", "abstract": "Isaac Lab API, powered by MuJoCo-Warp, for RL and robotics research.", "date": "2025-12-05", "url": "https://github.com/mujocolab/mjlab", "stars": 1111, "score": 2, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/999275152", "ai_score": 8, "ai_comment": "提供了MuJoCo-Warp驱动的Isaac Lab API，有助于RL和机器人研究，但缺乏具体的应用案例和实验结果展示。"}, {"id": "2512.05112v1", "type": "papers", "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation", "author": "Dongzhi Jiang, Renrui Zhang, Haodong Li", "abstract": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05112v1", "score": 3, "tags": ["LLM/VLA", "manipulation"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了结合草图和CoT的新颖推理框架，但在实验部分缺乏详细展示和对比分析。"}, {"id": "2512.05105v1", "type": "papers", "title": "Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning", "author": "Purbesh Mitra, Sennur Ulukus", "abstract": "Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \\textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05105v1", "score": 0, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了一个新颖的自蒸馏技术SSB，但缺乏具体实验验证其效果，理论性较强。"}, {"id": "2512.05066v1", "type": "papers", "title": "Multi-LLM Collaboration for Medication Recommendation", "author": "Huascar Sanchez, Briland Hitaj, Jules Bergmann", "abstract": "As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05066v1", "score": 3, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了多LLM协作的新框架，但在实际临床应用中的效果和稳定性需要进一步验证。"}, {"id": "2512.05049v1", "type": "papers", "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory", "author": "Yu-Chao Hsu, Jiun-Cheng Jiang, Chun-Hua Lin", "abstract": "Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05049v1", "score": 3, "tags": [], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了量子启发式的Kolmogorov-Arnold LSTM模型，但缺乏具体实验验证其有效性。"}, {"id": "2512.04988v1", "type": "papers", "title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets", "author": "Christopher Chiu, Simpson Zhang, Mihaela van der Schaar", "abstract": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04988v1", "score": 3, "tags": ["LLM/VLA", "sim2real"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了一个新颖的框架来理解AI代理在劳动力市场中的战略行为，但缺乏具体实验验证其有效性。"}, {"id": "2512.04987v1", "type": "papers", "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction", "author": "Nex-AGI Team,  :, Yuxuan Cai", "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04987v1", "score": 3, "tags": ["LLM/VLA", "sim2real"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了一个统一的生态系统来构建大规模交互环境，但缺乏具体实验验证其有效性和实用性。"}, {"id": "2512.04973v1", "type": "papers", "title": "Preliminary Analysis and Simulation of a Compact Variable Stiffness Wrist", "author": "Giuseppe Milazzo, Manuel G. Catalano, Antonio Bicchi", "abstract": "Variable Stiffness Actuators prove invaluable for robotics applications in unstructured environments, fostering safe interactions and enhancing task adaptability. Nevertheless, their mechanical design inevitably results in larger and heavier structures compared to classical rigid actuators. This paper introduces a novel 3 Degrees of Freedom (DoFs) parallel wrist that achieves variable stiffness through redundant elastic actuation. Leveraging its parallel architecture, the device employs only four motors, rendering it compact and lightweight. This characteristic makes it particularly well-suited for applications in prosthetics or humanoid robotics. The manuscript delves into the theoretical model of the device and proposes a sophisticated control strategy for independent regulation of joint position and stiffness. Furthermore, it validates the proposed controller through simulation, utilizing a comprehensive analysis of the system dynamics. The reported results affirm the ability of the device to achieve high accuracy and disturbance rejection in rigid configurations while minimizing interaction forces with its compliant behavior.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04973v1", "score": 3, "tags": ["humanoid", "sim2real"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了通过冗余弹性执行器实现可变刚度的新架构，但缺乏实机实验验证"}, {"id": "2512.04949v1", "type": "papers", "title": "CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent", "author": "Leyang Shen, Yang Zhang, Chun Kai Ling", "abstract": "Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for multi-step agents. CARL achieves focused training through providing action-level optimization signals for high-criticality actions while excluding low-criticality actions from model update. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency during training and inference across diverse evaluation settings.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04949v1", "score": 3, "tags": [], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了针对多步代理的基于关键动作的强化学习算法，但缺少详细的实验验证和对比分析。"}, {"id": "2512.04945v1", "type": "papers", "title": "TripleC Learning and Lightweight Speech Enhancement for Multi-Condition Target Speech Extraction", "author": "Ziling Huang", "abstract": "In our recent work, we proposed Lightweight Speech Enhancement Guided Target Speech Extraction (LGTSE) and demonstrated its effectiveness in multi-speaker-plus-noise scenarios. However, real-world applications often involve more diverse and complex conditions, such as one-speaker-plus-noise or two-speaker-without-noise. To address this challenge, we extend LGTSE with a Cross-Condition Consistency learning strategy, termed TripleC Learning. This strategy is first validated under multi-speaker-plus-noise condition and then evaluated for its generalization across diverse scenarios. Moreover, building upon the lightweight front-end denoiser in LGTSE, which can flexibly process both noisy and clean mixtures and shows strong generalization to unseen conditions, we integrate TripleC learning with a proposed parallel universal training scheme that organizes batches containing multiple scenarios for the same target speaker. By enforcing consistent extraction across different conditions, easier cases can assist harder ones, thereby fully exploiting diverse training data and fostering a robust universal model. Experimental results on the Libri2Mix three-condition tasks demonstrate that the proposed LGTSE with TripleC learning achieves superior performance over condition-specific models, highlighting its strong potential for universal deployment in real-world speech applications.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04945v1", "score": 3, "tags": [], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了TripleC学习策略以增强在不同条件下的语音提取效果，但缺乏具体实验验证其在实际应用场景中的表现。"}, {"id": "2512.04837v1", "type": "papers", "title": "A Sanity Check for Multi-In-Domain Face Forgery Detection in the Real World", "author": "Jikang Cheng, Renye Yan, Zhiyuan Yan", "abstract": "Existing methods for deepfake detection aim to develop generalizable detectors. Although \"generalizable\" is the ultimate target once and for all, with limited training forgeries and domains, it appears idealistic to expect generalization that covers entirely unseen variations, especially given the diversity of real-world deepfakes. Therefore, introducing large-scale multi-domain data for training can be feasible and important for real-world applications. However, within such a multi-domain scenario, the differences between multiple domains, rather than the subtle real/fake distinctions, dominate the feature space. As a result, despite detectors being able to relatively separate real and fake within each domain (i.e., high AUC), they struggle with single-image real/fake judgments in domain-unspecified conditions (i.e., low ACC). In this paper, we first define a new research paradigm named Multi-In-Domain Face Forgery Detection (MID-FFD), which includes sufficient volumes of real-fake domains for training. Then, the detector should provide definitive real-fake judgments to the domain-unspecified inputs, which simulate the frame-by-frame independent detection scenario in the real world. Meanwhile, to address the domain-dominant issue, we propose a model-agnostic framework termed DevDet (Developer for Detector) to amplify real/fake differences and make them dominant in the feature space. DevDet consists of a Face Forgery Developer (FFDev) and a Dose-Adaptive detector Fine-Tuning strategy (DAFT). Experiments demonstrate our superiority in predicting real-fake under the MID-FFD scenario while maintaining original generalization ability to unseen data.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04837v1", "score": 0, "tags": [], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了在多领域背景下对真实世界深度伪造检测的新视角，但缺乏具体实验验证其有效性。"}, {"id": "2512.04813v1", "type": "papers", "title": "MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation", "author": "Huanqian Wang, Chi Bene Chen, Yang Yue", "abstract": "Imitation learning method has shown immense promise for robotic manipulation, yet its practical deployment is fundamentally constrained by the data scarcity. Despite prior work on collecting large-scale datasets, there still remains a significant gap to robust spatial generalization. We identify a key limitation: individual trajectories, regardless of their length, are typically collected from a \\emph{single, static spatial configuration} of the environment. This includes fixed object and target spatial positions as well as unchanging camera viewpoints, which significantly restricts the diversity of spatial information available for learning. To address this critical bottleneck in data efficiency, we propose \\textbf{MOtion-Based Variability Enhancement} (\\emph{MOVE}), a simple yet effective data collection paradigm that enables the acquisition of richer spatial information from dynamic demonstrations. Our core contribution is an augmentation strategy that injects motion into any movable objects within the environment for each demonstration. This process implicitly generates a dense and diverse set of spatial configurations within a single trajectory. We conduct extensive experiments in both simulation and real-world environments to validate our approach. For example, in simulation tasks requiring strong spatial generalization, \\emph{MOVE} achieves an average success rate of 39.1\\%, a 76.1\\% relative improvement over the static data collection paradigm (22.2\\%), and yields up to 2--5$\\times$ gains in data efficiency on certain tasks. Our code is available at https://github.com/lucywang720/MOVE.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04813v1", "score": 3, "tags": ["sim2real", "perception", "manipulation"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了一个简单有效的数据收集方法以增强空间泛化能力，但缺乏具体实验验证其效果。"}, {"id": "2512.04785v1", "type": "papers", "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications", "author": "Eranga Bandara, Amin Hass, Ross Gore", "abstract": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04785v1", "score": 3, "tags": ["LLM/VLA", "perception", "manipulation"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了针对AI代理系统的新型威胁建模框架，但缺乏具体案例和实验验证其有效性。"}, {"id": "2512.04773v1", "type": "papers", "title": "Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions", "author": "Giorgos Polychronis, Foivos Pournaropoulos, Christos D. Antonopoulos", "abstract": "Drones are becoming indispensable in many application domains. In data-driven missions, besides sensing, the drone must process the collected data at runtime to decide whether additional action must be taken on the spot, before moving to the next point of interest. If processing does not reveal an event or situation that requires such an action, the drone has waited in vain instead of moving to the next point. If, however, the drone starts moving to the next point and it turns out that a follow-up action is needed at the previous point, it must spend time to fly-back. To take this decision, we propose different machine-learning methods based on branch prediction and reinforcement learning. We evaluate these methods for a wide range of scenarios where the probability of event occurrence changes with time. Our results show that the proposed methods consistently outperform the regression-based method proposed in the literature and can significantly improve the worst-case mission time by up to 4.1x. Also, the achieved median mission time is very close, merely up to 2.7% higher, to that of a method with perfect knowledge of the current underlying event probability at each point of interest.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04773v1", "score": 3, "tags": [], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了基于分支预测和强化学习的决策方法，但在广泛场景下的评估结果未完全展示。"}, {"id": "2512.04617v1", "type": "papers", "title": "Score Matching for Estimating Finite Point Processes", "author": "Haoqun Cao, Yixuan Zhang, Feng Zhou", "abstract": "Score matching estimators have garnered significant attention in recent years because they eliminate the need to compute normalizing constants, thereby mitigating the computational challenges associated with maximum likelihood estimation (MLE).While several studies have proposed score matching estimators for point processes, this work highlights the limitations of these existing methods, which stem primarily from the lack of a mathematically rigorous analysis of how score matching behaves on finite point processes -- special random configurations on bounded spaces where many of the usual assumptions and properties of score matching no longer hold. To this end, we develop a formal framework for score matching on finite point processes via Janossy measures and, within this framework, introduce an (autoregressive) weighted score-matching estimator, whose statistical properties we analyze in classical parametric settings. For general nonparametric (e.g., deep) point process models, we show that score matching alone does not uniquely identify the ground-truth distribution due to subtle normalization issues, and we propose a simple survival-classification augmentation that yields a complete, integration-free training objective for any intensity-based point process model for spatio-temporal case. Experiments on synthetic and real-world temporal and spatio-temporal datasets, demonstrate that our method accurately recovers intensities and achieves performance comparable to MLE with better efficiency.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04617v1", "score": 0, "tags": [], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了针对有限点过程的正式框架，但缺乏具体实验验证其有效性。"}, {"id": "2512.04596v1", "type": "papers", "title": "QoSDiff: An Implicit Topological Embedding Learning Framework Leveraging Denoising Diffusion and Adversarial Attention for Robust QoS Prediction", "author": "Guanchen Du, Jianlong Xu, Wei Wei", "abstract": "Accurate Quality of Service (QoS) prediction is fundamental to service computing, providing essential data-driven guidance for service selection and ensuring superior user experiences. However, prevalent approaches, particularly Graph Neural Networks (GNNs), heavily rely on constructing explicit user--service interaction graphs. This dependency introduces severe scalability bottlenecks and limits performance when explicit connections are sparse or corrupted by noise. To address these challenges, this paper introduces \\emph{QoSDiff}, a novel embedding learning framework that bypasses the prerequisite of explicit graph construction. Specifically, it leverages a denoising diffusion probabilistic model to recover intrinsic latent structures from noisy initializations. To further capture high-order interactions, we propose an adversarial interaction module that integrates a bidirectional hybrid attention mechanism. This adversarial paradigm dynamically distinguishes informative patterns from noise, enabling a dual-perspective modeling of intricate user--service associations. Extensive experiments on two large-scale real-world datasets demonstrate that QoSDiff significantly outperforms state-of-the-art baselines. Notably, the results highlight the framework's superior cross-dataset generalization capability and exceptional robustness against data sparsity and observational noise.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04596v1", "score": 3, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了利用去噪扩散和对抗注意力的新框架，但未详细说明其实验结果和对比分析。"}, {"id": "2512.04580v1", "type": "papers", "title": "A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution", "author": "Huifeng Zhu, Shijie Li, Qinfeng Li", "abstract": "To enhance the performance of large language models (LLMs) in various domain-specific applications, sensitive data such as healthcare, law, and finance are being used to privately customize or fine-tune these models. Such privately adapted LLMs are regarded as either personal privacy assets or corporate intellectual property. Therefore, protecting model weights and maintaining strict confidentiality during deployment and distribution have become critically important. However, existing model formats and deployment frameworks provide little to no built-in support for confidentiality, access control, or secure integration with trusted hardware. Current methods for securing model deployment either rely on computationally expensive cryptographic techniques or tightly controlled private infrastructure. Although these approaches can be effective in specific scenarios, they are difficult and costly for widespread deployment.   In this paper, we introduce CryptoTensors, a secure and format-compatible file structure for confidential LLM distribution. Built as an extension to the widely adopted Safetensors format, CryptoTensors incorporates tensor-level encryption and embedded access control policies, while preserving critical features such as lazy loading and partial deserialization. It enables transparent decryption and automated key management, supporting flexible licensing and secure model execution with minimal overhead. We implement a proof-of-concept library, benchmark its performance across serialization and runtime scenarios, and validate its compatibility with existing inference frameworks, including Hugging Face Transformers and vLLM. Our results highlight CryptoTensors as a light-weight, efficient, and developer-friendly solution for safeguarding LLM weights in real-world and widespread deployments.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04580v1", "score": 0, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了用于保护大型语言模型权重的安全文件格式，但在具体实现细节和实验验证方面还有待加强。"}, {"id": "2512.04579v1", "type": "papers", "title": "Gauss-Newton accelerated MPPI Control", "author": "Hannes Homburger, Katrin Baumgärtner, Moritz Diehl", "abstract": "Model Predictive Path Integral (MPPI) control is a sampling-based optimization method that has recently attracted attention, particularly in the robotics and reinforcement learning communities. MPPI has been widely applied as a GPU-accelerated random search method to deterministic direct single-shooting optimal control problems arising in model predictive control (MPC) formulations. MPPI offers several key advantages, including flexibility, robustness, ease of implementation, and inherent parallelizability. However, its performance can deteriorate in high-dimensional settings since the optimal control problem is solved via Monte Carlo sampling. To address this limitation, this paper proposes an enhanced MPPI method that incorporates a Jacobian reconstruction technique and the second-order Generalized Gauss-Newton method. This novel approach is called \\textit{Gauss-Newton accelerated MPPI}. The numerical results show that the Gauss-Newton accelerated MPPI approach substantially improves MPPI scalability and computational efficiency while preserving the key benefits of the classical MPPI framework, making it a promising approach even for high-dimensional problems.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04579v1", "score": 3, "tags": [], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了Gauss-Newton加速的MPPI控制方法，但在高维设置下的性能改进尚需更多实验验证。"}, {"id": "2512.04563v1", "type": "papers", "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence", "author": "Zefeng Zhang, Xiangzhao Hao, Hengzhu Tang", "abstract": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \\textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \\textbf{6.91\\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \\textbf{7.92\\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04563v1", "score": 0, "tags": ["LLM/VLA", "perception"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了统一模型COOPER以增强空间感知和推理，但缺乏具体实机实验验证其效果。"}, {"id": "2512.04552v1", "type": "papers", "title": "RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS", "author": "Cong Wang, Changfeng Gao, Yang Xiang", "abstract": "Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: https://lrwinr.github.io/RRPO-CosyVoice.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04552v1", "score": 3, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了针对LLM情感TTS的鲁棒奖励优化框架，但缺乏具体实验验证其效果。"}, {"id": "2512.04535v1", "type": "papers", "title": "GTM: Simulating the World of Tools for AI Agents", "author": "Zhenzhen Ren, Xinpeng Zhang, Zhenxing Qian", "abstract": "The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04535v1", "score": 3, "tags": ["LLM/VLA", "sim2real"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了通用工具模拟器GTM，通过提示配置实现快速、低成本的工具交互模拟，但缺乏具体实验验证其在实际任务中的效果。"}, {"id": "2512.04530v1", "type": "papers", "title": "Explainable Graph Representation Learning via Graph Pattern Analysis", "author": "Xudong Wang, Ziheng Sun, Chris Ding", "abstract": "Explainable artificial intelligence (XAI) is an important area in the AI community, and interpretability is crucial for building robust and trustworthy AI models. While previous work has explored model-level and instance-level explainable graph learning, there has been limited investigation into explainable graph representation learning. In this paper, we focus on representation-level explainable graph learning and ask a fundamental question: What specific information about a graph is captured in graph representations? Our approach is inspired by graph kernels, which evaluate graph similarities by counting substructures within specific graph patterns. Although the pattern counting vector can serve as an explainable representation, it has limitations such as ignoring node features and being high-dimensional. To address these limitations, we introduce a framework (PXGL-GNN) for learning and explaining graph representations through graph pattern analysis. We start by sampling graph substructures of various patterns. Then, we learn the representations of these patterns and combine them using a weighted sum, where the weights indicate the importance of each graph pattern's contribution. We also provide theoretical analyses of our methods, including robustness and generalization. In our experiments, we show how to learn and explain graph representations for real-world data using pattern analysis. Additionally, we compare our method against multiple baselines in both supervised and unsupervised learning tasks to demonstrate its effectiveness.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04530v1", "score": 0, "tags": [], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了基于图模式分析的可解释图表示学习方法，但缺乏具体实验验证其有效性。"}, {"id": "2512.04513v1", "type": "papers", "title": "BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models", "author": "Yu-Wei Zhan, Xin Wang, Pengzhe Mao", "abstract": "Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04513v1", "score": 3, "tags": ["LLM/VLA"], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了一个任务感知的模块化框架来解决多模态大语言模型和世界模型之间的双向耦合问题，但缺乏详细的实验验证和具体的应用案例。"}, {"id": "2512.04480v1", "type": "papers", "title": "AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Substituitions", "author": "Pedro Passos", "abstract": "In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the \"FAGNER Paradox\" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the \"Lukaku Paradox\", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04480v1", "score": 3, "tags": [], "source": "arXiv", "ai_score": 7, "ai_comment": "提出了基于模糊逻辑的决策支持系统，但缺乏具体实验验证其在实际比赛中的效果。"}, {"id": "2512.04463v1", "type": "papers", "title": "MARL Warehouse Robots", "author": "Price Allman, Lian Thang, Dre Simmons", "abstract": "We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04463v1", "score": 0, "tags": ["LLM/VLA", "sim2real"], "source": "arXiv", "ai_score": 7, "ai_comment": "展示了QMIX在多智能体强化学习中的优越性能，但实验规模较小，缺乏大规模部署的验证。"}, {"id": "479289739", "type": "projects", "title": "rerun-io/rerun", "author": "rerun-io", "abstract": "An open source SDK for logging, storing, querying, and visualizing multimodal and multi-rate data", "date": "2025-12-05", "url": "https://github.com/rerun-io/rerun", "stars": 9691, "score": 5, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/479289739", "ai_score": 7, "ai_comment": "提供了多模态和多速率数据的日志记录、存储、查询和可视化功能，但在具体应用场景和实验验证方面有待加强。"}, {"id": "74627617", "type": "projects", "title": "commaai/openpilot", "author": "commaai", "abstract": "openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.", "date": "2025-12-05", "url": "https://github.com/commaai/openpilot", "stars": 59132, "score": 5, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/74627617", "ai_score": 7, "ai_comment": "在多个车型上实现了驾驶辅助系统的升级，具有一定的实用价值，但缺乏更多技术细节和实机实验验证"}, {"id": "365250183", "type": "projects", "title": "zauberzeug/nicegui", "author": "zauberzeug", "abstract": "Create web-based user interfaces with Python. The nice way.", "date": "2025-12-05", "url": "https://github.com/zauberzeug/nicegui", "stars": 14805, "score": 5, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/365250183", "ai_score": 7, "ai_comment": "提供了一种使用Python创建基于Web的用户界面的方法，但缺乏具身智能或机器人相关的实际应用案例。"}, {"id": "1075128236", "type": "projects", "title": "wadeKeith/DeepThinkVLA", "author": "wadeKeith", "abstract": "DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models", "date": "2025-12-05", "url": "https://github.com/wadeKeith/DeepThinkVLA", "stars": 323, "score": 3, "tags": ["LLM/VLA", "perception"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1075128236", "ai_score": 7, "ai_comment": "提出了增强视觉-语言-行动模型推理能力的新方法，但缺少具体实验验证和实机测试结果。"}, {"id": "889452427", "type": "projects", "title": "QuestNav/QuestNav", "author": "QuestNav", "abstract": "A project that enables streaming Meta Quest headset pose to an FRC robot over Network Tables. Built in Unity. ", "date": "2025-12-05", "url": "https://github.com/QuestNav/QuestNav", "stars": 113, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/889452427", "ai_score": 7, "ai_comment": "实现了Meta Quest头显姿态流传输到FRC机器人，但在Unity中的实现细节和优化空间有待进一步阐述。"}, {"id": "102971729", "type": "projects", "title": "ros-controls/ros2_control", "author": "ros-controls", "abstract": "Generic and simple controls framework for ROS 2", "date": "2025-12-05", "url": "https://github.com/ros-controls/ros2_control", "stars": 767, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/102971729", "ai_score": 7, "ai_comment": "提供了ROS 2的通用控制框架，结构简洁，但缺乏具体应用案例和实验验证"}, {"id": "914053359", "type": "projects", "title": "OpenMind/OM1", "author": "OpenMind", "abstract": "Modular AI runtime for robots", "date": "2025-12-05", "url": "https://github.com/OpenMind/OM1", "stars": 2198, "score": 2, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/914053359", "ai_score": 7, "ai_comment": "提出了模块化AI运行时框架，但缺乏具体实验和案例研究支持"}, {"id": "486234106", "type": "projects", "title": "verivital/behaverify", "author": "verivital", "abstract": "BehaVerify: A Formal Verification Tool for Behavior Trees", "date": "2025-12-05", "url": "https://github.com/verivital/behaverify", "stars": 19, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/486234106", "ai_score": 7, "ai_comment": "提供了一种形式化验证行为树的方法，但在实际应用案例和详细实验结果方面还有提升空间。"}, {"id": "882091187", "type": "projects", "title": "AD-SDL/MADSci", "author": "AD-SDL", "abstract": "Main repository for the Modular Autonomous Discovery for Science (MADSci) Framework", "date": "2025-12-05", "url": "https://github.com/AD-SDL/MADSci", "stars": 30, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/882091187", "ai_score": 7, "ai_comment": "提出了模块化自主科学发现框架，但缺乏具体实机实验和数据支持"}, {"id": "885743839", "type": "projects", "title": "leofan90/Awesome-World-Models", "author": "leofan90", "abstract": "A comprehensive list of papers for the definition of World Models and using World Models for General Video Generation, Embodied AI, and Autonomous Driving, including papers, codes, and related websites.", "date": "2025-12-05", "url": "https://github.com/leofan90/Awesome-World-Models", "stars": 860, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/885743839", "ai_score": 7, "ai_comment": "提供了世界模型领域的综述，包括相关论文、代码和网站链接，但缺乏原创性或突破性成果。"}, {"id": "1087207313", "type": "projects", "title": "knightnemo/Awesome-World-Models", "author": "knightnemo", "abstract": "A Curated List of Awesome Works in World Modeling, Aiming to Serve as a One-stop Resource for Researchers, Practitioners, and Enthusiasts Interested in World Modeling.", "date": "2025-12-05", "url": "https://github.com/knightnemo/Awesome-World-Models", "stars": 1321, "score": 5, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1087207313", "ai_score": 7, "ai_comment": "提供了世界建模领域的综述，但缺乏具体的技术创新或实验证据支持"}, {"id": "1082733610", "type": "projects", "title": "DexForce/EmbodiChain", "author": "DexForce", "abstract": "An end-to-end, GPU-accelerated, and modular platform for building generalized Embodied Intelligence.", "date": "2025-12-04", "url": "https://github.com/DexForce/EmbodiChain", "stars": 21, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1082733610", "ai_score": 7, "ai_comment": "提出了一个模块化且GPU加速的平台，但缺乏具体的实机实验和案例研究来验证其有效性。"}, {"id": "460615268", "type": "projects", "title": "dora-rs/dora", "author": "dora-rs", "abstract": "DORA (Dataflow-Oriented Robotic Architecture) is middleware designed to streamline and simplify the creation of AI-based robotic applications. It offers low latency, composable, and distributed dataflow capabilities. Applications are modeled as directed graphs, also referred to as pipelines.", "date": "2025-12-05", "url": "https://github.com/dora-rs/dora", "stars": 2719, "score": 2, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/460615268", "ai_score": 7, "ai_comment": "提供了低延迟、可组合和分布式的数据流能力，但缺乏具体的实机实验和案例研究，难以评估其实际效果。"}, {"id": "810385004", "type": "projects", "title": "RobotecAI/rai", "author": "RobotecAI", "abstract": "RAI is a vendor-agnostic agentic framework for robotics, utilizing ROS 2 tools to perform complex actions, defined scenarios, free interface execution, log summaries, voice interaction and more.", "date": "2025-12-03", "url": "https://github.com/RobotecAI/rai", "stars": 420, "score": 3, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/810385004", "ai_score": 7, "ai_comment": "利用ROS 2工具实现复杂的机器人任务，功能全面，但缺乏具体案例和实机实验验证"}, {"id": "491699829", "type": "projects", "title": "google/pyglove", "author": "google", "abstract": "Manipulating Python Programs", "date": "2025-12-04", "url": "https://github.com/google/pyglove", "stars": 700, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/491699829", "ai_score": 7, "ai_comment": "提供了强大的编程工具，但在具身智能领域的应用尚不明确。"}, {"id": "52715040", "type": "projects", "title": "shenwei356/seqkit", "author": "shenwei356", "abstract": "A cross-platform and ultrafast toolkit for FASTA/Q file manipulation", "date": "2025-12-04", "url": "https://github.com/shenwei356/seqkit", "stars": 1488, "score": 2, "tags": ["manipulation"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/52715040", "ai_score": 7, "ai_comment": "提供了跨平台的超快速工具包用于处理FASTA/Q文件，但在具身智能领域的应用尚不明确。"}, {"id": "1063851187", "type": "projects", "title": "2toinf/X-VLA", "author": "2toinf", "abstract": "The offical Implementation of \"Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model\"", "date": "2025-12-05", "url": "https://github.com/2toinf/X-VLA", "stars": 302, "score": 3, "tags": ["LLM/VLA", "perception"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1063851187", "ai_score": 7, "ai_comment": "提出了软提示变换器作为可扩展的跨体态VLA模型的新架构，但缺少具体实机实验结果"}, {"id": "988069032", "type": "projects", "title": "philfung/awesome-reliable-robotics", "author": "philfung", "abstract": "Robotics research demonstrating reliability and robustness in the real world (continuously updated)", "date": "2025-12-05", "url": "https://github.com/philfung/awesome-reliable-robotics", "stars": 114, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/988069032", "ai_score": 7, "ai_comment": "收集了大量关于可靠性和鲁棒性的机器人研究，但缺乏原创性贡献或实机实验展示"}, {"id": "794915217", "type": "projects", "title": "CroboticSolutions/arm_api2", "author": "CroboticSolutions", "abstract": "Code wrapper for MoveIt2! and ROS 2.", "date": "2025-12-02", "url": "https://github.com/CroboticSolutions/arm_api2", "stars": 84, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/794915217", "ai_score": 7, "ai_comment": "提供了MoveIt2!和ROS 2的代码封装，但缺乏具体应用案例和实验验证"}, {"id": "970597960", "type": "projects", "title": "InternRobotics/GenManip", "author": "InternRobotics", "abstract": "[CVPR 2025] Official implementation of \"GenManip: LLM-driven Simulation for Generalizable Instruction-Following Manipulation\"", "date": "2025-12-04", "url": "https://github.com/InternRobotics/GenManip", "stars": 115, "score": 0, "tags": ["LLM/VLA", "sim2real", "manipulation"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/970597960", "ai_score": 7, "ai_comment": "提出了LLM驱动的模拟方法，但在实际应用中的验证和实验部分有待加强。"}, {"id": "992630802", "type": "projects", "title": "ReinFlow/ReinFlow", "author": "ReinFlow", "abstract": "[NeurIPS 2025] Flow x RL. \"ReinFlow: Fine-tuning Flow Policy with Online Reinforcement Learning\". Support VLAs e.g., pi0, pi0.5. Fully open-sourced. ", "date": "2025-12-03", "url": "https://github.com/ReinFlow/ReinFlow", "stars": 185, "score": 0, "tags": ["LLM/VLA"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/992630802", "ai_score": 7, "ai_comment": "提出了Flow与RL结合的新方法，但缺乏具体实验验证和实机应用展示。"}, {"id": "1019503597", "type": "projects", "title": "ROBOTIS-GIT/robotis_lab", "author": "ROBOTIS-GIT", "abstract": "This repository provides tutorials for reinforcement learning and imitation learning using ROBOTIS robots, and supports Sim2Real functionality for deploying the learned policies on real robots.", "date": "2025-12-04", "url": "https://github.com/ROBOTIS-GIT/robotis_lab", "stars": 75, "score": 3, "tags": ["sim2real"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1019503597", "ai_score": 7, "ai_comment": "提供了使用ROBOTIS机器人进行强化学习和模仿学习的教程，支持Sim2Real功能，但缺乏创新架构或显著降低成本的内容。"}, {"id": "1073406342", "type": "projects", "title": "mujocolab/g1_spinkick_example", "author": "mujocolab", "abstract": "Train a Unitree G1 humanoid to perform a double spin kick using mjlab", "date": "2025-12-05", "url": "https://github.com/mujocolab/g1_spinkick_example", "stars": 174, "score": 0, "tags": ["humanoid"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1073406342", "ai_score": 7, "ai_comment": "展示了G1人形机器人的双旋踢训练，但缺乏创新架构或显著降低成本的亮点"}, {"id": "956670330", "type": "projects", "title": "evronix/quadruped_sim2sim", "author": "evronix", "abstract": "Deploying a quadruped robot policy(DreamWaQ, walk-these-ways, HIMLoco) trained in Isaacgym to ROS2 Gazebo for Sim-to-Sim", "date": "2025-12-04", "url": "https://github.com/evronix/quadruped_sim2sim", "stars": 33, "score": 0, "tags": ["locomotion"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/956670330", "ai_score": 7, "ai_comment": "将训练好的四足机器人策略部署到ROS2 Gazebo进行Sim-to-Sim，工作扎实，但缺乏新颖的架构或显著的技术突破。"}, {"id": "983137183", "type": "projects", "title": "taco-group/GenAI4AD", "author": "taco-group", "abstract": "a comprehensive and critical synthesis of the emerging role of GenAI across the full autonomous driving stack", "date": "2025-11-25", "url": "https://github.com/taco-group/GenAI4AD", "stars": 218, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/983137183", "ai_score": 7, "ai_comment": "概述了GenAI在自动驾驶全栈中的作用，但缺乏具体的技术细节和实验验证。"}, {"id": "915137783", "type": "projects", "title": "LongchaoDa/AwesomeSim2Real", "author": "LongchaoDa", "abstract": "AwesomeSim2Real - An update-to-date Sim-to-Real repo of \"Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models\"", "date": "2025-12-01", "url": "https://github.com/LongchaoDa/AwesomeSim2Real", "stars": 110, "score": 3, "tags": ["LLM/VLA", "sim2real"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/915137783", "ai_score": 7, "ai_comment": "提供了Sim-to-Real领域的综述和资源汇总，但缺乏原创性实验或创新架构"}, {"id": "950053234", "type": "projects", "title": "zdchan/RobustDexGrasp", "author": "zdchan", "abstract": "This is a repository for RobustDexGrasp, which achieves robust dexterous grasping of 500+ unseen objects with random poses from single-view perception.", "date": "2025-12-01", "url": "https://github.com/zdchan/RobustDexGrasp", "stars": 123, "score": 0, "tags": ["manipulation"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/950053234", "ai_score": 7, "ai_comment": "实现了对500多种未见过物体的鲁棒性灵巧抓取，具有一定的创新性，但缺乏详细的实验数据和复现指南，难以评估其实际效果。"}, {"id": "753968001", "type": "projects", "title": "Tinker-Twins/AutoDRIVE-Coopertitive-MARL", "author": "Tinker-Twins", "abstract": "Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles", "date": "2025-10-28", "url": "https://github.com/Tinker-Twins/AutoDRIVE-Coopertitive-MARL", "stars": 24, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/753968001", "ai_score": 7, "ai_comment": "提出了多智能体深度强化学习在自主车辆协同与竞争中的应用，但缺乏具体实验验证和数据支持"}, {"id": "1077985558", "type": "projects", "title": "Dexmal/dexbotic", "author": "Dexmal", "abstract": "Dexbotic: Open-Source Vision-Language-Action Toolbox", "date": "2025-12-05", "url": "https://github.com/Dexmal/dexbotic", "stars": 554, "score": 3, "tags": ["perception"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1077985558", "ai_score": 7, "ai_comment": "提出了Vision-Language-Action的新工具箱，但在具体应用和实验验证方面有待加强。"}, {"id": "914796651", "type": "projects", "title": "phospho-app/phosphobot", "author": "phospho-app", "abstract": "Control AI robots. Community-driven UI middleware for controlling robots, recording datasets, training action models. Compatible with SO-100 and SO-101 ", "date": "2025-12-04", "url": "https://github.com/phospho-app/phosphobot", "stars": 317, "score": 3, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/914796651", "ai_score": 7, "ai_comment": "社区驱动的UI中间件，兼容特定型号的机器人，功能实用但缺乏创新架构或大规模实验支持。"}, {"id": "1001011932", "type": "projects", "title": "nimiCurtis/so101_ros2", "author": "nimiCurtis", "abstract": "A ROS2 Bridge for Lerobot so101 manipulator", "date": "2025-11-26", "url": "https://github.com/nimiCurtis/so101_ros2", "stars": 20, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1001011932", "ai_score": 7, "ai_comment": "实现了Lerobot so101机械臂的ROS2桥接，但缺乏详细的实验数据和性能评估。"}, {"id": "1003090041", "type": "projects", "title": "utiasDSL/crisp_gym", "author": "utiasDSL", "abstract": "Gym environments for manipulators based on crisp_py and ROS2: collect data and deploy policies on real ROS2 enabled manipulators.", "date": "2025-11-24", "url": "https://github.com/utiasDSL/crisp_gym", "stars": 16, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1003090041", "ai_score": 7, "ai_comment": "提供了基于ROS2的机械臂环境，但缺乏具体的新颖性和实机实验结果展示。"}, {"id": "484009818", "type": "projects", "title": "addmix/godot_aerodynamic_physics", "author": "addmix", "abstract": "", "date": "2025-12-05", "url": "https://github.com/addmix/godot_aerodynamic_physics", "stars": 156, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/484009818", "ai_score": 7, "ai_comment": "实现了基于Godot的游戏物理引擎扩展，增加了流体动力学模拟功能，但缺乏详细的实验结果和应用场景展示。"}, {"id": "912281829", "type": "projects", "title": "timschmidt/csgrs", "author": "timschmidt", "abstract": "Multi-modal constructive solid geometry kernel in Rust ", "date": "2025-12-05", "url": "https://github.com/timschmidt/csgrs", "stars": 170, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/912281829", "ai_score": 7, "ai_comment": "提出了多模态构造实体几何内核的概念，但在具体实现和实验验证方面还需加强。"}, {"id": "60089478", "type": "projects", "title": "sofa-framework/sofa", "author": "sofa-framework", "abstract": "Real-time multi-physics simulation with an emphasis on medical simulation.", "date": "2025-12-05", "url": "https://github.com/sofa-framework/sofa", "stars": 1116, "score": 2, "tags": ["sim2real"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/60089478", "ai_score": 7, "ai_comment": "专注于医疗模拟的实时多物理仿真，但缺乏具体的技术细节和实验验证"}, {"id": "937952934", "type": "projects", "title": "inclusionAI/AReaL", "author": "inclusionAI", "abstract": "Lightning-Fast RL for LLM Reasoning and Agents. Made Simple & Flexible.", "date": "2025-12-05", "url": "https://github.com/inclusionAI/AReaL", "stars": 3113, "score": 2, "tags": ["LLM/VLA"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/937952934", "ai_score": 7, "ai_comment": "提出了快速的RL方法用于LLM推理和代理，但缺乏具体的应用场景和实机实验展示。"}, {"id": "968528285", "type": "projects", "title": "ROBOTIS-GIT/physical_ai_tools", "author": "ROBOTIS-GIT", "abstract": "ROBOTIS Physical AI Tools: Physical AI Development Interface with LeRobot and ROS 2", "date": "2025-12-03", "url": "https://github.com/ROBOTIS-GIT/physical_ai_tools", "stars": 104, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/968528285", "ai_score": 7, "ai_comment": "提供了与LeRobot和ROS 2集成的物理AI开发接口，但缺乏具体实验和案例研究支持"}, {"id": "1003449858", "type": "projects", "title": "RoboticsData/score_lerobot_episodes", "author": "RoboticsData", "abstract": "A lightweight toolkit for quantitatively scoring LeRobot episodes.", "date": "2025-12-02", "url": "https://github.com/RoboticsData/score_lerobot_episodes", "stars": 44, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1003449858", "ai_score": 7, "ai_comment": "提供了LeRobot比赛的评分工具包，但缺乏具体的实验结果和案例分析。"}, {"id": "1002178952", "type": "projects", "title": "TimPietrusky/lerobot.js", "author": "TimPietrusky", "abstract": "interact with your robot in JS, inspired by LeRobot", "date": "2025-11-24", "url": "https://github.com/TimPietrusky/lerobot.js", "stars": 33, "score": 3, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1002178952", "ai_score": 7, "ai_comment": "提供了JavaScript与机器人交互的接口，但缺乏具体实验和案例展示"}, {"id": "1027339581", "type": "projects", "title": "ycheng517/lerobot-ros", "author": "ycheng517", "abstract": "Lightweight interface for controlling ROS-based robotic arms using LeRobot", "date": "2025-12-04", "url": "https://github.com/ycheng517/lerobot-ros", "stars": 128, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1027339581", "ai_score": 7, "ai_comment": "提供了轻量级接口控制基于ROS的机械臂，但缺少具体实验和数据支持"}, {"id": "1035347985", "type": "projects", "title": "fracapuano/robot-learning-tutorial", "author": "fracapuano", "abstract": "All the source code for \"Robot Learning: A Tutorial\". Get involved to be featured in the next iteration!", "date": "2025-12-05", "url": "https://github.com/fracapuano/robot-learning-tutorial", "stars": 399, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1035347985", "ai_score": 7, "ai_comment": "提供了机器人学习的教程代码，内容扎实，但缺乏创新点或显著的实验结果展示。"}, {"id": "986639754", "type": "projects", "title": "astroyat/lerobot-ros", "author": "astroyat", "abstract": "ROS 2 bridge for LeRobot mobile base, arm and sensors.", "date": "2025-12-03", "url": "https://github.com/astroyat/lerobot-ros", "stars": 21, "score": 0, "tags": ["perception"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/986639754", "ai_score": 7, "ai_comment": "实现了LeRobot移动基座、机械臂和传感器的ROS 2桥接，但缺乏详细的实验数据和性能分析"}, {"id": "InternRobotics/InternVLA-N1", "type": "models", "title": "InternRobotics/InternVLA-N1", "author": "InternRobotics", "abstract": "🔥 Trending on HF (36 likes). Task: robotics", "date": "None", "url": "https://huggingface.co/InternRobotics/InternVLA-N1", "stars": 36, "score": 5, "tags": ["LLM/VLA"], "source": "HuggingFace", "ai_score": 7, "ai_comment": "提出了基于VLA的新架构，但缺乏实机实验"}, {"id": "2512.04976v1", "type": "papers", "title": "Multipole decomposition of the gravitational field of a point mass at the black hole horizon", "author": "João P. B. Brito, Atsushi Higuchi, Luís C. B. Crispino", "abstract": "The portion of the gravitational energy absorbed by the black hole due to the radial infall of a point mass is known to diverge at leading order in perturbation theory. This divergence is an artifact of the point-particle model, where the contribution of each multipole to the total absorbed energy is observed to be roughly constant. We show explicitly that this divergent energy arises from the infinite energy present in the singular static field arbitrarily close to the point mass, which also flows into the black hole when the particle trajectory crosses the horizon. We perform a multipole decomposition of the linearized gravitational field generated by the point mass near its world line at the black hole horizon. By applying the standard field-theoretical approach to the particle field, we compute the corresponding partial energy and find that it matches the constant multipole contribution.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04976v1", "score": 3, "tags": [], "source": "arXiv", "ai_score": 5, "ai_comment": "工作在理论上探讨了点质量在黑洞视界附近的引力场分解，但缺乏具身智能或机器人相关的实验验证。"}, {"id": "2512.04831v1", "type": "papers", "title": "Clustering country-level all-cause mortality data: a review", "author": "Pedro Menezes de Araujo, Isobel Claire Gormley, Thomas Brendan Murphy", "abstract": "Mortality data are relevant to demography, public health, and actuarial science. Whilst clustering is increasingly used to explore patterns in such data, no study has reviewed its application to country-level all-cause mortality. This review therefore summarises recent work and addresses key questions: why clustering is used, which mortality data are analysed, which methods are most common, and what main findings emerge. To address these questions, we examine studies applying clustering to country-level all-cause mortality, focusing on mortality indices, data sources, and methodological choices, and we replicate some approaches using Human Mortality Database (HMD) data. Our analysis reveals that clustering is mainly motivated by forecasting and by studying convergence and inequality. Most studies use HMD data from developed countries and rely on k-means, hierarchical, or functional clustering. Main findings include a persistent East-West European division across applications, with clustering generally improving forecast accuracy over single-country models. Overall, this review highlights the methodological range in the literature, summarises clustering results, and identifies gaps, such as the limited evaluation of clustering quality and the underuse of data from countries outside the high-income world.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04831v1", "score": 3, "tags": ["perception"], "source": "arXiv", "ai_score": 5, "ai_comment": "对国家层面全因死亡率数据的聚类分析进行了综述，但缺乏具身智能或机器人相关的实际应用或创新。"}, {"id": "2512.04800v1", "type": "papers", "title": "Time-periodic solutions to an energy balance model coupled with an active fluid under arbitrary large forces", "author": "Gianmarco Del Sarto, Matthias Hieber, Filippo Palma", "abstract": "This article concerns time-periodic solutions to a two-dimensional Sellers-type energy balance model coupled to the three-dimensional primitive equations via a dynamic boundary condition. It is shown that the underlying equations admit at least one strong time-periodic solution, provided the forcing term is time-periodic. The forcing term does not need to satisfy a smallness condition and is allowed to be arbitrarily large.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.04800v1", "score": 3, "tags": [], "source": "arXiv", "ai_score": 5, "ai_comment": "理论分析扎实，但缺乏具身智能或机器人相关的实验验证。"}, {"id": "1061157005", "type": "projects", "title": "Grigorij-Dudnik/RoboCrew", "author": "Grigorij-Dudnik", "abstract": "🦾Set up your embodied LLM agent with the same ease as normal agents in CrewAI or Autogen", "date": "2025-12-04", "url": "https://github.com/Grigorij-Dudnik/RoboCrew", "stars": 21, "score": 0, "tags": ["LLM/VLA"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1061157005", "ai_score": 5, "ai_comment": "介绍了使用CrewAI或Autogen的具身LLM代理设置方法，但缺乏具体实验或案例支持"}, {"id": "999550439", "type": "projects", "title": "SIBench/Awesome-Visual-Spatial-Reasoning", "author": "SIBench", "abstract": "This is a project about visual spatial reasoning.", "date": "2025-12-04", "url": "https://github.com/SIBench/Awesome-Visual-Spatial-Reasoning", "stars": 81, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/999550439", "ai_score": 5, "ai_comment": "项目标题和摘要信息量不足，缺乏具体的研究内容和实验结果展示。"}, {"id": "801867971", "type": "projects", "title": "TianxingChen/Embodied-AI-Guide", "author": "TianxingChen", "abstract": "[Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide", "date": "2025-12-05", "url": "https://github.com/TianxingChen/Embodied-AI-Guide", "stars": 9466, "score": 5, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/801867971", "ai_score": 5, "ai_comment": "内容较为基础，缺乏创新性架构或显著实验结果，适合初学者入门但无突破性贡献。"}, {"id": "790280061", "type": "projects", "title": "rh20624/Awesome-IMU-Sensing", "author": "rh20624", "abstract": "A collection of datasets, papers, and resources  for Generalizable Human Activity Recognition and IMU sensing.", "date": "2025-12-04", "url": "https://github.com/rh20624/Awesome-IMU-Sensing", "stars": 123, "score": 3, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/790280061", "ai_score": 5, "ai_comment": "收集了大量数据集、论文和资源，但缺乏具体的技术贡献或创新点。"}, {"id": "1104737428", "type": "projects", "title": "AKS-Lab-Univertsity-of-Tartu/bimanual_manipulation", "author": "AKS-Lab-Univertsity-of-Tartu", "abstract": "This repository is for tracking progress of bimanual manipulation for different tasks. ", "date": "2025-12-04", "url": "https://github.com/AKS-Lab-Univertsity-of-Tartu/bimanual_manipulation", "stars": 11, "score": 0, "tags": ["manipulation"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1104737428", "ai_score": 5, "ai_comment": "记录了双臂操作的不同任务进展，但缺乏具体的实验数据和结果展示，内容较为常规。"}, {"id": "1048227303", "type": "projects", "title": "CAI23sbP/go2_parkour_deploy", "author": "CAI23sbP", "abstract": "IsaacLab to Mujoco GO2 deploy, IsaacLab to Real world GO2 deploy", "date": "2025-12-05", "url": "https://github.com/CAI23sbP/go2_parkour_deploy", "stars": 37, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1048227303", "ai_score": 5, "ai_comment": "项目涉及从IsaacLab到Mujoco和现实世界的GO2部署，但缺乏具体细节和实验结果，难以评估其实用性和创新性。"}, {"id": "921614730", "type": "projects", "title": "luohongk/Embodied-AI-Daily", "author": "luohongk", "abstract": "📚这个仓库是在arxiv上收集的有关VLN，VLA，World Model，SLAM，Gaussian Splatting,非线性优化等相关论文。每天都会自动更新！issue区域是最新10篇论文", "date": "2025-12-04", "url": "https://github.com/luohongk/Embodied-AI-Daily", "stars": 123, "score": 0, "tags": ["LLM/VLA", "navigation"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/921614730", "ai_score": 5, "ai_comment": "收集了多个领域的论文，但缺乏具体的研究成果或创新，仅作为文献汇总，实用性有限。"}, {"id": "1086165349", "type": "projects", "title": "Dexmal/realtime-vla", "author": "Dexmal", "abstract": "Running VLA at 30Hz frame rate and 480Hz trajectory frequency", "date": "2025-12-04", "url": "https://github.com/Dexmal/realtime-vla", "stars": 297, "score": 0, "tags": ["LLM/VLA"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1086165349", "ai_score": 5, "ai_comment": "提出了在30Hz帧率和480Hz轨迹频率下运行VLA的方法，但缺乏详细实验结果和对比分析"}, {"id": "1076162882", "type": "projects", "title": "csmile-1006/DEAS-Isaac-GR00T", "author": "csmile-1006", "abstract": "DEAS + Isaac-GR00T + RoboCasa", "date": "2025-12-05", "url": "https://github.com/csmile-1006/DEAS-Isaac-GR00T", "stars": 14, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1076162882", "ai_score": 5, "ai_comment": "结合了DEAS和Isaac-GR00T，但缺少具体实验结果和RoboCasa的详细应用说明"}, {"id": "738986318", "type": "projects", "title": "WaterFutures/EPyT-Flow", "author": "WaterFutures", "abstract": "A Python package designed for the easy generation of hydraulic and water quality scenario data of water distribution networks.", "date": "2025-12-05", "url": "https://github.com/WaterFutures/EPyT-Flow", "stars": 38, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/738986318", "ai_score": 5, "ai_comment": "提供了用于生成水分布网络水力和水质场景数据的Python包，但缺乏具体的应用案例和实验验证。"}, {"id": "5616877", "type": "projects", "title": "Revolutionary-Games/Thrive", "author": "Revolutionary-Games", "abstract": "The main repository for the development of the evolution game Thrive. ", "date": "2025-12-05", "url": "https://github.com/Revolutionary-Games/Thrive", "stars": 3349, "score": 2, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/5616877", "ai_score": 5, "ai_comment": "这是一个关于进化游戏的开发仓库，但缺乏具体的具身智能或机器人相关实验或应用，内容较为常规。"}, {"id": "235830706", "type": "projects", "title": "ie3-institute/PowerSystemDataModel", "author": "ie3-institute", "abstract": "Provides an elaborated data model to model energy systems with a high granularity e.g. for bottom-up simulations. Additionally, useful functionalities to handle the model i/o are provided as well.", "date": "2025-12-05", "url": "https://github.com/ie3-institute/PowerSystemDataModel", "stars": 23, "score": 0, "tags": ["sim2real"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/235830706", "ai_score": 5, "ai_comment": "提供了高粒度的能量系统数据模型，但缺乏具身智能或机器人相关的实际应用或实验验证。"}, {"id": "607539353", "type": "projects", "title": "AspirinCode/papers-for-molecular-design-using-DL", "author": "AspirinCode", "abstract": "List of Molecular and Material design using Generative AI and Deep Learning ", "date": "2025-12-05", "url": "https://github.com/AspirinCode/papers-for-molecular-design-using-DL", "stars": 886, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/607539353", "ai_score": 5, "ai_comment": "内容涉及分子和材料设计的生成AI和深度学习，但缺乏具体的技术细节和实验结果支持，难以评估其实用价值。"}, {"id": "948904124", "type": "projects", "title": "SkyworkAI/Skywork-R1V", "author": "SkyworkAI", "abstract": "Skywork-R1V is an advanced multimodal AI model series developed by Skywork AI (Kunlun Inc.), specializing in vision-language reasoning.", "date": "2025-12-05", "url": "https://github.com/SkyworkAI/Skywork-R1V", "stars": 3113, "score": 2, "tags": ["perception"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/948904124", "ai_score": 5, "ai_comment": "专注于视觉-语言推理的多模态AI模型系列，但缺乏具体的技术细节和实验结果展示。"}, {"id": "1004147641", "type": "projects", "title": "microsoft/agent-lightning", "author": "microsoft", "abstract": "The absolute trainer to light up AI agents.", "date": "2025-12-05", "url": "https://github.com/microsoft/agent-lightning", "stars": 9391, "score": 5, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1004147641", "ai_score": 5, "ai_comment": "提出了具身智能代理的新训练方法，但缺乏具体实验和数据支持"}, {"id": "935190749", "type": "projects", "title": "Tavish9/any4lerobot", "author": "Tavish9", "abstract": "🎁 A collection of utilities for LeRobot.", "date": "2025-12-05", "url": "https://github.com/Tavish9/any4lerobot", "stars": 689, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/935190749", "ai_score": 5, "ai_comment": "提供了LeRobot的实用工具集合，但缺乏具体的实验或案例展示其有效性。"}, {"id": "1020834177", "type": "projects", "title": "SpesRobotics/lerobot-robot-xarm", "author": "SpesRobotics", "abstract": "xArm integration for LeRobot", "date": "2025-12-03", "url": "https://github.com/SpesRobotics/lerobot-robot-xarm", "stars": 12, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1020834177", "ai_score": 5, "ai_comment": "介绍了xArm在LeRobot中的集成，但缺乏详细的实验数据和结果展示，内容较为常规。"}, {"id": "667155414", "type": "projects", "title": "PCrnjak/PAROL6-Desktop-robot-arm", "author": "PCrnjak", "abstract": "BOM, STL files and instructions for PAROL6 3D printed robot arm", "date": "2025-12-05", "url": "https://github.com/PCrnjak/PAROL6-Desktop-robot-arm", "stars": 2310, "score": 2, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/667155414", "ai_score": 5, "ai_comment": "提供了3D打印机器人手臂的BOM和STL文件，但缺乏详细的实验结果和应用场景展示。"}, {"id": "lerobot/xvla-base", "type": "models", "title": "lerobot/xvla-base", "author": "lerobot", "abstract": "🔥 Trending on HF (5 likes). Task: robotics", "date": "None", "url": "https://huggingface.co/lerobot/xvla-base", "stars": 5, "score": 5, "tags": ["LLM/VLA"], "source": "HuggingFace", "ai_score": 5, "ai_comment": "任务明确指向机器人领域，但缺乏具体实验或数据支持，难以评估其实用价值。"}, {"id": "lerobot/xvla-google-robot", "type": "models", "title": "lerobot/xvla-google-robot", "author": "lerobot", "abstract": "🔥 Trending on HF (4 likes). Task: robotics", "date": "None", "url": "https://huggingface.co/lerobot/xvla-google-robot", "stars": 4, "score": 5, "tags": ["LLM/VLA"], "source": "HuggingFace", "ai_score": 5, "ai_comment": "任务明确指向机器人领域，但缺乏具体的技术突破或实验证据支持，内容较为常规。"}, {"id": "lerobot/xvla-folding", "type": "models", "title": "lerobot/xvla-folding", "author": "lerobot", "abstract": "🔥 Trending on HF (4 likes). Task: robotics", "date": "None", "url": "https://huggingface.co/lerobot/xvla-folding", "stars": 4, "score": 5, "tags": ["LLM/VLA"], "source": "HuggingFace", "ai_score": 5, "ai_comment": "任务明确为机器人领域，但缺乏具体的技术突破或实验证据支持，内容较为常规。"}, {"id": "Adilbai/stock-trading-rl-agent", "type": "models", "title": "Adilbai/stock-trading-rl-agent", "author": "Adilbai", "abstract": "🔥 Trending on HF (68 likes). Task: reinforcement-learning", "date": "None", "url": "https://huggingface.co/Adilbai/stock-trading-rl-agent", "stars": 68, "score": 5, "tags": [], "source": "HuggingFace", "ai_score": 5, "ai_comment": "该工作涉及强化学习在股票交易中的应用，但缺乏具体实验和实机验证，难以评估其实用性。"}, {"id": "lerobot/xvla-agibot-world", "type": "models", "title": "lerobot/xvla-agibot-world", "author": "lerobot", "abstract": "🏛️ Official release from lerobot. Task: robotics", "date": "2025-12-03", "url": "https://huggingface.co/lerobot/xvla-agibot-world", "stars": 3, "score": 10, "tags": ["LLM/VLA"], "source": "HuggingFace", "ai_score": 5, "ai_comment": "任务明确指向机器人领域，但缺乏具体的技术突破或实验证据支持"}, {"id": "2512.05045v1", "type": "papers", "title": "On random matrix statistics of 3d gravity", "author": "Daniel L. Jafferis, Liza Rozenberg, Debmalya Sarkar", "abstract": "We show that 3d gravity on manifolds that are topologically a Riemann surface times an interval $Σ_{g,n}\\times I$ with end-of-the-world branes at the ends of the interval is described by a random matrix model, namely the Virasoro minimal string. Because these manifolds have $n$ annular asymptotic boundaries, the path integrals naturally correspond to spectral correlators of open strings upon inverse Fourier transforms. For $g=0$ and $n=2$, we carry out an explicit path integration and find precise agreement with the universal random matrix expression. For Riemann surfaces with negative Euler characteristic, we evaluate the path integral as a gravitational inner product between states prepared by two copies of Virasoro TQFT. Along the way, we clarify the effects of gauging the mapping class group and the connection to chiral 3d gravity.", "date": "2025-12-04", "url": "https://arxiv.org/pdf/2512.05045v1", "score": 3, "tags": [], "source": "arXiv", "ai_score": 4, "ai_comment": "该工作探讨了3d引力与随机矩阵统计之间的关系，但与具身智能/机器人领域关联不大。"}, {"id": "1037741738", "type": "projects", "title": "RLinf/RLinf", "author": "RLinf", "abstract": "RLinf is a flexible and scalable open-source infrastructure designed for post-training foundation models (LLMs, VLMs, VLAs) via reinforcement learning.", "date": "2025-12-05", "url": "https://github.com/RLinf/RLinf", "stars": 1565, "score": 2, "tags": ["LLM/VLA"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1037741738", "ai_score": 4, "ai_comment": "提出了基于LLMs、VLMs和VLAs的强化学习后训练框架，但缺乏具体实机实验和详细案例支持。"}, {"id": "958377083", "type": "projects", "title": "ZizoTheDev/ffmpeg-mcp", "author": "ZizoTheDev", "abstract": "An MCP server for FFmpeg", "date": "2025-12-05", "url": "https://github.com/ZizoTheDev/ffmpeg-mcp", "stars": 15, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/958377083", "ai_score": 4, "ai_comment": "项目仅描述了一个FFmpeg的MCP服务器，缺乏具体实现和实验验证，对于具身智能领域的贡献有限。"}, {"id": "371028278", "type": "projects", "title": "easystats/datawizard", "author": "easystats", "abstract": "Magic potions to clean and transform your data 🧙 ", "date": "2025-12-01", "url": "https://github.com/easystats/datawizard", "stars": 233, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/371028278", "ai_score": 4, "ai_comment": "专注于数据清洗和转换的工具包，但缺乏与具身智能直接相关的应用或实验验证。"}, {"id": "622841087", "type": "projects", "title": "permas4edu/permas4edu", "author": "permas4edu", "abstract": "Collection of finite element models ", "date": "2025-12-05", "url": "https://github.com/permas4edu/permas4edu", "stars": 22, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/622841087", "ai_score": 4, "ai_comment": "内容仅为有限元模型集合，缺乏具体应用场景和实验验证，实用性有限。"}, {"id": "429772577", "type": "projects", "title": "ie3-institute/simona", "author": "ie3-institute", "abstract": "simona is an agent-based discrete-event power system simulation model developed @ie3-institute ", "date": "2025-12-05", "url": "https://github.com/ie3-institute/simona", "stars": 33, "score": 0, "tags": ["sim2real"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/429772577", "ai_score": 4, "ai_comment": "这是一个基于离散事件的电力系统仿真模型，但缺乏具身智能或机器人相关的内容，难以直接应用于具身智能领域。"}, {"id": "134826304", "type": "projects", "title": "PHAREHUB/PHARE", "author": "PHAREHUB", "abstract": "💫 Parallel Hybrid Particle In Cell code with Adaptive mesh REfinement", "date": "2025-12-05", "url": "https://github.com/PHAREHUB/PHARE", "stars": 74, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/134826304", "ai_score": 4, "ai_comment": "该工作涉及粒子在细胞中的并行混合算法，但未见其与具身智能或机器人技术的直接关联，且缺乏具体应用实例或实验验证。"}, {"id": "604105885", "type": "projects", "title": "AqwamCreates/DataPredict", "author": "AqwamCreates", "abstract": "Lua-Based Machine Learning, Deep Learning And Reinforcement Learning Library (For Roblox And Pure Lua). Contains Over 85 Models!", "date": "2025-12-05", "url": "https://github.com/AqwamCreates/DataPredict", "stars": 11, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/604105885", "ai_score": 4, "ai_comment": "这是一个基于Lua的机器学习库，包含多种模型，但缺乏与具身智能或机器人技术的具体关联，难以评估其实用性。"}, {"id": "664913876", "type": "projects", "title": "InternLM/xtuner", "author": "InternLM", "abstract": "A Next-Generation Training Engine Built for Ultra-Large MoE Models", "date": "2025-12-05", "url": "https://github.com/InternLM/xtuner", "stars": 5015, "score": 5, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/664913876", "ai_score": 4, "ai_comment": "介绍了超大规模Moe模型的训练引擎，但缺乏具体实验和实机测试数据，难以评估实际效果。"}, {"id": "71932349", "type": "projects", "title": "ray-project/ray", "author": "ray-project", "abstract": "Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.", "date": "2025-12-05", "url": "https://github.com/ray-project/ray", "stars": 40155, "score": 5, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/71932349", "ai_score": 4, "ai_comment": "介绍了AI计算引擎Ray，但缺乏与具身智能或机器人相关的具体应用或实验验证。"}, {"id": "1044554840", "type": "projects", "title": "NVIDIA-NeMo/Gym", "author": "NVIDIA-NeMo", "abstract": "Build RL environments for LLM training", "date": "2025-12-05", "url": "https://github.com/NVIDIA-NeMo/Gym", "stars": 72, "score": 0, "tags": ["LLM/VLA"], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1044554840", "ai_score": 4, "ai_comment": "专注于语言模型训练的RL环境构建，与具身智能关系不大。"}, {"id": "nvidia/GR00T-N1.5-3B", "type": "models", "title": "nvidia/GR00T-N1.5-3B", "author": "nvidia", "abstract": "🔥 Trending on HF (165 likes). Task: robotics", "date": "None", "url": "https://huggingface.co/nvidia/GR00T-N1.5-3B", "stars": 165, "score": 5, "tags": [], "source": "HuggingFace", "ai_score": 4, "ai_comment": "与具身智能任务相关，但缺乏具体实验或数据支持，难以评估其实用性。"}, {"id": "PrimeIntellect/INTELLECT-3", "type": "models", "title": "PrimeIntellect/INTELLECT-3", "author": "PrimeIntellect", "abstract": "🔥 Trending on HF (176 likes). Task: text-generation", "date": "None", "url": "https://huggingface.co/PrimeIntellect/INTELLECT-3", "stars": 176, "score": 5, "tags": [], "source": "HuggingFace", "ai_score": 4, "ai_comment": "文本生成任务，缺乏具身智能或机器人领域的实际应用和实验验证。"}, {"id": "bartowski/PrimeIntellect_INTELLECT-3-GGUF", "type": "models", "title": "bartowski/PrimeIntellect_INTELLECT-3-GGUF", "author": "bartowski", "abstract": "🔥 Trending on HF (20 likes). Task: text-generation", "date": "None", "url": "https://huggingface.co/bartowski/PrimeIntellect_INTELLECT-3-GGUF", "stars": 20, "score": 5, "tags": [], "source": "HuggingFace", "ai_score": 4, "ai_comment": "文本生成任务，缺乏具身智能或机器人相关应用，纯理论工作。"}, {"id": "PrimeIntellect/INTELLECT-3-FP8", "type": "models", "title": "PrimeIntellect/INTELLECT-3-FP8", "author": "PrimeIntellect", "abstract": "🔥 Trending on HF (18 likes). Task: text-generation", "date": "None", "url": "https://huggingface.co/PrimeIntellect/INTELLECT-3-FP8", "stars": 18, "score": 5, "tags": [], "source": "HuggingFace", "ai_score": 4, "ai_comment": "文本生成任务的实现，但缺乏具身智能或机器人相关的应用或创新。"}, {"id": "AXONVERTEX-AI-RESEARCH/Orchestrator-8B-Q8_0-GGUF", "type": "models", "title": "AXONVERTEX-AI-RESEARCH/Orchestrator-8B-Q8_0-GGUF", "author": "AXONVERTEX-AI-RESEARCH", "abstract": "🔥 Trending on HF (7 likes). Task: reinforcement-learning", "date": "None", "url": "https://huggingface.co/AXONVERTEX-AI-RESEARCH/Orchestrator-8B-Q8_0-GGUF", "stars": 7, "score": 5, "tags": [], "source": "HuggingFace", "ai_score": 4, "ai_comment": "任务为强化学习，但缺乏具体应用场景和实机实验，难以评估其实用价值。"}, {"id": "keras-io/conv-lstm", "type": "models", "title": "keras-io/conv-lstm", "author": "keras-io", "abstract": "🔥 Trending on HF (4 likes). Task: video-to-video", "date": "None", "url": "https://huggingface.co/keras-io/conv-lstm", "stars": 4, "score": 5, "tags": [], "source": "HuggingFace", "ai_score": 4, "ai_comment": "这是一个视频到视频的任务，但缺乏具体的实验结果和实机应用展示，难以评估其实用价值。"}, {"id": "973471437", "type": "projects", "title": "militarandroid/cybersecurity_hack", "author": "militarandroid", "abstract": "Social Media, Website, Crypto Hacking 🔥 Hacker, Cyber, Cybersecurity. Instagram, TikTok, YouTube, Telegram, Snapchat Hacking. Cryptocurrency Bitcoin Ethereum Litecoin Dogecoin Solana Wallet. Dark Deep Web Hacker. Password cracker, mail gmail, webapp, zip, vulnerability, RAT, malware, trojan, black hat, PHP HTML hacker. Exploit social engineering.", "date": "2025-12-05", "url": "https://github.com/militarandroid/cybersecurity_hack", "stars": 36, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/973471437", "ai_score": 3, "ai_comment": "内容涉及黑客攻击和恶意软件，与具身智能关系不大，且存在法律和道德风险。"}, {"id": "thuml/ivideogpt-oxe-64-act-free-medium", "type": "models", "title": "thuml/ivideogpt-oxe-64-act-free-medium", "author": "thuml", "abstract": "🔥 Trending on HF (0 likes). Task: None", "date": "None", "url": "https://huggingface.co/thuml/ivideogpt-oxe-64-act-free-medium", "stars": 0, "score": 8, "tags": [], "source": "HuggingFace", "ai_score": 3, "ai_comment": "与具身智能/机器人无关，且无具体任务说明，缺乏实际应用价值。"}, {"id": "thuml/ivideogpt-oxe-64-act-free", "type": "models", "title": "thuml/ivideogpt-oxe-64-act-free", "author": "thuml", "abstract": "🔥 Trending on HF (2 likes). Task: None", "date": "None", "url": "https://huggingface.co/thuml/ivideogpt-oxe-64-act-free", "stars": 2, "score": 8, "tags": [], "source": "HuggingFace", "ai_score": 2, "ai_comment": "与具身智能/机器人无关，仅标注为Trending但未说明具体任务或贡献。"}, {"id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2", "type": "models", "title": "nvidia/NVIDIA-Nemotron-Nano-9B-v2", "author": "nvidia", "abstract": "🏛️ Official release from nvidia. Task: text-generation", "date": "2025-12-05", "url": "https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2", "stars": 448, "score": 10, "tags": [], "source": "HuggingFace", "ai_score": 2, "ai_comment": "与具身智能/机器人无关，仅为文本生成任务，缺乏相关性。"}, {"id": "nvidia/music-flamingo-hf", "type": "models", "title": "nvidia/music-flamingo-hf", "author": "nvidia", "abstract": "🏛️ Official release from nvidia. Task: audio-text-to-text", "date": "2025-12-04", "url": "https://huggingface.co/nvidia/music-flamingo-hf", "stars": 56, "score": 10, "tags": [], "source": "HuggingFace", "ai_score": 2, "ai_comment": "与具身智能/机器人无关，专注于音频文本生成任务，缺乏与具身智能的相关性。"}, {"id": "1701384", "type": "projects", "title": "ufz/ogs", "author": "ufz", "abstract": "DO NOT USE THIS REPO! Migrated to https://gitlab.opengeosys.org/ogs/ogs!", "date": "2025-12-05", "url": "https://github.com/ufz/ogs", "stars": 121, "score": 0, "tags": [], "source": "GitHub", "media_url": "https://opengraph.githubassets.com/1/1701384", "ai_score": 0, "ai_comment": "与具身智能/机器人无关，该仓库已被迁移，内容不再更新。"}, {"id": "thuml/ivideogpt-oxe-64-goal-cond", "type": "models", "title": "thuml/ivideogpt-oxe-64-goal-cond", "author": "thuml", "abstract": "🔥 Trending on HF (0 likes). Task: None", "date": "None", "url": "https://huggingface.co/thuml/ivideogpt-oxe-64-goal-cond", "stars": 0, "score": 5, "tags": [], "source": "HuggingFace", "ai_score": 0, "ai_comment": "与具身智能/机器人无关，且无具体任务说明，缺乏研究价值。"}, {"id": "thuml/ivideogpt-oxe-256-act-free", "type": "models", "title": "thuml/ivideogpt-oxe-256-act-free", "author": "thuml", "abstract": "🔥 Trending on HF (0 likes). Task: None", "date": "None", "url": "https://huggingface.co/thuml/ivideogpt-oxe-256-act-free", "stars": 0, "score": 8, "tags": [], "source": "HuggingFace", "ai_score": 0, "ai_comment": "与具身智能/机器人无关，且无具体任务说明，缺乏研究价值。"}, {"id": "nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1", "type": "models", "title": "nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1", "author": "nvidia", "abstract": "🏛️ Official release from nvidia. Task: image-text-to-text", "date": "2025-12-04", "url": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1", "stars": 168, "score": 10, "tags": [], "source": "HuggingFace", "ai_score": 0, "ai_comment": "与具身智能/机器人无关，标题和摘要信息不相关。"}];
    window.ALL_TAGS = ["perception", "navigation", "sim2real", "manipulation", "locomotion", "LLM/VLA", "humanoid"];
    window.DAILY_SUMMARY = "🤖 Embodied AI 日报\n📅 2025-12-05\n\n1. ⭐ [7.0分] InternRobotics/InternVLA-N1\n   💡 提出了基于VLA的新架构，但缺乏实机实验\n   🔗 https://huggingface.co/InternRobotics/InternVLA-N1\n\n2. ⭐ [7.0分] Wan-AI/Wan2.2-TI2V-5B\n   💡 文本转视频任务有趣且扎实，但缺乏具体实验细节和性能指标展示\n   🔗 https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B\n\n3. 🔥 [8.0分] mujocolab/mjlab\n   💡 提供了MuJoCo-Warp驱动的Isaac Lab API，有助于RL和机器人研究，但缺乏具体的应用案例和实验结果展示。\n   🔗 https://github.com/mujocolab/mjlab\n\n4. ⭐ [7.0分] zauberzeug/nicegui\n   💡 提供了一种使用Python创建基于Web的用户界面的方法，但缺乏具身智能或机器人相关的实际应用案例。\n   🔗 https://github.com/zauberzeug/nicegui\n\n5. ⭐ [7.0分] wadeKeith/DeepThinkVLA\n   💡 提出了增强视觉-语言-行动模型推理能力的新方法，但缺少具体实验验证和实机测试结果。\n   🔗 https://github.com/wadeKeith/DeepThinkVLA\n\n6. ⭐ [7.0分] QuestNav/QuestNav\n   💡 实现了Meta Quest头显姿态流传输到FRC机器人，但在Unity中的实现细节和优化空间有待进一步阐述。\n   🔗 https://github.com/QuestNav/QuestNav\n\n7. ⭐ [7.0分] ros-controls/ros2_control\n   💡 提供了ROS 2的通用控制框架，结构简洁，但缺乏具体应用案例和实验验证\n   🔗 https://github.com/ros-controls/ros2_control\n\n8. ⭐ [7.0分] OpenMind/OM1\n   💡 提出了模块化AI运行时框架，但缺乏具体实验和案例研究支持\n   🔗 https://github.com/OpenMind/OM1\n\n9. ⭐ [7.0分] verivital/behaverify\n   💡 提供了一种形式化验证行为树的方法，但在实际应用案例和详细实验结果方面还有提升空间。\n   🔗 https://github.com/verivital/behaverify\n\n10. ⭐ [7.0分] AD-SDL/MADSci\n   💡 提出了模块化自主科学发现框架，但缺乏具体实机实验和数据支持\n   🔗 https://github.com/AD-SDL/MADSci\n";
    window.LAST_UPDATE = "2025-12-05 16:18:08";
    